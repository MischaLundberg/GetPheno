#!/usr/bin/env python3


from __future__ import annotations
import argparse
import configparser
import csv
import gc
import os
import pickle
import random
import re
import socket
import sqlite3
import subprocess
import sys
import uuid
import warnings
import logging
import datetime
from collections import OrderedDict
from concurrent.futures import ThreadPoolExecutor
from datetime import date, datetime, timedelta
from typing import Any, Dict, Iterable, List, Optional, Sequence, Tuple
import json

import numpy as np
import pandas as pd
from packaging.version import Version, parse
from pandas.errors import ParserWarning
import psutil

#-------------------------------------------------------------------------------
# Pre-compiled regex for splitting ICD strings.
SPLIT_PATTERN = re.compile(r"^([A-Za-z]+)(\d+)(?:\.(\d+))?")
#-------------------------------------------------------------------------------

#-------------------------------------------------------------------------------
# Logger setup

# logger = logging.getLogger(script_name)
# logger.setLevel(logging.DEBUG)

# basic_formatter = logging.Formatter(
# "[%(levelname)s] %(message)s"
# )

# stream_handler = logging.StreamHandler()
# stream_handler.setLevel(logging.INFO)

# stream_handler.setFormatter(basic_formatter)
# logger.addHandler(stream_handler)
#-------------------------------------------------------------------------------

#-------------------------------------------------------------------------------
# Set global variables
python_min_version = "3.8.1"
pd_min_version = "1.3.2"
pd_recommended_version = "1.3.6"
DateFormat = "%Y-%m-%d"
ATC_Requested = "NotSet"
DayFirst = False
cluster_run = "Default"
verbose = False
dta_input = False
lifetime_exclusions = pd.DataFrame()
oneYearPrior_exclusions = pd.DataFrame()
post_exclusions = pd.DataFrame()
covariates = pd.DataFrame()
extra_cols_to_keep = []
min_Age = -1
max_Age = -1
DK_clusters = ("NCRR_DST", "IBP_DST", "IBP_computerome", "iPSYCH", "CHB_DBDS", "DBDS_DST")
__version__ = "V1.24_Alpha-10.2025"
disclaimer_text = (
    f"GetPheno version: {__version__}\n"
    "Disclaimer:\n"
    "This tool is provided as-is without any guarantees or warranties regarding the accuracy, completeness, or reliability of the results. \n"
    "The information generated is intended to assist and should not be relied upon as the sole source for decision-making. \n"
    "Users are strongly encouraged to double-check all outputs, validate results independently, and apply their own judgment and common sense. \n"
    "The creators of this tool are not responsible for any consequences arising from its use. \n"
    "By using this tool, you acknowledge and agree to these terms."
)    
script_name = "get_pheno."+__version__+"."
logger = logging.getLogger(script_name)
#-------------------------------------------------------------------------------



#################################################################
#===============================================================#
#################################################################

MDD_Codes = ["ICD10:F32", "ICD10:F32.0", "ICD10:F32.1", "ICD10:F32.2", "ICD10:F32.3", "ICD10:F32.8", "ICD10:F32.9", "ICD10:F33", "ICD10:F33.0", 
"ICD10:F33.1", "ICD10:F33.2", "ICD10:F33.3", "ICD10:F33.4", "ICD10:F33.8", "ICD10:F33.9", "ICD10-CM:F32", "ICD10-CM:F32.0", "ICD10-CM:F32.1", 
"ICD10-CM:F32.2", "ICD10-CM:F32.3", "ICD10-CM:F32.8", "ICD10-CM:F32.9", "ICD10-CM:F33", "ICD10-CM:F33.0", "ICD10-CM:F33.1", "ICD10-CM:F33.2", 
"ICD10-CM:F33.3", "ICD10-CM:F33.4", "ICD10-CM:F33.8", "ICD10-CM:F33.9", "ICD9:296.1", "ICD9:296.10", "ICD9:296.11", "ICD9:296.12", "ICD9:296.13", 
"ICD9:296.14", "ICD9:296.15", "ICD9:296.16", "ICD9:296.2", "ICD9:296.20", "ICD9:296.21", "ICD9:296.22", "ICD9:296.23", 
"ICD9:296.24", "ICD9:296.25", "ICD9:296.26", "ICD9:298.0", "ICD9:300.4", "ICD9-CM:296.1", "ICD9-CM:296.10", "ICD9-CM:296.11", "ICD9-CM:296.12", "ICD9-CM:296.13", 
"ICD9-CM:296.14", "ICD9-CM:296.15", "ICD9-CM:296.16", "ICD9-CM:296.2", "ICD9-CM:296.20", "ICD9-CM:296.21", "ICD9-CM:296.22", "ICD9-CM:296.23", "ICD9-CM:296.24", 
"ICD9-CM:296.25", "ICD9-CM:296.26", "ICD9-CM:298.0", "ICD9-CM:300.4", "ICD8:296.2", "ICD8:298.0", "ICD8:300.4"]

# Intellectual Disability / Mental retardation (ID)
ID_Codes = ["ICD10:F70", "ICD10:F70.0", "ICD10:F70.1", "ICD10:F70.8", "ICD10:F70.9", "ICD10:F71", "ICD10:F71.0", "ICD10:F71.1", 
    "ICD10:F71.8", "ICD10:F71.9", "ICD10:F72", "ICD10:F72.0", "ICD10:F72.1", "ICD10:F72.8", "ICD10:F72.9", "ICD10:F73", "ICD10:F73.0", 
    "ICD10:F73.1", "ICD10:F73.8", "ICD10:F73.9", "ICD10:F74", "ICD10:F74.0", "ICD10:F74.1", "ICD10:F74.8", "ICD10:F74.9", "ICD10:F75", 
    "ICD10:F75.0", "ICD10:F75.1", "ICD10:F75.8", "ICD10:F75.9", "ICD10:F76", "ICD10:F76.0", "ICD10:F76.1", "ICD10:F76.8", "ICD10:F76.9", 
    "ICD10:F77", "ICD10:F77.0", "ICD10:F77.1", "ICD10:F77.8", "ICD10:F77.9", "ICD10:F78", "ICD10:F78.0", "ICD10:F78.1", "ICD10:F78.8", 
    "ICD10:F78.9", "ICD10:F79", "ICD10:F79.0", "ICD10:F79.1", "ICD10:F79.8", "ICD10:F79.9", "ICD10-CM:F70", "ICD10-CM:F70.0", "ICD10-CM:F70.1", 
    "ICD10-CM:F70.8", "ICD10-CM:F70.9", "ICD10-CM:F71", "ICD10-CM:F71.0", "ICD10-CM:F71.1", "ICD10-CM:F71.8", "ICD10-CM:F71.9", "ICD10-CM:F72", 
    "ICD10-CM:F72.0", "ICD10-CM:F72.1", "ICD10-CM:F72.8", "ICD10-CM:F72.9", "ICD10-CM:F73", "ICD10-CM:F73.0", "ICD10-CM:F73.1", "ICD10-CM:F73.8",
    "ICD10-CM:F73.9", "ICD10-CM:F74", "ICD10-CM:F74.0", "ICD10-CM:F74.1", "ICD10-CM:F74.8", "ICD10-CM:F74.9", "ICD10-CM:F75", "ICD10-CM:F75.0", 
    "ICD10-CM:F75.1", "ICD10-CM:F75.8", "ICD10-CM:F75.9", "ICD10-CM:F76", "ICD10-CM:F76.0", "ICD10-CM:F76.1", "ICD10-CM:F76.8", "ICD10-CM:F76.9", 
    "ICD10-CM:F77", "ICD10-CM:F77.0", "ICD10-CM:F77.1", "ICD10-CM:F77.8", "ICD10-CM:F77.9", "ICD10-CM:F78", "ICD10-CM:F78.0", "ICD10-CM:F78.1", 
    "ICD10-CM:F78.8", "ICD10-CM:F78.9", "ICD10-CM:F79", "ICD10-CM:F79.0", "ICD10-CM:F79.1", "ICD10-CM:F79.8", "ICD10-CM:F79.9", "ICD9:317", 
    "ICD9:318.0", "ICD9:318.1", "ICD9:318.2", "ICD9:319", "ICD9-CM:317", "ICD9-CM:318.0", "ICD9-CM:318.1", "ICD9-CM:318.2", "ICD9-CM:319", 
    "ICD8:310", "ICD8:310.0", "ICD8:310.1", "ICD8:310.2", "ICD8:310.3", "ICD8:310.4", "ICD8:310.5", "ICD8:310.7", "ICD8:310.8", "ICD8:310.9", 
    "ICD8:311", "ICD8:311.0", "ICD8:311.1", "ICD8:311.2", "ICD8:311.3", "ICD8:311.4", "ICD8:311.5", "ICD8:311.7", "ICD8:311.8", "ICD8:311.9", 
    "ICD8:312", "ICD8:312.0", "ICD8:312.1", "ICD8:312.2", "ICD8:312.3", "ICD8:312.4", "ICD8:312.5", "ICD8:312.7", "ICD8:312.8", "ICD8:312.9", 
    "ICD8:313", "ICD8:313.0", "ICD8:313.1", "ICD8:313.2", "ICD8:313.3", "ICD8:313.4", "ICD8:313.5", "ICD8:313.7", "ICD8:313.8", "ICD8:313.9", 
    "ICD8:314", "ICD8:314.0", "ICD8:314.1", "ICD8:314.2", "ICD8:314.3", "ICD8:314.4", "ICD8:314.5", "ICD8:314.7", "ICD8:314.8", "ICD8:314.9", 
    "ICD8:315", "ICD8:315.0", "ICD8:315.1", "ICD8:315.2", "ICD8:315.3", "ICD8:315.4", "ICD8:315.5", "ICD8:315.7", "ICD8:315.8", "ICD8:315.9",
    ]

# Schizophrenia (SCZ)
SCZ_Codes =  ["ICD10:F20", "ICD10:F20.0", "ICD10:F20.1", "ICD10:F20.2", "ICD10:F20.3", "ICD10:F20.5", "ICD10:F20.6", "ICD10:F20.8", "ICD10:F20.9",
    "ICD10-CM:F20", "ICD10-CM:F20.0", "ICD10-CM:F20.1", "ICD10-CM:F20.2", "ICD10-CM:F20.3", "ICD10-CM:F20.5", "ICD10-CM:F20.6", "ICD10-CM:F20.8", 
    "ICD10-CM:F20.9", "ICD9:295", "ICD9:295.0", "ICD9:295.00", "ICD9:295.01", "ICD9:295.02", "ICD9:295.03", "ICD9:295.04", "ICD9:295.05", 
    "ICD9:295.1", "ICD9:295.10", "ICD9:295.11", "ICD9:295.12", "ICD9:295.13", "ICD9:295.14", "ICD9:295.15", "ICD9:295.2", "ICD9:295.20", 
    "ICD9:295.21", "ICD9:295.22", "ICD9:295.23", "ICD9:295.24", "ICD9:295.25", "ICD9:295.3", "ICD9:295.30", "ICD9:295.31", "ICD9:295.32", 
    "ICD9:295.33", "ICD9:295.34", "ICD9:295.35", "ICD9:295.40", "ICD9:295.41", "ICD9:295.42", "ICD9:295.43", "ICD9:295.44", "ICD9:295.45", 
    "ICD9:295.5", "ICD9:295.50", "ICD9:295.51", "ICD9:295.52", "ICD9:295.53", "ICD9:295.54", "ICD9:295.55", "ICD9:295.6", "ICD9:295.60", "ICD9:295.61", 
    "ICD9:295.62", "ICD9:295.63", "ICD9:295.64", "ICD9:295.65", "ICD9:295.7", "ICD9:295.70", "ICD9:295.71", "ICD9:295.72", "ICD9:295.73", "ICD9:295.74", 
    "ICD9:295.75", "ICD9:295.8", "ICD9:295.80", "ICD9:295.81", "ICD9:295.82", "ICD9:295.83", "ICD9:295.84", "ICD9:295.85", "ICD9:295.9", "ICD9:295.90", 
    "ICD9:295.91", "ICD9:295.92", "ICD9:295.93", "ICD9:295.94", "ICD9:295.95", "ICD9-CM:295", "ICD9-CM:295.0", "ICD9-CM:295.00", "ICD9-CM:295.01", 
    "ICD9-CM:295.02", "ICD9-CM:295.03", "ICD9-CM:295.04", "ICD9-CM:295.05", "ICD9-CM:295.1", "ICD9-CM:295.10", "ICD9-CM:295.11", "ICD9-CM:295.12", 
    "ICD9-CM:295.13", "ICD9-CM:295.14", "ICD9-CM:295.15", "ICD9-CM:295.2", "ICD9-CM:295.20", "ICD9-CM:295.21", "ICD9-CM:295.22", "ICD9-CM:295.23", 
    "ICD9-CM:295.24", "ICD9-CM:295.25", "ICD9-CM:295.3", "ICD9-CM:295.30", "ICD9-CM:295.31", "ICD9-CM:295.32", "ICD9-CM:295.33", "ICD9-CM:295.34", 
    "ICD9-CM:295.35", "ICD9-CM:295.40", "ICD9-CM:295.41", "ICD9-CM:295.42", "ICD9-CM:295.43", "ICD9-CM:295.44", "ICD9-CM:295.45", "ICD9-CM:295.5", 
    "ICD9-CM:295.50", "ICD9-CM:295.51", "ICD9-CM:295.52", "ICD9-CM:295.53", "ICD9-CM:295.54", "ICD9-CM:295.55", "ICD9-CM:295.6", "ICD9-CM:295.60", 
    "ICD9-CM:295.61", "ICD9-CM:295.62", "ICD9-CM:295.63", "ICD9-CM:295.64", "ICD9-CM:295.65", "ICD9-CM:295.7", "ICD9-CM:295.70", "ICD9-CM:295.71", 
    "ICD9-CM:295.72", "ICD9-CM:295.73", "ICD9-CM:295.74", "ICD9-CM:295.75", "ICD9-CM:295.8", "ICD9-CM:295.80", "ICD9-CM:295.81", "ICD9-CM:295.82", 
    "ICD9-CM:295.83", "ICD9-CM:295.84", "ICD9-CM:295.85", "ICD9-CM:295.9", "ICD9-CM:295.90", "ICD9-CM:295.91", "ICD9-CM:295.92", "ICD9-CM:295.93", 
    "ICD9-CM:295.94", "ICD9-CM:295.95", "ICD9-CM:V11.0", "ICD8:295", "ICD8:295.0", "ICD8:295.09", "ICD8:295.1", "ICD8:295.19", "ICD8:295.2", 
    "ICD8:295.29", "ICD8:295.3", "ICD8:295.39", "ICD8:295.4", "ICD8:295.49", "ICD8:295.5", "ICD8:295.59", "ICD8:295.6", "ICD8:295.69", 
    "ICD8:295.8", "ICD8:295.89", "ICD8:295.9", "ICD8:295.99"]

# Bipolar Disorder (BPD)
BPD_Codes = ["ICD10:F30", "ICD10:F30.0", "ICD10:F30.1", "ICD10:F30.2", "ICD10:F30.8", "ICD10:F30.9", "ICD10:F31", "ICD10:F31.0", "ICD10:F31.1", 
    "ICD10:F31.2", "ICD10:F31.3", "ICD10:F31.4", "ICD10:F31.5", "ICD10:F31.6", "ICD10:F31.7", "ICD10:F31.8", "ICD10:F31.9", "ICD10-CM:F30", 
    "ICD10-CM:F30.0", "ICD10-CM:F30.1", "ICD10-CM:F30.2", "ICD10-CM:F30.8", "ICD10-CM:F30.9", "ICD10-CM:F31", "ICD10-CM:F31.0", "ICD10-CM:F31.1", 
    "ICD10-CM:F31.2", "ICD10-CM:F31.3", "ICD10-CM:F31.4", "ICD10-CM:F31.5", "ICD10-CM:F31.6", "ICD10-CM:F31.7", "ICD10-CM:F31.8", "ICD10-CM:F31.9",
    "ICD9:296.0", "ICD9:296.00", "ICD9:296.01", "ICD9:296.02", "ICD9:296.03", "ICD9:296.04", "ICD9:296.05", "ICD9:296.06", "ICD9:296.1", "ICD9:296.10", 
    "ICD9:296.11", "ICD9:296.12", "ICD9:296.13", "ICD9:296.14", "ICD9:296.15", "ICD9:296.16", "ICD9:296.4", "ICD9:296.40", "ICD9:296.41", "ICD9:296.42", 
    "ICD9:296.43", "ICD9:296.44", "ICD9:296.45", "ICD9:296.46", "ICD9:296.5", "ICD9:296.50", "ICD9:296.51", "ICD9:296.52", "ICD9:296.53", "ICD9:296.54", 
    "ICD9:296.55", "ICD9:296.56", "ICD9:296.6", "ICD9:296.60", "ICD9:296.61", "ICD9:296.62", "ICD9:296.63", "ICD9:296.64", "ICD9:296.65", "ICD9:296.66", 
    "ICD9:296.7", "ICD9:296.70", "ICD9:296.8", "ICD9:296.80", "ICD9:296.81", "ICD9:296.82", "ICD9:296.89", "ICD9-CM:296.0", "ICD9-CM:296.00", 
    "ICD9-CM:296.01", "ICD9-CM:296.02", "ICD9-CM:296.03", "ICD9-CM:296.04", "ICD9-CM:296.05", "ICD9-CM:296.06", "ICD9-CM:296.1", "ICD9-CM:296.10", 
    "ICD9-CM:296.11", "ICD9-CM:296.12", "ICD9-CM:296.13", "ICD9-CM:296.14", "ICD9-CM:296.15", "ICD9-CM:296.16", "ICD9-CM:296.4", "ICD9-CM:296.40", 
    "ICD9-CM:296.41", "ICD9-CM:296.42", "ICD9-CM:296.43", "ICD9-CM:296.44", "ICD9-CM:296.45", "ICD9-CM:296.46", "ICD9-CM:296.5", "ICD9-CM:296.50", 
    "ICD9-CM:296.51", "ICD9-CM:296.52", "ICD9-CM:296.53", "ICD9-CM:296.54", "ICD9-CM:296.55", "ICD9-CM:296.56", "ICD9-CM:296.6", "ICD9-CM:296.60", 
    "ICD9-CM:296.61", "ICD9-CM:296.62", "ICD9-CM:296.63", "ICD9-CM:296.64", "ICD9-CM:296.65", "ICD9-CM:296.66", "ICD9-CM:296.7", "ICD9-CM:296.70", 
    "ICD9-CM:296.8", "ICD9-CM:296.80", "ICD9-CM:296.81", "ICD9-CM:296.82", "ICD9-CM:296.89", "ICD8:296.1", "ICD8:296.19", "ICD8:296.2", "ICD8:296.29", 
    "ICD8:296.3", "ICD8:296.39", "ICD8:296.8", "ICD8:296.89", "ICD8:296.9", "ICD8:296.99"]

# Dementia (DEM)
DEM_Codes = ["ICD10:F00", "ICD10:F00.0", "ICD10:F00.1", "ICD10:F00.2", "ICD10:F00.9", "ICD10:F01.1", "ICD10:F01.2", "ICD10:F01.3", "ICD10:F01.8", 
    "ICD10:F01.9", "ICD10:F02.0", "ICD10:F02.1", "ICD10:F02.2", "ICD10:F02.3", "ICD10:F02.4", "ICD10:F02.8", "ICD10:F03", "ICD10-CM:F00", "ICD10-CM:F00.0", 
    "ICD10-CM:F00.1", "ICD10-CM:F00.2", "ICD10-CM:F00.9", "ICD10-CM:F01.1", "ICD10-CM:F01.2", "ICD10-CM:F01.3", "ICD10-CM:F01.8", "ICD10-CM:F01.9", 
    "ICD10-CM:F02.0", "ICD10-CM:F02.1", "ICD10-CM:F02.2", "ICD10-CM:F02.3", "ICD10-CM:F02.4", "ICD10-CM:F02.8", "ICD10-CM:F03", "ICD9:290.0", 
    "ICD9:290.1", "ICD9:290.2", "ICD9:290.4", "ICD9:290.8", "ICD9:290.9", "ICD9:294.1", "ICD9-CM:290.0", "ICD9-CM:290.1", "ICD9-CM:290.2", "ICD9-CM:290.4", 
    "ICD9-CM:290.8", "ICD9-CM:290.9", "ICD9-CM:294.1", "ICD8:290", "ICD8:290.0", "ICD8:290.1", "ICD8:293.0", "ICD8:293.2", "ICD8:293.4", "ICD8:293.9", "ICD8:294.8"]

# Alcohol use Disorder (AUD)
AUD_Codes = ["ICD10:F10.2", "ICD10-CM:F10.2", "ICD9:303", "ICD9:303.0", "ICD9:303.1", "ICD9:303.2", "ICD9:303.9",
    "ICD9-CM:303", "ICD9-CM:303.0", "ICD9-CM:303.1", "ICD9-CM:303.2", "ICD9-CM:303.9", "ICD8:303.2"]

# Drug use Disorder (DUD)
DUD_Codes = ["ICD10:F11.2", "ICD10:F12.2", "ICD10:F13.2", "ICD10:F14.2", "ICD10:F15.2", "ICD10:F16.2", "ICD10:F18.2", "ICD10:F19.2", "ICD10-CM:F11.2", 
    "ICD10-CM:F12.2", "ICD10-CM:F13.2", "ICD10-CM:F14.2", "ICD10-CM:F15.2", "ICD10-CM:F16.2", "ICD10-CM:F18.2", "ICD10-CM:F19.2", "ICD9:304.0", 
    "ICD9:304.3", "ICD9:304.1", "ICD9:304.2", "ICD9:304.4", "ICD9:304.5", "ICD9:304.6", "ICD9:304.7", "ICD9-CM:304.0", "ICD9-CM:304.3", "ICD9-CM:304.1", 
    "ICD9-CM:304.2", "ICD9-CM:304.4", "ICD9-CM:304.5", "ICD9-CM:304.6", "ICD9-CM:304.7", "ICD8:304.0", "ICD8:304.09", "ICD8:304.19", "ICD8:304.5", 
    "ICD8:304.59", "ICD8:304.2", "ICD8:304.29", "ICD8:304.39", "ICD8:304.4", "ICD8:304.49", "ICD8:304.69", "ICD8:304.7", "ICD8:304.79", "ICD8:304.89", 
    "ICD8:304.99", "ICD8:304.8"]

# Mild cognitive Impairment (MCI)
MCI_Codes = ["ICD10:G31.84", "ICD10:F06.7", "ICD10-CM:G31.84", "ICD10-CM:F06.7", "ICD9:331.83", "ICD9:310.1",
    "ICD9-CM:331.83", "ICD9-CM:310.1"]

# Concurrent terminal illness (CTI)
CTI_Codes = ["ICD10:C25", "ICD10:C25.0", "ICD10:C25.1", "ICD10:C25.2", "ICD10:C25.3", "ICD10:C25.4", "ICD10:C25.7", "ICD10:C25.8", 
    "ICD10:C25.9", "ICD10:C22", "ICD10:C22.0", "ICD10:C22.1", "ICD10:C22.2", "ICD10:C22.3", "ICD10:C22.4", "ICD10:C22.7", "ICD10:C22.8", 
    "ICD10:C22.9", "ICD10:C34", "ICD10:C34.0", "ICD10:C34.1", "ICD10:C34.2", "ICD10:C34.3", "ICD10:C34.8", "ICD10:C34.9", "ICD10:C71", 
    "ICD10:C71.0", "ICD10:C71.1", "ICD10:C71.2", "ICD10:C71.3", "ICD10:C71.4", "ICD10:C71.5", "ICD10:C71.6", "ICD10:C71.7", "ICD10:C71.8", 
    "ICD10:C71.9", "ICD10:C15", "ICD10:C15.0", "ICD10:C15.1", "ICD10:C15.2", "ICD10:C15.3", "ICD10:C15.4", "ICD10:C15.5", "ICD10:C15.8", 
    "ICD10:C15.9", "ICD10:C16", "ICD10:C16.0", "ICD10:C16.1", "ICD10:C16.2", "ICD10:C16.3", "ICD10:C16.4", "ICD10:C16.5", "ICD10:C16.6", 
    "ICD10:C16.7", "ICD10:C16.8", "ICD10:C16.9", "ICD10:C50", "ICD10:C50.0", "ICD10:C50.1", "ICD10:C50.2", "ICD10:C50.3", "ICD10:C50.4", 
    "ICD10:C50.5", "ICD10:C50.6", "ICD10:C50.8", "ICD10:C50.9", "ICD10:C61", "ICD10:C61.9", "ICD10:C67", "ICD10:C67.0", "ICD10:C67.1", 
    "ICD10:C67.2", "ICD10:C67.3", "ICD10:C67.4", "ICD10:C67.5", "ICD10:C67.6", "ICD10:C67.7", "ICD10:C67.8", "ICD10:C67.9", "ICD10:C64", 
    "ICD10:C64.0", "ICD10:C64.1", "ICD10:C64.2", "ICD10:C64.8", "ICD10:C64.9", "ICD10:C56", "ICD10:C56.9", "ICD10:C79.9", "ICD10:C77.9", 
    "ICD10:G30", "ICD10:G30.0", "ICD10:G30.1", "ICD10:G30.8", "ICD10:G30.9", "ICD10:G12.21", "ICD10:G12.2G", "ICD10:N18.5", "ICD10:I50", 
    "ICD10:I50.0", "ICD10:I50.1", "ICD10:I50.2", "ICD10:I50.3", "ICD10:I50.4", "ICD10:I50.8", "ICD10:I50.9", "ICD10:J44", "ICD10:J44.0", 
    "ICD10:J44.1", "ICD10:J44.8", "ICD10:J44.9", "ICD10:C18", "ICD10:C18.0", "ICD10:C18.1", "ICD10:C18.2", "ICD10:C18.3", "ICD10:C18.4", 
    "ICD10:C18.5", "ICD10:C18.6", "ICD10:C18.7", "ICD10:C18.8", "ICD10:C18.9", "ICD10:G20", "ICD10:G20.9", "ICD10:G20.9A", "ICD10:G35", 
    "ICD10:G35.9", "ICD10:G35.9A", "ICD10:G35.9B", "ICD10:G35.9C", "ICD10:K72.9", "ICD10:K72.90", "ICD10:K72.9A", "ICD10:K72.10", 
    "ICD10:K72.11", "ICD10:K74.6", "ICD10:K74.60", "ICD10:K74.6A", "ICD10:K74.6B", "ICD10:K74.6C", "ICD10:K74.6D", "ICD10:K74.6E", 
    "ICD10:K74.6F", "ICD10:K74.6G", "ICD10:K74.6H", "ICD10:K70.3", "ICD10:K70.30", "ICD10:K70.3A", "ICD10:K74.69", "ICD10-CM:C25.0", 
    "ICD10-CM:C25.1", "ICD10-CM:C25.2", "ICD10-CM:C25.3", "ICD10-CM:C25.4", "ICD10-CM:C25.7", "ICD10-CM:C25.8", "ICD10-CM:C25.9", "ICD10-CM:C22.0", 
    "ICD10-CM:C22.1", "ICD10-CM:C22.2", "ICD10-CM:C22.3", "ICD10-CM:C22.4", "ICD10-CM:C22.7", "ICD10-CM:C22.8", "ICD10-CM:C22.9", "ICD10-CM:C34", 
    "ICD10-CM:C34.0", "ICD10-CM:C34.1", "ICD10-CM:C34.2", "ICD10-CM:C34.3", "ICD10-CM:C34.8", "ICD10-CM:C34.9", "ICD10-CM:C71", "ICD10-CM:C71.0", 
    "ICD10-CM:C71.1", "ICD10-CM:C71.2", "ICD10-CM:C71.3", "ICD10-CM:C71.4", "ICD10-CM:C71.5", "ICD10-CM:C71.6", "ICD10-CM:C71.7", 
    "ICD10-CM:C71.8", "ICD10-CM:C71.9", "ICD10-CM:C15", "ICD10-CM:C15.0", "ICD10-CM:C15.1", "ICD10-CM:C15.2", "ICD10-CM:C15.3", "ICD10-CM:C15.4", 
    "ICD10-CM:C15.5", "ICD10-CM:C15.8", "ICD10-CM:C15.9", "ICD10-CM:C16", "ICD10-CM:C16.0", "ICD10-CM:C16.1", "ICD10-CM:C16.2", "ICD10-CM:C16.3", 
    "ICD10-CM:C16.4", "ICD10-CM:C16.5", "ICD10-CM:C16.6", "ICD10-CM:C16.7", "ICD10-CM:C16.8", "ICD10-CM:C16.9", "ICD10-CM:C50", "ICD10-CM:C50.0", 
    "ICD10-CM:C50.1", "ICD10-CM:C50.2", "ICD10-CM:C50.3", "ICD10-CM:C50.4", "ICD10-CM:C50.5", "ICD10-CM:C50.6", "ICD10-CM:C50.8", 
    "ICD10-CM:C50.9", "ICD10-CM:C61", "ICD10-CM:C61.9", "ICD10-CM:C67", "ICD10-CM:C67.0", "ICD10-CM:C67.1", "ICD10-CM:C67.2", "ICD10-CM:C67.3", "ICD10-CM:C67.4", 
    "ICD10-CM:C67.5", "ICD10-CM:C67.6", "ICD10-CM:C67.7", "ICD10-CM:C67.8", "ICD10-CM:C67.9,C64", "ICD10-CM:C64.0", "ICD10-CM:C64.1", 
    "ICD10-CM:C64.2", "ICD10-CM:C64.8", "ICD10-CM:C64.9", "ICD10-CM:C56", "ICD10-CM:C56.9", "ICD10-CM:C79.9", "ICD10-CM:C77.9,G30", "ICD10-CM:G30.0", "ICD10-CM:G30.1", 
    "ICD10-CM:G30.8", "ICD10-CM:G30.9", "ICD10-CM:G12.21", "ICD10-CM:G12.2A", "ICD10-CM:N18.6", "ICD10-CM:I50", "ICD10-CM:I50.0", "ICD10-CM:I50.1", "ICD10-CM:I50.2", "ICD10-CM:I50.3", 
    "ICD10-CM:I50.4", "ICD10-CM:I50.8", "ICD10-CM:I50.9", "ICD10-CM:J44", "ICD10-CM:J44.0", "ICD10-CM:J44.1", "ICD10-CM:J44.8", "ICD10-CM:J44.9", "ICD10-CM:C18", 
    "ICD10-CM:C18.0", "ICD10-CM:C18.1", "ICD10-CM:C18.2", "ICD10-CM:C18.3", "ICD10-CM:C18.4", "ICD10-CM:C18.5", "ICD10-CM:C18.6", "ICD10-CM:C18.7", 
    "ICD10-CM:C18.8", "ICD10-CM:C18.9", "ICD10-CM:G20", "ICD10-CM:G20.A", "ICD10-CM:G20.A1", "ICD10-CM:G20.A2", "ICD10-CM:G20.B", "ICD10-CM:G20.B1", 
    "ICD10-CM:G20.B2", "ICD10-CM:G20.C", "ICD10-CM:G35", "ICD10-CM:K72.9", "ICD10-CM:K72.90", "ICD10-CM:K72.91,K72.1", "ICD10-CM:K72.10", "ICD10-CM:K72.11", 
    "ICD10-CM:K72.90", "ICD10-CM:K74.60", "ICD10-CM:K74.69", "ICD10-CM:K70.30", "ICD10-CM:K70.31", "ICD10-CM:K74.60", "ICD10-CM:K74.69",
    "ICD9:157", "ICD9:157.0", "ICD9:157.1", "ICD9:157.2", "ICD9:157.3", "ICD9:157.4", "ICD9:157.8", "ICD9:157.9", "ICD9:155", "ICD9:155.0", 
    "ICD9:155.1", "ICD9:155.2", "ICD9:162", "ICD9:162.0", "ICD9:162.2", "ICD9:162.3", "ICD9:162.4", "ICD9:162.5", "ICD9:162.8", 
    "ICD9:162.9", "ICD9:191", "ICD9:191.0", "ICD9:191.1", "ICD9:191.2", "ICD9:191.3", "ICD9:191.4", "ICD9:191.5", "ICD9:191.6", 
    "ICD9:191.7", "ICD9:191.8", "ICD9:191.9", "ICD9:150", "ICD9:150.0", "ICD9:150.1", "ICD9:150.2", "ICD9:150.3", "ICD9:150.4", 
    "ICD9:150.5", "ICD9:150.8", "ICD9:150.9", "ICD9:151", "ICD9:151.0", "ICD9:151.1", "ICD9:151.2", "ICD9:151.3", "ICD9:151.4", 
    "ICD9:151.5", "ICD9:151.6", "ICD9:151.7", "ICD9:151.8", "ICD9:151.9", "ICD9:174", "ICD9:174.0", "ICD9:174.1", "ICD9:174.2", 
    "ICD9:174.3", "ICD9:174.4", "ICD9:174.5", "ICD9:174.6", "ICD9:174.8", "ICD9:174.9", "ICD9:185", "ICD9:188", "ICD9:188.0", 
    "ICD9:188.1", "ICD9:188.2", "ICD9:188.3", "ICD9:188.4", "ICD9:188.5", "ICD9:188.6", "ICD9:188.7", "ICD9:188.8", "ICD9:188.9", 
    "ICD9:189", "ICD9:189.0", "ICD9:189.1", "ICD9:189.2", "ICD9:189.3", "ICD9:189.4", "ICD9:189.8", "ICD9:189.9", "ICD9:183", 
    "ICD9:183.0", "ICD9:183.2", "ICD9:183.3", "ICD9:183.4", "ICD9:183.5", "ICD9:183.8", "ICD9:183.9", "ICD9:199", "ICD9:331", 
    "ICD9:331.0", "ICD9:331.1", "ICD9:331.2", "ICD9:331.9", "ICD9:335.2", "ICD9:335.20", "ICD9:335.21", "ICD9:335.22", "ICD9:335.23", 
    "ICD9:335.24", "ICD9:335.29", "ICD9:586", "ICD9:428", "ICD9:428.0", "ICD9:428.1", "ICD9:428.2", "ICD9:428.3", "ICD9:428.4", 
    "ICD9:428.9", "ICD9:496", "ICD9:153", "ICD9:153.0", "ICD9:153.1", "ICD9:153.2", "ICD9:153.3", "ICD9:153.4", "ICD9:153.5", 
    "ICD9:153.6", "ICD9:153.7", "ICD9:153.8", "ICD9:153.9", "ICD9:332", "ICD9:332.0", "ICD9:332.1", "ICD9:340", "ICD9:572.8", 
    "ICD9:571.9", "ICD9:571.2", "ICD9:571.5", "ICD9:571.9", "ICD9-CM:157", "ICD9-CM:157.0", "ICD9-CM:157.1", "ICD9-CM:157.2", "ICD9-CM:157.3", 
    "ICD9-CM:157.4", "ICD9-CM:157.8", "ICD9-CM:157.9", "ICD9-CM:155", "ICD9-CM:155.0", "ICD9-CM:155.1", "ICD9-CM:155.2", "ICD9-CM:162", 
    "ICD9-CM:162.0", "ICD9-CM:162.2", "ICD9-CM:162.3", "ICD9-CM:162.4", "ICD9-CM:162.5", "ICD9-CM:162.8", "ICD9-CM:162.9", "ICD9-CM:191", 
    "ICD9-CM:191.0", "ICD9-CM:191.1", "ICD9-CM:191.2", "ICD9-CM:191.3", "ICD9-CM:191.4", "ICD9-CM:191.5", "ICD9-CM:191.6", "ICD9-CM:191.7", 
    "ICD9-CM:191.8", "ICD9-CM:191.9", "ICD9-CM:150", "ICD9-CM:150.0", "ICD9-CM:150.1", "ICD9-CM:150.2", "ICD9-CM:150.3", "ICD9-CM:150.4", 
    "ICD9-CM:150.5", "ICD9-CM:150.8", "ICD9-CM:150.9", "ICD9-CM:151", "ICD9-CM:151.0", "ICD9-CM:151.1", "ICD9-CM:151.2", "ICD9-CM:151.3", 
    "ICD9-CM:151.4", "ICD9-CM:151.5", "ICD9-CM:151.6", "ICD9-CM:151.7", "ICD9-CM:151.8", "ICD9-CM:151.9", "ICD9-CM:174", "ICD9-CM:174.0", 
    "ICD9-CM:174.1", "ICD9-CM:174.2", "ICD9-CM:174.3", "ICD9-CM:174.4", "ICD9-CM:174.5", "ICD9-CM:174.6", "ICD9-CM:174.8", "ICD9-CM:174.9", 
    "ICD9-CM:185", "ICD9-CM:188", "ICD9-CM:188.0", "ICD9-CM:188.1", "ICD9-CM:188.2", "ICD9-CM:188.3", "ICD9-CM:188.4", "ICD9-CM:188.5", 
    "ICD9-CM:188.6", "ICD9-CM:188.7", "ICD9-CM:188.8", "ICD9-CM:188.9", "ICD9-CM:189", "ICD9-CM:189.0", "ICD9-CM:189.1", "ICD9-CM:189.2", 
    "ICD9-CM:189.3", "ICD9-CM:189.4", "ICD9-CM:189.8", "ICD9-CM:189.9", "ICD9-CM:183", "ICD9-CM:183.0", "ICD9-CM:183.2", "ICD9-CM:183.3", 
    "ICD9-CM:183.4", "ICD9-CM:183.5", "ICD9-CM:183.8", "ICD9-CM:183.9", "ICD9-CM:199", "ICD9-CM:331", "ICD9-CM:331.0", "ICD9-CM:331.1", 
    "ICD9-CM:331.2", "ICD9-CM:331.9", "ICD9-CM:335.2", "ICD9-CM:335.20", "ICD9-CM:335.21", "ICD9-CM:335.22", "ICD9-CM:335.23", "ICD9-CM:335.24", 
    "ICD9-CM:335.29", "ICD9-CM:586", "ICD9-CM:428", "ICD9-CM:428.0", "ICD9-CM:428.1", "ICD9-CM:428.2", "ICD9-CM:428.3", "ICD9-CM:428.4", 
    "ICD9-CM:428.9", "ICD9-CM:496", "ICD9-CM:153", "ICD9-CM:153.0", "ICD9-CM:153.1", "ICD9-CM:153.2", "ICD9-CM:153.3", "ICD9-CM:153.4", 
    "ICD9-CM:153.5", "ICD9-CM:153.6", "ICD9-CM:153.7", "ICD9-CM:153.8", "ICD9-CM:153.9", "ICD9-CM:332", "ICD9-CM:332.0", "ICD9-CM:332.1", 
    "ICD9-CM:340", "ICD9-CM:572.8", "ICD9-CM:572.8", "ICD9-CM:572.8", "ICD9-CM:572.8", "ICD9-CM:571.9", "ICD9-CM:571.2", "ICD9-CM:571.5", 
    "ICD9-CM:571.9", "ICD8:157", "ICD8:157.09", "ICD8:157.80", "ICD8:157.81", "ICD8:157.89", 
    "ICD8:157.99", "ICD8:155", "ICD8:155.09", "ICD8:155.19", "ICD8:155.89", "ICD8:162", "ICD8:162.09", "ICD8:162.10", "ICD8:162.11", 
    "ICD8:162.12", "ICD8:162.13", "ICD8:162.14", "ICD8:162.15", "ICD8:162.16", "ICD8:162.18", "ICD8:162.19", "ICD8:191", "ICD8:191.00", 
    "ICD8:191.01", "ICD8:191.02", "ICD8:191.03", "ICD8:191.04", "ICD8:191.05", "ICD8:191.06", "ICD8:191.07", "ICD8:191.08", 
    "ICD8:191.09", "ICD8:150", "ICD8:150.00", "ICD8:150.01", "ICD8:150.02", "ICD8:150.08", "ICD8:150.09", "ICD8:151", "ICD8:151.09", 
    "ICD8:151.19", "ICD8:151.80", "ICD8:151.81", "ICD8:151.82", "ICD8:151.89", "ICD8:151.99", "ICD8:170", "ICD8:170.09", "ICD8:170.19", 
    "ICD8:170.29", "ICD8:170.39", "ICD8:170.49", "ICD8:170.59", "ICD8:170.69", "ICD8:170.79", "ICD8:170.89", "ICD8:170.99", "ICD8:185",
    "ICD8:185.09", "ICD8:188", "ICD8:188.00", "ICD8:188.01", "ICD8:188.02", "ICD8:188.08", "ICD8:188.09", "ICD8:189", "ICD8:189.09",
    "ICD8:189.19", "ICD8:189.29", "ICD8:189.90", "ICD8:189.91", "ICD8:189.92", "ICD8:189.99", "ICD8:183", "ICD8:183.00", "ICD8:183.01", 
    "ICD8:183.02", "ICD8:183.03", "ICD8:183.08", "ICD8:183.09", "ICD8:183.19", "ICD8:183.99", "ICD8:198.99", "ICD8:196.99", "ICD8:290.1", 
    "ICD8:290.10", "ICD8:348", "ICD8:348.09", "ICD8:348.19", "ICD8:348.20", "ICD8:348.29", "ICD8:348.99", "ICD8:792.9", "ICD8:792.99", 
    "ICD8:427", "ICD8:427.09", "ICD8:427.10", "ICD8:427.11", "ICD8:427.19", "ICD8:427.20", "ICD8:427.21", "ICD8:427.22", "ICD8:427.23", 
    "ICD8:427.24", "ICD8:427.25", "ICD8:427.26", "ICD8:427.27", "ICD8:427.28", "ICD8:427.29", "ICD8:427.90", "ICD8:427.91", "ICD8:427.92", 
    "ICD8:427.93", "ICD8:427.94", "ICD8:427.95", "ICD8:427.96", "ICD8:427.97", "ICD8:427.99", "ICD8:491", "ICD8:491.00", "ICD8:491.01", 
    "ICD8:491.02", "ICD8:491.03", "ICD8:491.04", "ICD8:491.08", "ICD8:491.09", "ICD8:153", "ICD8:153.00", "ICD8:153.01", "ICD8:153.02", 
    "ICD8:153.09", "ICD8:153.19", "ICD8:153.29", "ICD8:153.39", "ICD8:153.80", "ICD8:153.89", "ICD8:153.99", "ICD8:342", "ICD8:342.99", 
    "ICD8:348.09", "ICD8:348.19", "ICD8:348.20", "ICD8:348.29", "ICD8:348.99", "ICD8:427.09", "ICD8:571.92", "ICD8:571.99"]

# Anxiety Disorder (ANX)
ANX_Codes = ["ICD10:F40", "ICD10:F40.0", "ICD10:F40.1", "ICD10:F40.2", "ICD10:F40.8", "ICD10:F40.9", "ICD10:F41", "ICD10:F41.0", 
    "ICD10:F41.1", "ICD10:F41.2", "ICD10:F41.3", "ICD10:F41.8", "ICD10:F41.9", 
    "ICD8:300.0", "ICD8:300.2", "ICD8:300.00", "ICD8:300.20"]

# ADHD #DONT DO EXACT MAPPING!
ADHD_Codes = ["ICD10:F90", "ICD10:F90.0", "ICD10:F90.1", "ICD10:F90.2", "ICD10:F90.8", "ICD10:F90.9",
    "ICD10-CM:F90", "ICD10-CM:F90.0", "ICD10-CM:F90.1", "ICD10-CM:F90.2", "ICD10-CM:F90.8", "ICD10-CM:F90.9",
    "ICD9:314", "ICD9:314.0", "ICD9:314.00", "ICD9:314.01", "ICD9:314.1", "ICD9-CM:314", "ICD9-CM:314.0", "ICD9-CM:314.00", 
    "ICD9-CM:314.01", "ICD9-CM:314.1", "ICD8:308.01", "ICD8:308.02"]

# Autism (ASD)
ASD_Codes = ["ICD10:F84.0", "ICD10:F84.1", "ICD10:F84.5", "ICD10:F84.9", 
    "ICD10-CM:F84.0", "ICD10-CM:F84.1", "ICD10-CM:F84.5", "ICD10-CM:F84.9",
    "ICD9:299.0", "ICD9:299.00", "ICD9:299.01", "ICD9:299.1", "ICD9:299.10", "ICD9:299.11", "ICD9:299.8", "ICD9:299.80", 
    "ICD9:299.81", "ICD9:299.9", "ICD9:299.90", "ICD9:299.91", "ICD9-CM:299.0", "ICD9-CM:299.00", "ICD9-CM:299.01", 
    "ICD9-CM:299.1", "ICD9-CM:299.10", "ICD9-CM:299.11", "ICD9-CM:299.8", "ICD9-CM:299.80", "ICD9-CM:299.81", "ICD9-CM:299.9", 
    "ICD9-CM:299.90", "ICD9-CM:299.91", "ICD8:299.00", "ICD8:299.01", "ICD8:299.02", "ICD8:299.03"]

# Eating Disorder
Eating_Disorder_Codes = ["ICD10:F50.2", "ICD10:F50.3", "ICD10:F50.0", "ICD8:306.5"]

# Posttraumatic Stress Disorder (PTSD)
PTSD_Codes = ["ICD10:F43.1", "ICD10-CM:F43.1", "ICD9:309.8", "ICD9-CM:309.8"]

Dysthymia_Codes = ["ICD10:F34.1"]

# No exact mapping
Stressreaction_Codes = ["ICD10:F43.0", "ICD10:F43.2", "ICD10:F43.8", "ICD10:F43.9"]

# Obsessive-compulsive disorder (OCD)
OCD_Codes = ["ICD10:F42", "ICD10:F42.0", "ICD10:F42.1", "ICD10:F42.2", "ICD10:F42.8", "ICD10:F42.9",
    "ICD10-CM:F42", "ICD10-CM:F42.0", "ICD10-CM:F42.1", "ICD10-CM:F42.2", "ICD10-CM:F42.8", "ICD10-CM:F42.9",
    "ICD9:300.3", "ICD9-CM:300.3", "ICD8:300.3", "ICD8:300.39"]

# Chronic Pain (CP)
CP_Codes = ["ICD10:M79.60", "ICD10:M54.5", "ICD10:M54.2", "ICD10:G89.0", "ICD10:G89.4", "ICD10:G89.8", "ICD10:G89.29", "ICD10:G89.21", 
    "ICD10:M06.9", "ICD10:M15", "ICD10:M15.0", "ICD10:M15.1", "ICD10:M15.2", "ICD10:M15.3", "ICD10:M15.4", "ICD10:M15.8", "ICD10:M15.9", 
    "ICD10:M16", "ICD10:M16.0", "ICD10:M16.1", "ICD10:M16.2", "ICD10:M16.3", "ICD10:M16.4", "ICD10:M16.5", "ICD10:M16.6", "ICD10:M16.7", 
    "ICD10:M16.9", "ICD10:M17", "ICD10:M17.0", "ICD10:M17.1", "ICD10:M17.2", "ICD10:M17.3", "ICD10:M17.4", "ICD10:M17.5", "ICD10:M17.9", 
    "ICD10:M18", "ICD10:M18.0", "ICD10:M18.1", "ICD10:M18.2", "ICD10:M18.3", "ICD10:M18.4", "ICD10:M18.5", "ICD10:M18.9", "ICD10:M19", 
    "ICD10:M19.0", "ICD10:M19.1", "ICD10:M19.2", "ICD10:M19.8", "ICD10:M19.9", "ICD10:G90.50", "ICD10:G90.511", "ICD10:G53.0", "ICD10:D57.81", 
    "ICD10:G35", "ICD10:M45", "ICD10:M79.7", "ICD10:M05", "ICD10:M05.0", "ICD10:M05.1", "ICD10:M05.2", "ICD10:M05.3", "ICD10:M05.8", 
    "ICD10:M05.9", "ICD10:M06", "ICD10:M06.0", "ICD10:M06.1", "ICD10:M06.2", "ICD10:M06.3", "ICD10:M06.4", "ICD10:M06.8", "ICD10:M06.9",
    "ICD10-CM:M79.60", "ICD10-CM:M54.5", "ICD10-CM:M54.2", "ICD10-CM:G89.0", "ICD10-CM:G89.4", "ICD10-CM:G89.8", "ICD10-CM:G89.29", 
    "ICD10-CM:G89.21", "ICD10-CM:M06.9", "ICD10-CM:M15", "ICD10-CM:M15.0", "ICD10-CM:M15.1", "ICD10-CM:M15.2", "ICD10-CM:M15.3", 
    "ICD10-CM:M15.4", "ICD10-CM:M15.8", "ICD10-CM:M15.9", "ICD10-CM:M16", "ICD10-CM:M16.0", "ICD10-CM:M16.1", "ICD10-CM:M16.2", 
    "ICD10-CM:M16.3", "ICD10-CM:M16.4", "ICD10-CM:M16.5", "ICD10-CM:M16.6", "ICD10-CM:M16.7", "ICD10-CM:M16.9", "ICD10-CM:M17", 
    "ICD10-CM:M17.0", "ICD10-CM:M17.1", "ICD10-CM:M17.2", "ICD10-CM:M17.3", "ICD10-CM:M17.4", "ICD10-CM:M17.5", "ICD10-CM:M17.9", 
    "ICD10-CM:M18", "ICD10-CM:M18.0", "ICD10-CM:M18.1", "ICD10-CM:M18.2", "ICD10-CM:M18.3", "ICD10-CM:M18.4", "ICD10-CM:M18.5", "ICD10-CM:M18.9", 
    "ICD10-CM:M19", "ICD10-CM:M19.0", "ICD10-CM:M19.1", "ICD10-CM:M19.2", "ICD10-CM:M19.8", "ICD10-CM:M19.9", "ICD10-CM:G90.50", 
    "ICD10-CM:G90.511", "ICD10-CM:G53.0", "ICD10-CM:D57.81", "ICD10-CM:G35", "ICD10-CM:M45", "ICD10-CM:M79.7", "ICD10-CM:M05", "ICD10-CM:M05.0", 
    "ICD10-CM:M05.1", "ICD10-CM:M05.2", "ICD10-CM:M05.3", "ICD10-CM:M05.8", "ICD10-CM:M05.9", "ICD10-CM:M06", "ICD10-CM:M06.0", 
    "ICD10-CM:M06.1", "ICD10-CM:M06.2", "ICD10-CM:M06.3", "ICD10-CM:M06.4", "ICD10-CM:M06.8", "ICD10-CM:M06.9", "ICD9:338.2", "ICD9:724.2", 
    "ICD9:723.1", "ICD9:338.3", "ICD9:338.4", "ICD9:338.8", "ICD9:714.0", "ICD9:714.1", "ICD9:714.2", "ICD9:714.3", "ICD9:714.4", 
    "ICD9:714.5", "ICD9:714.6", "ICD9:714.7", "ICD9:714.8", "ICD9:714.9", "ICD9:715.0", "ICD9:715.1", "ICD9:715.2", "ICD9:715.3", 
    "ICD9:715.4", "ICD9:715.5", "ICD9:715.6", "ICD9:715.7", "ICD9:715.8", "ICD9:715.9", "ICD9:337.0", "ICD9:337.1", "ICD9:337.2", 
    "ICD9:337.3", "ICD9:337.4", "ICD9:337.5", "ICD9:337.6", "ICD9:337.7", "ICD9:337.8", "ICD9:337.9", "ICD9:337.22", "ICD9:053.12", 
    "ICD9:282.60", "ICD9:340", "ICD9:714.0", "ICD9-CM:338.2", "ICD9-CM:724.2", "ICD9-CM:723.1", "ICD9-CM:338.3", "ICD9-CM:338.4", 
    "ICD9-CM:338.8", "ICD9-CM:714.0", "ICD9-CM:714.1", "ICD9-CM:714.2", "ICD9-CM:714.3", "ICD9-CM:714.4", "ICD9-CM:714.5", "ICD9-CM:714.6", 
    "ICD9-CM:714.7", "ICD9-CM:714.8", "ICD9-CM:714.9", "ICD9-CM:715.0", "ICD9-CM:715.1", "ICD9-CM:715.2", "ICD9-CM:715.3", "ICD9-CM:715.4", 
    "ICD9-CM:715.5", "ICD9-CM:715.6", "ICD9-CM:715.7", "ICD9-CM:715.8", "ICD9-CM:715.9", "ICD9-CM:337.0", "ICD9-CM:337.1", "ICD9-CM:337.2", 
    "ICD9-CM:337.3", "ICD9-CM:337.4", "ICD9-CM:337.5", "ICD9-CM:337.6", "ICD9-CM:337.7", "ICD9-CM:337.8", "ICD9-CM:337.9", "ICD9-CM:337.22", 
    "ICD9-CM:053.12", "ICD9-CM:282.60", "ICD9-CM:340", "ICD9-CM:714.0"]    

Sleep_Disorder_Codes = ["ICD10:F51.0", "ICD10:F51.00", "ICD10:F51.05", "ICD10:F51.02", "ICD10:F51.09", 
    "ICD10:F51.01", "ICD10:F51.03", "ICD10:F51.04", "ICD10:G47.00", "ICD10:G47.01", "ICD10:G47.09", 
    "ICD10:F51.1", "ICD10:F51.10", "ICD10:F51.11", "ICD10:F51.12", "ICD10:F51.19", "ICD10:G47.10", 
    "ICD10:G47.11", "ICD10:G47.12", "ICD10:G47.13", "ICD10:G47.14", "ICD10:G47.19", "ICD10:F51.2", 
    "ICD10:F51.20", "ICD10:F51.21", "ICD10:F51.22", "ICD10:F51.23", "ICD10:G47.20", "ICD10:G47.21", 
    "ICD10:G47.22", "ICD10:G47.23", "ICD10:G47.24", "ICD10:G47.26", "ICD10:F51.3", "ICD10:F51.4", 
    "ICD10:F51.5", "ICD10:G47.5", "ICD10:G47.50", "ICD10:G47.51", "ICD10:G47.52", "ICD10:G47.53", 
    "ICD10:G47.54", "ICD10:G47.55", "ICD10:G47.56", "ICD10:G47.57", "ICD10:G47.59", "ICD10:G47.5A", 
    "ICD10:G47.5B", "ICD10:G47.5W", "ICD10:G47.59", "ICD10:G47.6", "ICD10:G47.60", "ICD10:G47.61", 
    "ICD10:G47.62", "ICD10:G47.63", "ICD10:G47.64", "ICD10:G47.65", "ICD10:G47.66", "ICD10:G47.67", 
    "ICD10:G47.69", "ICD10:G47.41", "ICD10:G47.411", "ICD10:G47.42", "ICD10:G47.419", 
    "ICD10:G47.3", "ICD10:G47.32", "ICD10:G47.33", "ICD10:G47.34", "ICD10:G47.36", "ICD10:G47.31", 
    "ICD10:G47.37", "ICD10:G47.39", "ICD10:G47.30", "ICD10-CM:F51.0", "ICD10-CM:F51.05", "ICD10-CM:F51.02", 
    "ICD10-CM:F51.09", "ICD10-CM:F51.01", "ICD10-CM:F51.03", "ICD10-CM:F51.04", "ICD10-CM:G47.00", "ICD10-CM:G47.01", 
    "ICD10-CM:G47.09", "ICD10-CM:F51.1", "ICD10-CM:F51.11", "ICD10-CM:F51.12", "ICD10-CM:F51.19", "ICD10-CM:G47.10", 
    "ICD10-CM:G47.11", "ICD10-CM:G47.12", "ICD10-CM:G47.13", "ICD10-CM:G47.14", "ICD10-CM:G47.19", "ICD10-CM:G47.20", 
    "ICD10-CM:G47.21", "ICD10-CM:G47.22", "ICD10-CM:G47.23", "ICD10-CM:G47.24", "ICD10-CM:G47.26", "ICD10-CM:F51.3", 
    "ICD10-CM:F51.4", "ICD10-CM:F51.5", "ICD10-CM:G47.5", "ICD10-CM:G47.50", "ICD10-CM:G47.51", "ICD10-CM:G47.52", 
    "ICD10-CM:G47.53", "ICD10-CM:G47.54", "ICD10-CM:G47.55", "ICD10-CM:G47.56", "ICD10-CM:G47.57", "ICD10-CM:G47.59", 
    "ICD10-CM:G47.5A", "ICD10-CM:G47.5B", "ICD10-CM:G47.5W", "ICD10-CM:G47.59", "ICD10-CM:G47.6", "ICD10-CM:G47.60", 
    "ICD10-CM:G47.61", "ICD10-CM:G47.62", "ICD10-CM:G47.63", "ICD10-CM:G47.64", "ICD10-CM:G47.65", "ICD10-CM:G47.66", 
    "ICD10-CM:G47.67", "ICD10-CM:G47.69", "ICD10-CM:G47.41", "ICD10-CM:G47.411", "ICD10-CM:G47.42", "ICD10-CM:G47.419", 
    "ICD10-CM:G47.3", "ICD10-CM:G47.32", "ICD10-CM:G47.33", "ICD10-CM:G47.34", "ICD10-CM:G47.36", "ICD10-CM:G47.31", 
    "ICD10-CM:G47.37", "ICD10-CM:G47.39", "ICD10-CM:G47.30", "ICD9:307.42", "ICD9:307.44", "ICD9:780.54", "ICD9:307.45", 
    "ICD9:780.55", "ICD9:307.46", "ICD9:307.47", "ICD9:307.48", "ICD9:780.51", "ICD9:347.00", "ICD9:347.10", "ICD9:780.53", 
    "ICD9:770.81", "ICD9:327.23", "ICD9:780.51", "ICD9:327.21", "ICD9:327.29", "ICD9:327.20",
    "ICD9-CM:307.42", "ICD9-CM:307.44", "ICD9-CM:780.54", "ICD9-CM:307.45", "ICD9-CM:780.55", "ICD9-CM:307.46", "ICD9-CM:307.47", 
    "ICD9-CM:307.48", "ICD9-CM:780.51", "ICD9-CM:347.00", "ICD9-CM:347.10", "ICD9-CM:780.53", "ICD9-CM:780.53", "ICD9-CM:780.53", 
    "ICD9-CM:770.81", "ICD9-CM:780.57", "ICD9-CM:780.51", "ICD9-CM:327.21", "ICD9-CM:327.29", "ICD9-CM:327.20",
    "ICD8:306.49", "ICD8:347.00", "ICD8:347.01", "ICD8:347.09", "ICD8:347.10"]

AD_Codes = ["ICD10:G30.1", "ICD10:F00.1", "ICD10:F00.2", "ICD10:G30.8", "ICD10:G30.9", "ICD10:F00.9",
    "ICD10-CM:G30.1", "ICD10-CM:F00.1", "ICD10-CM:G30.8", "ICD10-CM:G30.9", "ICD10-CM:F00.9",
    "ICD9:331.0", "ICD9-CM:331.0", "ICD8:290.1", "ICD8:290.10"]

#Don't use exact Matching!
Chronic_Codes = ["ICD10:K50", "ICD10:K51", "ICD10:K52", "ICD10:B20", "ICD10:B22", "ICD10:B23", "ICD10:B24", 
    "ICD10:E10", "ICD10:E11", "ICD10:J45", "ICD10:G80", "ICD10:I10", "ICD10:I11", "ICD10:I12", "ICD10:I13", "ICD10:I14", "ICD10:I15", 
    "ICD10:I20", "ICD10:I21", "ICD10:I22", "ICD10:I23", "ICD10:I24", "ICD10:I25", "ICD10:N18", "ICD10:J44", "ICD10:N80", "ICD10:N80.0", "ICD10:N80.1", 
    "ICD10:N80.2", "ICD10:N80.3", "ICD10:N80.4", "ICD10:N80.4A", "ICD10:N80.4B", "ICD10:N80.5", "ICD10:N80.5A", "ICD10:N80.5B", "ICD10:N80.5C", 
    "ICD10:N80.6", "ICD10:N80.8", "ICD10:N80.8A", "ICD10:N80.8B", "ICD10:N80.8C", "ICD10:N80.8D", "ICD10:N80.9", "ICD10:M05", "ICD10:M06",
    "ICD10-CM:K50", "ICD10-CM:K51", "ICD10-CM:K52", "ICD10-CM:B20", "ICD10-CM:E10", "ICD10-CM:E11", 
    "ICD10-CM:J45", "ICD10-CM:G80", "ICD10-CM:I10", "ICD10-CM:I11", "ICD10-CM:I12", "ICD10-CM:I13", "ICD10-CM:I14", "ICD10-CM:I15", 
    "ICD10-CM:I20", "ICD10-CM:I21", "ICD10-CM:I22", "ICD10-CM:I23", "ICD10-CM:I24", "ICD10-CM:I25", "ICD10-CM:N18", "ICD10-CM:J44", 
    "ICD10-CM:N80", "ICD10-CM:N80.0", "ICD10-CM:N80.1", "ICD10-CM:N80.2", "ICD10-CM:N80.3", 
    "ICD10-CM:N80.4", "ICD10-CM:N80.5", "ICD10-CM:N80.6", "ICD10-CM:N80.8", "ICD10-CM:N80.9", "ICD10-CM:M05", "ICD10-CM:M06K50", 
    "ICD10-CM:K51", "ICD10-CM:K52", "ICD10-CM:K50", "ICD10-CM:K51", "ICD10-CM:B20", "ICD10-CM:E10", "ICD10-CM:E11", "ICD10-CM:J45", 
    "ICD10-CM:G80", "ICD10-CM:I10", "ICD10-CM:I11", "ICD10-CM:I12", "ICD10-CM:I13", "ICD10-CM:I14", "ICD10-CM:I15", "ICD10-CM:I20", 
    "ICD10-CM:I21", "ICD10-CM:I22", "ICD10-CM:I23", "ICD10-CM:I24", "ICD10-CM:I25", "ICD10-CM:N18", "ICD10-CM:J44", "ICD10-CM:C00", 
    "ICD10-CM:C01", "ICD10-CM:C02", "ICD10-CM:C03", "ICD10-CM:C04", "ICD10-CM:C05", "ICD10-CM:C06", "ICD10-CM:C07", "ICD10-CM:C08", 
    "ICD10-CM:C09", "ICD10-CM:C10", "ICD10-CM:C11", "ICD10-CM:C12", "ICD10-CM:C13", "ICD10-CM:C14", "ICD10-CM:C15", "ICD10-CM:C16", 
    "ICD10-CM:C17", "ICD10-CM:C18", "ICD10-CM:C19", "ICD10-CM:C20", "ICD10-CM:C21", "ICD10-CM:C22", "ICD10-CM:C23", "ICD10-CM:C24", 
    "ICD10-CM:C25", "ICD10-CM:C26", "ICD10-CM:C30", "ICD10-CM:C31", "ICD10-CM:C32", "ICD10-CM:C33", "ICD10-CM:C34", "ICD10-CM:C37", 
    "ICD10-CM:C38", "ICD10-CM:C39", "ICD10-CM:C40", "ICD10-CM:C41", "ICD10-CM:C43", "ICD10-CM:C44", "ICD10-CM:C4A", "ICD10-CM:C45", 
    "ICD10-CM:C46", "ICD10-CM:C47", "ICD10-CM:C48", "ICD10-CM:C49", "ICD10-CM:C50", "ICD10-CM:C51", "ICD10-CM:C52", "ICD10-CM:C53", 
    "ICD10-CM:C54", "ICD10-CM:C55", "ICD10-CM:C56", "ICD10-CM:C57", "ICD10-CM:C58", "ICD10-CM:C60", "ICD10-CM:C61", "ICD10-CM:C62", 
    "ICD10-CM:C63", "ICD10-CM:C64", "ICD10-CM:C65", "ICD10-CM:C66", "ICD10-CM:C67", "ICD10-CM:C68", "ICD10-CM:C69", "ICD10-CM:C70", 
    "ICD10-CM:C71", "ICD10-CM:C72", "ICD10-CM:C73", "ICD10-CM:C74", "ICD10-CM:C75", "ICD10-CM:C76", "ICD10-CM:C77", "ICD10-CM:C78", 
    "ICD10-CM:C79", "ICD10-CM:C7A", "ICD10-CM:C7B", "ICD10-CM:C80", "ICD10-CM:C81", "ICD10-CM:C82", "ICD10-CM:C83", "ICD10-CM:C84", 
    "ICD10-CM:C85", "ICD10-CM:C86", "ICD10-CM:C88", "ICD10-CM:C90", "ICD10-CM:C91", "ICD10-CM:C92", "ICD10-CM:C93", "ICD10-CM:C94", 
    "ICD10-CM:C99", "ICD10-CM:C96", "ICD10-CM:D00", "ICD10-CM:D01", "ICD10-CM:D02", "ICD10-CM:D03", "ICD10-CM:D04", "ICD10-CM:D05", 
    "ICD10-CM:D06", "ICD10-CM:D07", "ICD10-CM:D09", "ICD10-CM:D10", "ICD10-CM:D11", "ICD10-CM:D12", "ICD10-CM:D13", "ICD10-CM:D14", 
    "ICD10-CM:D15", "ICD10-CM:D16", "ICD10-CM:D17", "ICD10-CM:D18", "ICD10-CM:D19", "ICD10-CM:D20", "ICD10-CM:D21", "ICD10-CM:D22", 
    "ICD10-CM:D23", "ICD10-CM:D24", "ICD10-CM:D25", "ICD10-CM:D26", "ICD10-CM:D27", "ICD10-CM:D28", "ICD10-CM:D29", "ICD10-CM:D30", 
    "ICD10-CM:D31", "ICD10-CM:D32", "ICD10-CM:D33", "ICD10-CM:D34", "ICD10-CM:D35", "ICD10-CM:D36", "ICD10-CM:D37", "ICD10-CM:D38", 
    "ICD10-CM:D39", "ICD10-CM:D40", "ICD10-CM:D41", "ICD10-CM:D42", "ICD10-CM:D43", "ICD10-CM:D44", "ICD10-CM:D45", "ICD10-CM:D46", 
    "ICD10-CM:D47", "ICD10-CM:D48", "ICD10-CM:N80", "ICD10-CM:N80.0", "ICD10-CM:N80.1", "ICD10-CM:N80.2", "ICD10-CM:N80.3", 
    "ICD10-CM:N80.4", "ICD10-CM:N80.5", "ICD10-CM:N80.6", "ICD10-CM:N80.8", "ICD10-CM:N80.9", "ICD10-CM:M05", "ICD10-CM:M06",
    "ICD9:555", "ICD9:556", "ICD9:042", "ICD9:043", "ICD9:044", "ICD9:250.01", "ICD9:250.11", "ICD9:250.21", "ICD9:250.31", 
    "ICD9:250.41", "ICD9:250.51", "ICD9:250.61", "ICD9:250.71", "ICD9:250.81", "ICD9:250.91", "ICD9:250.00", "ICD9:250.10", 
    "ICD9:250.20", "ICD9:250.30", "ICD9:250.40", "ICD9:250.50", "ICD9:250.60", "ICD9:250.70", "ICD9:250.80", "ICD9:250.90", 
    "ICD9:493", "ICD9:343", "ICD9:401", "ICD9:402", "ICD9:403", "ICD9:404", "ICD9:405", "ICD9:410", "ICD9:411", "ICD9:412", 
    "ICD9:413", "ICD9:414", "ICD9:585", "ICD9:490", "ICD9:491", "ICD9:492", "ICD9:493", "ICD9:494", "ICD9:495", "ICD9:496",  
    "ICD9:617", "ICD9:617.0", "ICD9:617.1", "ICD9:617.2", "ICD9:617.3", "ICD9:617.4", "ICD9:617.5", "ICD9:617.6", "ICD9:617.8", 
    "ICD9:617.9 ", "ICD9:714", "ICD9-CM:555", "ICD9-CM:556", "ICD9-CM:042", "ICD9-CM:250.01", "ICD9-CM:250.11", "ICD9-CM:250.21", 
    "ICD9-CM:250.31", "ICD9-CM:250.41", "ICD9-CM:250.51", "ICD9-CM:250.61", "ICD9-CM:250.71", "ICD9-CM:250.81", "ICD9-CM:250.91", 
    "ICD9-CM:250.00", "ICD9-CM:250.10", "ICD9-CM:250.20", "ICD9-CM:250.30", "ICD9-CM:250.40", "ICD9-CM:250.50", "ICD9-CM:250.60", 
    "ICD9-CM:250.70", "ICD9-CM:250.80", "ICD9-CM:250.90", "ICD9-CM:493", "ICD9-CM:343", "ICD9-CM:401", "ICD9-CM:402", "ICD9-CM:403", 
    "ICD9-CM:404", "ICD9-CM:405", "ICD9-CM:410", "ICD9-CM:411", "ICD9-CM:412", "ICD9-CM:413", "ICD9-CM:414", "ICD9-CM:585", 
    "ICD9-CM:490", "ICD9-CM:491", "ICD9-CM:492", "ICD9-CM:493", "ICD9-CM:494", "ICD9-CM:495", "ICD9-CM:496", 
    "ICD9-CM:617", "ICD9-CM:617.0", "ICD9-CM:617.1", "ICD9-CM:617.2", "ICD9-CM:617.3", 
    "ICD9-CM:617.4", "ICD9-CM:617.5", "ICD9-CM:617.6", "ICD9-CM:617.8", "ICD9-CM:617.9 ", "ICD9-CM:714",
    "ICD8:563", "ICD8:563.1", "ICD8:563.2", "ICD8:079.83", "ICD8:250.01", "ICD8:250.02", "ICD8:250.03", "ICD8:250.11", 
    "ICD8:250.12", "ICD8:250.13", "ICD8:250.21", "ICD8:250.22", "ICD8:250.23", "ICD8:250.31", "ICD8:250.32", "ICD8:250.33", 
    "ICD8:250.41", "ICD8:250.42", "ICD8:250.43", "ICD8:250.51", "ICD8:250.52", "ICD8:250.53", "ICD8:250.61", "ICD8:250.62", 
    "ICD8:250.63", "ICD8:250.71", "ICD8:250.72", "ICD8:250.73", "ICD8:250.81", "ICD8:250.82", "ICD8:250.83", "ICD8:250.91", 
    "ICD8:250.92", "ICD8:250.93", "ICD8:493", "ICD8:400.09", "ICD8:400.19", "ICD8:400.29", "ICD8:400.39", 
    "ICD8:400.99", "ICD8:401.99", "ICD8:402.99", "ICD8:403.99", "ICD8:404.99", "ICD8:410.09", "ICD8:410.99", "ICD8:411.09", 
    "ICD8:411.99", "ICD8:412.09", "ICD8:412.99", "ICD8:413.09", "ICD8:413.99", "ICD8:414.09", "ICD8:414.99", "ICD8:584", 
    "ICD8:491", "ICD8:492", "ICD8:493", "ICD8:625.30", "ICD8:625.31", "ICD8:625.32", "ICD8:625.33", "ICD8:625.34", "ICD8:625.35", 
    "ICD8:625.36", "ICD8:625.37", "ICD8:625.38", "ICD8:625.39", "ICD8:712.09", "ICD8:712.19", "ICD8:712.39", "ICD8:712.59"]


Other_Mental_Codes = ["ICD10:F04*", "ICD10:F05*", "ICD10:F06.0", "ICD10:F06.1", "ICD10:F06.2", "ICD10:F06.3", "ICD10:F06.4", 
    "ICD10:F06.5", "ICD10:F06.6", "ICD10:F06.8", "ICD10:F06.9", "ICD10:F07*", "ICD10:F09*", 
    "ICD10:F10.1", "ICD10:F10.3", "ICD10:F10.4", "ICD10:F10.5", "ICD10:F10.6", "ICD10:F10.7", "ICD10:F10.8", "ICD10:F10.9",
    "ICD10:F11.1", "ICD10:F11.3", "ICD10:F11.4", "ICD10:F11.5", "ICD10:F11.6", "ICD10:F11.7", "ICD10:F11.8", "ICD10:F11.9",
    "ICD10:F12.1", "ICD10:F12.3", "ICD10:F12.4", "ICD10:F12.5", "ICD10:F12.6", "ICD10:F12.7", "ICD10:F12.8", "ICD10:F12.9",
    "ICD10:F13.1", "ICD10:F13.3", "ICD10:F13.4", "ICD10:F13.5", "ICD10:F13.6", "ICD10:F13.7", "ICD10:F13.8", "ICD10:F13.9",
    "ICD10:F14.1", "ICD10:F14.3", "ICD10:F14.4", "ICD10:F14.5", "ICD10:F14.6", "ICD10:F14.7", "ICD10:F14.8", "ICD10:F14.9",
    "ICD10:F15.1", "ICD10:F15.3", "ICD10:F15.4", "ICD10:F15.5", "ICD10:F15.6", "ICD10:F15.7", "ICD10:F15.8", "ICD10:F15.9",
    "ICD10:F16.1", "ICD10:F16.3", "ICD10:F16.4", "ICD10:F16.5", "ICD10:F16.6", "ICD10:F16.7", "ICD10:F16.8", "ICD10:F16.9",
    "ICD10:F17.1", "ICD10:F17.3", "ICD10:F17.4", "ICD10:F17.5", "ICD10:F17.6", "ICD10:F17.7", "ICD10:F17.8", "ICD10:F17.9",
    "ICD10:F18.1", "ICD10:F18.3", "ICD10:F18.4", "ICD10:F18.5", "ICD10:F18.6", "ICD10:F18.7", "ICD10:F18.8", "ICD10:F18.9",
    "ICD10:F19.1", "ICD10:F19.3", "ICD10:F19.4", "ICD10:F19.5", "ICD10:F19.6", "ICD10:F19.7", "ICD10:F19.8", "ICD10:F19.9",
    "ICD10:F34.2", "ICD10:F34.3", "ICD10:F34.4", "ICD10:F34.5", "ICD10:F34.6", "ICD10:F34.7", "ICD10:F34.8", "ICD10:F34.9", 
    "ICD10:F38*", "ICD10:F39*", "ICD10:F44*", "ICD10:F45", "ICD10:F46", "ICD10:F47", "ICD10:F48", "ICD10:F49",
    "ICD10:F5*", "ICD10:F6", "ICD10:F8", "ICD10:F9", "ICD10-CM:F04", 
    "ICD10-CM:F05", "ICD10-CM:F06.0", "ICD10-CM:F06.1", "ICD10-CM:F06.2", "ICD10-CM:F06.3", "ICD10-CM:F06.4", 
    "ICD10-CM:F06.5", "ICD10-CM:F06.6", "ICD10-CM:F06.8", "ICD10-CM:F06.9", "ICD10-CM:F07", "ICD10-CM:F09", "ICD10-CM:F10.1", "ICD10-CM:F10.9", 
    "ICD10-CM:F11.1", "ICD10-CM:F11.9", "ICD10-CM:F12.1", "ICD10-CM:F12.9",  "ICD10-CM:F13.1", "ICD10-CM:F13.9",
    "ICD10-CM:F14.1", "ICD10-CM:F14.9", "ICD10-CM:F15.1", "ICD10-CM:F15.9", "ICD10-CM:F16.1", "ICD10-CM:F16.9",
    "ICD10-CM:F17.1", "ICD10-CM:F17.9","ICD10-CM:F18.1", "ICD10-CM:F18.9","ICD10-CM:F19.1", "ICD10-CM:F19.9",
    "ICD10-CM:F34.2", "ICD10-CM:F34.3", "ICD10-CM:F34.4", "ICD10-CM:F34.5", "ICD10-CM:F34.6", "ICD10-CM:F34.7", "ICD10-CM:F34.8", "ICD10-CM:F34.9",
    "ICD10-CM:F38", "ICD10-CM:F39", "ICD10-CM:F4", "ICD10-CM:F5", "ICD10-CM:F6",
    "ICD10-CM:F8", "ICD10-CM:F9", "ICD9:046.1", "ICD9:279.1", "ICD9:290.3", 
    "ICD9:290.5", "ICD9:290.6", "ICD9:290.7", "ICD9:291", "ICD9:291.0", "ICD9:291.1", 
    "ICD9:291.2", "ICD9:291.3", "ICD9:291.4", "ICD9:291.5", "ICD9:291.6", "ICD9:291.7", "ICD9:291.8", "ICD9:291.9", "ICD9:292", 
    "ICD9:292.0", "ICD9:292.1", "ICD9:292.2", "ICD9:292.3", "ICD9:292.4", "ICD9:292.5", "ICD9:292.6", "ICD9:292.7", "ICD9:292.8", 
    "ICD9:292.9", "ICD9:293", "ICD9:293.0", "ICD9:293.1", "ICD9:293.2", "ICD9:293.3", "ICD9:293.4", "ICD9:293.5", "ICD9:293.6", 
    "ICD9:293.7", "ICD9:293.8", "ICD9:293.9", "ICD9:294.0", "ICD9:294.8", "ICD9:294.9", "ICD9:296.3", 
    "ICD9:296.9", "ICD9:297", "ICD9:297.0", "ICD9:297.1", "ICD9:297.2", "ICD9:297.3", "ICD9:297.4", "ICD9:297.5", 
    "ICD9:297.6", "ICD9:297.7", "ICD9:297.8", "ICD9:297.9", "ICD9:298.1", "ICD9:298.2", "ICD9:298.3", "ICD9:298.4", "ICD9:298.5", "ICD9:298.6", 
    "ICD9:298.7", "ICD9:298.8", "ICD9:298.9", "ICD9:300.0", "ICD9:300.1", "ICD9:300.2", 
    "ICD9:301", "ICD9:301.0", "ICD9:301.1", "ICD9:301.2", "ICD9:301.3", "ICD9:301.4", "ICD9:301.5", "ICD9:301.6", "ICD9:301.7", 
    "ICD9:301.8", "ICD9:301.9", "ICD9:302.1", "ICD9:302.2", "ICD9:302.3", "ICD9:302.4", "ICD9:302.5", "ICD9:302.6", "ICD9:302.7", "ICD9:302.8", 
    "ICD9:302.9", "ICD9:303.3", "ICD9:303.4", "ICD9:303.5", "ICD9:303.6", "ICD9:303.7", "ICD9:303.8",  
    "ICD9:304.7", "ICD9:304.8", "ICD9:304.9", "ICD9:305", "ICD9:305.0", "ICD9:305.1", "ICD9:305.2", "ICD9:305.3", "ICD9:305.4", "ICD9:305.5", 
    "ICD9:305.6", "ICD9:305.7", "ICD9:305.8", "ICD9:305.9", "ICD9:306", "ICD9:306.0", "ICD9:306.1", "ICD9:306.2", "ICD9:306.3", "ICD9:306.4", 
    "ICD9:306.5", "ICD9:306.6", "ICD9:306.7", "ICD9:306.8", "ICD9:306.9", "ICD9:307.0", "ICD9:307.1", "ICD9:307.2", "ICD9:307.3",
    "ICD9:307.5", "ICD9:307.5", "ICD9:307.6", "ICD9:307.7", "ICD9:307.9", "ICD9:308", "ICD9:308.0", "ICD9:308.1", "ICD9:308.2", "ICD9:308.3", 
    "ICD9:308.4", "ICD9:308.5", "ICD9:308.6", "ICD9:308.7", "ICD9:308.8", "ICD9:308.9", "ICD9:309", "ICD9:309.0", "ICD9:309.1", "ICD9:309.2", 
    "ICD9:309.3", "ICD9:309.4", "ICD9:309.5", "ICD9:309.6", "ICD9:309.7", "ICD9:309.8", "ICD9:309.9", "ICD9:309.2", "ICD9:309.8", "ICD9:310", 
    "ICD9:310.0", "ICD9:310.2", "ICD9:310.3", "ICD9:310.4", "ICD9:310.5", "ICD9:310.6", "ICD9:310.7", "ICD9:310.8", "ICD9:310.9", 
    "ICD9:312.0", "ICD9:312.1", "ICD9:312.2", "ICD9:312.3", "ICD9:312.8", "ICD9:312.9", "ICD9:313.0", "ICD9:313.3", "ICD9:313.8", "ICD9:313.9", 
    "ICD9:314.1", "ICD9:314.2", "ICD9:314.8", "ICD9:314.9", "ICD9:315.0", "ICD9:315.1", "ICD9:315.2", "ICD9:315.3", "ICD9:315.4", 
    "ICD9:315.5", "ICD9:315.8", "ICD9:315.9", "ICD9:316", "ICD9:316.0", "ICD9:316.1", "ICD9:316.2", "ICD9:316.3", "ICD9:316.4", "ICD9:316.5", 
    "ICD9:316.6", "ICD9:316.7", "ICD9:316.8", "ICD9:316.9", "ICD9:319", "ICD9:319.0", 
    "ICD9:319.1", "ICD9:319.2", "ICD9:319.3", "ICD9:319.4", "ICD9:319.5", "ICD9:319.6", "ICD9:319.7", "ICD9:319.8", "ICD9:319.9", "ICD9:332.0", 
    "ICD9:333.4", "ICD9:V40.2", "ICD9:V40.3", "ICD9:V40.9", "ICD9:V61.1", "ICD9-CM:046.1", "ICD9-CM:279.1", "ICD9-CM:290.3", "ICD9-CM:290.5", "ICD9-CM:290.6", "ICD9-CM:290.7", 
    "ICD9-CM:291", "ICD9-CM:291.0", "ICD9-CM:291.1", "ICD9-CM:291.2", "ICD9-CM:291.3", "ICD9-CM:291.4", "ICD9-CM:291.5", 
    "ICD9-CM:291.6", "ICD9-CM:291.7", "ICD9-CM:291.8", "ICD9-CM:291.9", "ICD9-CM:292", "ICD9-CM:292.0", "ICD9-CM:292.1", "ICD9-CM:292.2", 
    "ICD9-CM:292.3", "ICD9-CM:292.4", "ICD9-CM:292.5", "ICD9-CM:292.6", "ICD9-CM:292.7", "ICD9-CM:292.8", "ICD9-CM:292.9", "ICD9-CM:293", 
    "ICD9-CM:293.0", "ICD9-CM:293.1", "ICD9-CM:293.2", "ICD9-CM:293.3", "ICD9-CM:293.4", "ICD9-CM:293.5", "ICD9-CM:293.6", "ICD9-CM:293.7", 
    "ICD9-CM:293.8", "ICD9-CM:293.9", "ICD9-CM:294.0", "ICD9-CM:294.8", "ICD9-CM:294.9", "ICD9-CM:296.3", "ICD9-CM:296.30", "ICD9-CM:296.31", "ICD9-CM:296.32", 
    "ICD9-CM:296.33", "ICD9-CM:296.34", "ICD9-CM:296.35", "ICD9-CM:296.9", "ICD9-CM:296.90", "ICD9-CM:296.91", "ICD9-CM:296.92", 
    "ICD9-CM:296.93", "ICD9-CM:296.94", "ICD9-CM:296.95", "ICD9-CM:296.96", "ICD9-CM:296.97", "ICD9-CM:296.98", "ICD9-CM:296.99", "ICD9-CM:297", 
    "ICD9-CM:297.0", "ICD9-CM:297.1", "ICD9-CM:297.2", "ICD9-CM:297.3", "ICD9-CM:297.4", "ICD9-CM:297.5", "ICD9-CM:297.6", "ICD9-CM:297.7", "ICD9-CM:297.8", 
    "ICD9-CM:297.9", "ICD9-CM:298.1", "ICD9-CM:298.2", "ICD9-CM:298.3", "ICD9-CM:298.4", "ICD9-CM:298.5", "ICD9-CM:298.6", "ICD9-CM:298.7", "ICD9-CM:298.8", 
    "ICD9-CM:298.9", "ICD9-CM:300.0", "ICD9-CM:300.00", "ICD9-CM:300.01", "ICD9-CM:300.02", "ICD9-CM:300.03", 
    "ICD9-CM:300.04", "ICD9-CM:300.05", "ICD9-CM:300.06", "ICD9-CM:300.07", "ICD9-CM:300.08", "ICD9-CM:300.09", "ICD9-CM:300.1", "ICD9-CM:300.2", "ICD9-CM:300.20", 
    "ICD9-CM:300.21", "ICD9-CM:300.22", "ICD9-CM:300.23", "ICD9-CM:300.24", "ICD9-CM:300.25", "ICD9-CM:300.26", "ICD9-CM:300.2", "ICD9-CM:300.28", "ICD9-CM:300.29", 
    "ICD9-CM:301", "ICD9-CM:301.0", "ICD9-CM:301.1", "ICD9-CM:301.2", "ICD9-CM:301.3", "ICD9-CM:301.4", "ICD9-CM:301.5", 
    "ICD9-CM:301.6", "ICD9-CM:301.7", "ICD9-CM:301.8", "ICD9-CM:301.81", "ICD9-CM:301.82", "ICD9-CM:301.83", "ICD9-CM:301.84", "ICD9-CM:301.85", "ICD9-CM:301.86", 
    "ICD9-CM:301.87", "ICD9-CM:301.88", "ICD9-CM:301.89", "ICD9-CM:301.9", "ICD9-CM:302.1", "ICD9-CM:302.2", "ICD9-CM:302.3", "ICD9-CM:302.4", "ICD9-CM:302.5", 
    "ICD9-CM:302.6", "ICD9-CM:302.7", "ICD9-CM:302.8", "ICD9-CM:302.9", "ICD9-CM:303.3", "ICD9-CM:303.30", "ICD9-CM:303.31", "ICD9-CM:303.32", "ICD9-CM:303.33", "ICD9-CM:303.34", "ICD9-CM:303.35", "ICD9-CM:303.36", 
    "ICD9-CM:303.37", "ICD9-CM:303.38", "ICD9-CM:303.39", "ICD9-CM:303.4", "ICD9-CM:303.40", "ICD9-CM:303.41", "ICD9-CM:303.42", "ICD9-CM:303.43", "ICD9-CM:303.44", 
    "ICD9-CM:303.45", "ICD9-CM:303.46", "ICD9-CM:303.47", "ICD9-CM:303.48", "ICD9-CM:303.49", "ICD9-CM:303.5", "ICD9-CM:303.50", "ICD9-CM:303.51", "ICD9-CM:303.52", 
    "ICD9-CM:303.53", "ICD9-CM:303.54", "ICD9-CM:303.55", "ICD9-CM:303.56", "ICD9-CM:303.57", "ICD9-CM:303.58", "ICD9-CM:303.59", "ICD9-CM:303.6", "ICD9-CM:303.60", 
    "ICD9-CM:303.61", "ICD9-CM:303.62", "ICD9-CM:303.63", "ICD9-CM:303.64", "ICD9-CM:303.65", "ICD9-CM:303.66", "ICD9-CM:303.67", "ICD9-CM:303.68", "ICD9-CM:303.69", 
    "ICD9-CM:303.7", "ICD9-CM:303.70", "ICD9-CM:303.71", "ICD9-CM:303.72", "ICD9-CM:303.73", "ICD9-CM:303.74", "ICD9-CM:303.75", "ICD9-CM:303.76", "ICD9-CM:303.77", 
    "ICD9-CM:303.78", "ICD9-CM:303.79", "ICD9-CM:303.8", "ICD9-CM:303.80", "ICD9-CM:303.81", "ICD9-CM:303.82", "ICD9-CM:303.83", "ICD9-CM:303.84", "ICD9-CM:303.85", 
    "ICD9-CM:303.86", "ICD9-CM:303.87", "ICD9-CM:303.88", "ICD9-CM:303.89", "ICD9-CM:304.8", "ICD9-CM:304.80", "ICD9-CM:304.81", "ICD9-CM:304.82", 
    "ICD9-CM:304.83", "ICD9-CM:304.84", "ICD9-CM:304.85", "ICD9-CM:304.86", "ICD9-CM:304.87", "ICD9-CM:304.88", "ICD9-CM:304.89", "ICD9-CM:304.9", "ICD9-CM:304.90", 
    "ICD9-CM:304.91", "ICD9-CM:304.92", "ICD9-CM:304.93", "ICD9-CM:304.94", "ICD9-CM:304.95", "ICD9-CM:304.96", "ICD9-CM:304.97", "ICD9-CM:304.98", "ICD9-CM:304.99", 
    "ICD9-CM:305", "ICD9-CM:305.0", "ICD9-CM:305.01", "ICD9-CM:305.02", "ICD9-CM:305.03", "ICD9-CM:305.1", "ICD9-CM:305.10", "ICD9-CM:305.11", "ICD9-CM:305.12", 
    "ICD9-CM:305.13", "ICD9-CM:305.2", "ICD9-CM:305.20", "ICD9-CM:305.21", "ICD9-CM:305.22", "ICD9-CM:305.23", "ICD9-CM:305.3", "ICD9-CM:305.30", "ICD9-CM:305.31", 
    "ICD9-CM:305.32", "ICD9-CM:305.33", "ICD9-CM:305.4", "ICD9-CM:305.40", "ICD9-CM:305.41", "ICD9-CM:305.42", "ICD9-CM:305.43", "ICD9-CM:305.5", "ICD9-CM:305.50", 
    "ICD9-CM:305.51", "ICD9-CM:305.52", "ICD9-CM:305.53", "ICD9-CM:305.6", "ICD9-CM:305.60", "ICD9-CM:305.61", "ICD9-CM:305.62", "ICD9-CM:305.63", "ICD9-CM:305.7", 
    "ICD9-CM:305.70", "ICD9-CM:305.71", "ICD9-CM:305.72", "ICD9-CM:305.73", "ICD9-CM:305.8", "ICD9-CM:305.80", "ICD9-CM:305.81", "ICD9-CM:305.82", "ICD9-CM:305.83", 
    "ICD9-CM:305.9", "ICD9-CM:305.90", "ICD9-CM:305.91", "ICD9-CM:305.92", "ICD9-CM:305.93", "ICD9-CM:306", "ICD9-CM:306.0", "ICD9-CM:306.1", "ICD9-CM:306.2", "ICD9-CM:306.3", 
    "ICD9-CM:306.4", "ICD9-CM:306.5", "ICD9-CM:306.6", "ICD9-CM:306.7", "ICD9-CM:306.8", "ICD9-CM:306.9", "ICD9-CM:307.0", "ICD9-CM:307.1", "ICD9-CM:307.2", "ICD9-CM:307.20", 
    "ICD9-CM:307.21", "ICD9-CM:307.22", "ICD9-CM:307.23", "ICD9-CM:307.24", "ICD9-CM:307.25", "ICD9-CM:307.26", "ICD9-CM:307.27", "ICD9-CM:307.28", "ICD9-CM:307.29", "ICD9-CM:307.3", 
    "ICD9-CM:307.40", "ICD9-CM:307.41", "ICD9-CM:307.43", 
    "ICD9-CM:307.49", "ICD9-CM:307.5", "ICD9-CM:307.5", "ICD9-CM:307.6", "ICD9-CM:307.7", "ICD9-CM:307.9", "ICD9-CM:308", "ICD9-CM:308.0", "ICD9-CM:308.1", "ICD9-CM:308.2", "ICD9-CM:308.3", 
    "ICD9-CM:308.4", "ICD9-CM:308.5", "ICD9-CM:308.6", "ICD9-CM:308.7", "ICD9-CM:308.8", "ICD9-CM:308.9", "ICD9-CM:309", "ICD9-CM:309.0", "ICD9-CM:309.1", "ICD9-CM:309.2", "ICD9-CM:309.3", 
    "ICD9-CM:309.4", "ICD9-CM:309.5", "ICD9-CM:309.6", "ICD9-CM:309.7", "ICD9-CM:309.8", "ICD9-CM:309.9", "ICD9-CM:310", "ICD9-CM:310.0", "ICD9-CM:310.2", "ICD9-CM:310.3", 
    "ICD9-CM:310.4", "ICD9-CM:310.5", "ICD9-CM:310.6", "ICD9-CM:310.7", "ICD9-CM:310.8", "ICD9-CM:310.9", "ICD9-CM:312.0", "ICD9-CM:312.00", "ICD9-CM:312.01", "ICD9-CM:312.02", "ICD9-CM:312.03", 
    "ICD9-CM:312.04", "ICD9-CM:312.05", "ICD9-CM:312.06", "ICD9-CM:312.07", "ICD9-CM:312.08", "ICD9-CM:312.09", "ICD9-CM:312.1", "ICD9-CM:312.10", "ICD9-CM:312.11", "ICD9-CM:312.12", "ICD9-CM:312.13", 
    "ICD9-CM:312.14", "ICD9-CM:312.15", "ICD9-CM:312.16", "ICD9-CM:312.17", "ICD9-CM:312.18", "ICD9-CM:312.19", "ICD9-CM:312.2", "ICD9-CM:312.20", "ICD9-CM:312.21", "ICD9-CM:312.22", "ICD9-CM:312.23", 
    "ICD9-CM:312.24", "ICD9-CM:312.25", "ICD9-CM:312.26", "ICD9-CM:312.27", "ICD9-CM:312.28", "ICD9-CM:312.29", "ICD9-CM:312.3", "ICD9-CM:312.30", "ICD9-CM:312.31", "ICD9-CM:312.32", "ICD9-CM:312.33", 
    "ICD9-CM:312.34", "ICD9-CM:312.35", "ICD9-CM:312.36", "ICD9-CM:312.37", "ICD9-CM:312.38", "ICD9-CM:312.39", "ICD9-CM:312.8", "ICD9-CM:312.80", "ICD9-CM:312.81", "ICD9-CM:312.82", "ICD9-CM:312.83", 
    "ICD9-CM:312.84", "ICD9-CM:312.85", "ICD9-CM:312.86", "ICD9-CM:312.87", "ICD9-CM:312.88", "ICD9-CM:312.89", "ICD9-CM:312.9", "ICD9-CM:312.90", "ICD9-CM:312.91", "ICD9-CM:312.92", "ICD9-CM:312.93", 
    "ICD9-CM:312.94", "ICD9-CM:312.95", "ICD9-CM:312.96", "ICD9-CM:312.97", "ICD9-CM:312.98", "ICD9-CM:312.99", "ICD9-CM:313.0", "ICD9-CM:313.2", "ICD9-CM:313.3", "ICD9-CM:313.8", "ICD9-CM:313.9", 
    "ICD9-CM:314.2", "ICD9-CM:314.8", "ICD9-CM:314.9", "ICD9-CM:315.0", "ICD9-CM:315.00", "ICD9-CM:315.01", "ICD9-CM:315.02", 
    "ICD9-CM:315.03", "ICD9-CM:315.04", "ICD9-CM:315.05", "ICD9-CM:315.06", "ICD9-CM:315.07", "ICD9-CM:315.08", "ICD9-CM:315.09", "ICD9-CM:315.1", "ICD9-CM:315.2", "ICD9-CM:315.3", "ICD9-CM:315.4", 
    "ICD9-CM:315.5", "ICD9-CM:315.8", "ICD9-CM:315.9", "ICD9-CM:316", "ICD9-CM:316.0", "ICD9-CM:316.1", "ICD9-CM:316.2", "ICD9-CM:316.3", "ICD9-CM:316.4", "ICD9-CM:316.5", "ICD9-CM:316.6", "ICD9-CM:316.7", 
    "ICD9-CM:316.8", "ICD9-CM:316.9", "ICD9-CM:319", "ICD9-CM:319.0", "ICD9-CM:319.1", "ICD9-CM:319.2", "ICD9-CM:319.3", "ICD9-CM:319.4", "ICD9-CM:319.5", "ICD9-CM:319.6", 
    "ICD9-CM:319.7", "ICD9-CM:319.8", "ICD9-CM:319.9", "ICD9-CM:332.0", "ICD9-CM:333.4", "ICD9-CM:V40.2", "ICD9-CM:V40.3", "ICD9-CM:V40.9", "ICD9-CM:V61.1",
    "ICD8:291", "ICD8:291.2", "ICD8:291.3", "ICD8:291.9", "ICD8:292", "ICD8:292.9", "ICD8:293.1", "ICD8:294", "ICD8:294.3", "ICD8:294.4", 
    "ICD8:294.8", "ICD8:294.9", 
    "ICD8:297", "ICD8:297.1", "ICD8:297.9", "ICD8:298.1", "ICD8:298.2", "ICD8:298.3", "ICD8:298.9", "ICD8:300.1", "ICD8:300.5", "ICD8:300.6", "ICD8:300.7", 
    "ICD8:300.8", "ICD8:300.9", "ICD8:301", "ICD8:301.1", "ICD8:301.2", "ICD8:301.3", "ICD8:301.4", "ICD8:301.5", "ICD8:301.6", "ICD8:301.7", "ICD8:301.8", "ICD8:301.9", "ICD8:302.1", "ICD8:302.3", 
    "ICD8:302.4", "ICD8:302.8", "ICD8:302.9", "ICD8:303", "ICD8:303.1", "ICD8:303.9", "ICD8:304.10", "ICD8:304.11", "ICD8:304.12", "ICD8:304.13", "ICD8:304.14", "ICD8:304.15", "ICD8:304.16", "ICD8:304.17", "ICD8:304.18", 
    "ICD8:304.20", "ICD8:304.21", "ICD8:304.22", "ICD8:304.23", "ICD8:304.24", "ICD8:304.25", "ICD8:304.26", "ICD8:304.27", "ICD8:304.28", "ICD8:304.30", "ICD8:304.31", "ICD8:304.32", "ICD8:304.33", "ICD8:304.34", "ICD8:304.35", 
    "ICD8:304.36", "ICD8:304.37", "ICD8:304.38", "ICD8:304.40", "ICD8:304.41", "ICD8:304.42", "ICD8:304.43", "ICD8:304.44", "ICD8:304.45", "ICD8:304.46", "ICD8:304.47", "ICD8:304.48", "ICD8:304.50", "ICD8:304.51", "ICD8:304.52", 
    "ICD8:304.53", "ICD8:304.54", "ICD8:304.55", "ICD8:304.56", "ICD8:304.57", "ICD8:304.58", "ICD8:304.70", "ICD8:304.71", "ICD8:304.72", "ICD8:304.73", "ICD8:304.74", "ICD8:304.75", "ICD8:304.76", "ICD8:304.77", "ICD8:304.78", 
    "ICD8:304.80", "ICD8:304.81", "ICD8:304.82", "ICD8:304.83", "ICD8:304.84", "ICD8:304.85", "ICD8:304.86", "ICD8:304.87", "ICD8:304.88", 
    "ICD8:304.90", "ICD8:304.91", "ICD8:304.92", "ICD8:304.93", "ICD8:304.94", "ICD8:304.95", "ICD8:304.96", "ICD8:304.97", "ICD8:304.98", 
    "ICD8:305", "ICD8:305.1", "ICD8:305.2", "ICD8:305.3", "ICD8:305.4", "ICD8:305.5", "ICD8:305.6", "ICD8:305.7", "ICD8:305.8", "ICD8:305.9", "ICD8:306.1", "ICD8:306.2", "ICD8:306.3", 
    "ICD8:306.6", "ICD8:306.7", "ICD8:306.8", "ICD8:306.9", "ICD8:307", "ICD8:308", "ICD8:308.1", "ICD8:308.2", "ICD8:308.4", "ICD8:309", "ICD8:309.2", "ICD8:309.9", "ICD8:342", "ICD8:390.1", "ICD8:393"]

# Generlaized Anxiety Disorder (GAD)
GAD_Codes = ["ICD10:F41.1", "ICD10-CM:F41.1"]

# Panic Disorder (PD)
PD_Codes = ["ICD10:F41.0", "ICD10-CM:F41.0"]

# Phobias
Phobias_Codes = ["ICD10:F40", "ICD10:F40.0", "ICD10:F40.1", "ICD10:F40.2", "ICD10:F40.8", "ICD10:F40.9",
    "ICD10-CM:F40", "ICD10-CM:F40.0", "ICD10-CM:F40.1", "ICD10-CM:F40.2", "ICD10-CM:F40.8", "ICD10-CM:F40.9",
    "ICD9:300.2", "ICD9:300.0", "ICD9-CM:300.2", "ICD9-CM:300.0", "ICD8:300.2", "ICD8:300.0"]

# Bulimia (BUL)
BUL_Codes = ["ICD10:F50.2", "ICD10:F50.3"]

# Anorexia (ANO)
ANO_Codes = ["ICD10:F50.0", "ICD8:306.50"]

# Ken's Pain (22.12.2023 via E-Mail)
Ken_Pain_Codes = ["ICD10:R52", "ICD10:M50", "ICD10:M51", "ICD10:M53", "ICD10:M54"]

Pain_Codes = ["ICD10:R52", "ICD10:R52.0", "ICD10:R52.1", "ICD10:R52.2", "ICD10:R52.9", "ICD10:M50", "ICD10:M50.0", "ICD10:M50.1", 
    "ICD10:M50.2", "ICD10:M50.3", "ICD10:M50.8", "ICD10:M50.9", "ICD10:M51", "ICD10:M51.0", "ICD10:M51.1", "ICD10:M51.2", "ICD10:M51.3", 
    "ICD10:M51.4", "ICD10:M51.8", "ICD10:M51.9", "ICD10:M53", "ICD10:M53.0", "ICD10:M53.1", "ICD10:M53.2", "ICD10:M53.3", "ICD10:M53.8", 
    "ICD10:M53.9", "ICD10:M54", "ICD10:M54.0", "ICD10:M54.1", "ICD10:M54.2", "ICD10:M54.3", "ICD10:M54.4", "ICD10:M54.5", "ICD10:M54.6", 
    "ICD10:M54.8", "ICD10:M54.9", "ICD10-CM:R52", "ICD10-CM:R52.0", "ICD10-CM:R52.1", "ICD10-CM:R52.2", "ICD10-CM:R52.9", "ICD10-CM:M50", 
    "ICD10-CM:M50.0", "ICD10-CM:M50.1", "ICD10-CM:M50.2", "ICD10-CM:M50.3", "ICD10-CM:M50.8", "ICD10-CM:M50.9", "ICD10-CM:M51", 
    "ICD10-CM:M51.0", "ICD10-CM:M51.1", "ICD10-CM:M51.2", "ICD10-CM:M51.3", "ICD10-CM:M51.4", "ICD10-CM:M51.8", "ICD10-CM:M51.9", 
    "ICD10-CM:M53", "ICD10-CM:M53.0", "ICD10-CM:M53.1", "ICD10-CM:M53.2", "ICD10-CM:M53.3", "ICD10-CM:M53.8", "ICD10-CM:M53.9", 
    "ICD10-CM:M54", "ICD10-CM:M54.0", "ICD10-CM:M54.1", "ICD10-CM:M54.2", "ICD10-CM:M54.3", "ICD10-CM:M54.4", "ICD10-CM:M54.5", 
    "ICD10-CM:M54.6", "ICD10-CM:M54.8", "ICD10-CM:M54.9", "ICD9:338.19", "ICD9:780.96", "ICD9-CM:338.19", "ICD9-CM:780.96", "ICD8:834.10", 
    "ICD8:821.71"]
Antidepressants_Codes = ["ATC:N06A"]
Antipsychotics_Codes = ["ATC:N05A"]
Mood_stabilizers_Codes = ["ATC:N03AG01", "ATC:N03AX09", "ATC:N03AF01", "ATC:N03AF02", "ATC:N05AN01"] 
ECT_Codes = ["BRXA1*", "BRTB1*"]
SuicideAttempt_Codes = ["main=ICD10:F;sub=ICD10:T36-ICD10:T50,ICD10:T52-ICD10:T60","main=ICD10:F;sub=ICD10:S51*,ICD10:S55*,ICD10:S59*,ICD10:S61*,ICD10:S65*,ICD10:S69*",
                      "ICD10:T39*","ICD10:T42*","ICD10:T43*","ICD10:T58*","ICD10:X60-ICD10:X84"]
SuicideCompleted_Codes = ["ICD10:X60-ICD10:X84","ICD10:Y87.0"]
MDD_ATC_Jorgensen_Codes = ["ATC:N06AB","ATC:N06AX11","ATC:N06AX03","ATC:N06AX16","ATC:N06AX21","ATC:N06AA","ATC:N06AF01","ATC:N06AG02","ATC:N06AX22","ATC:N06AX26","ATC:N06AX18","ATC:N06AH04","ATC:N06AH03","ATC:N06AX12","ATC:N06AX08","ATC:N06AN01"]

# --------------------------------------------------------------------------------------
# Utilities
# --------------------------------------------------------------------------------------

def BirthCountry_DK(x):
    x = pd.Series(x)  # allows both scalars and Series input

    conditions = [
        (x == 0),
        (x.between(101, 900)),
        (x.between(901, 961)),
        (x == 3999),
        (x.between(2401, 2599)),
        (x.between(4001, 4007)),
        (x.between(4301, 4499)),
        (x.between(4501, 4599)),
        (x.between(4601, 4687)),
        (x.between(4688, 4799)),
        (x.between(4801, 4989)),
        (x == 4998),
        (x == 4999),
        (x == 5001),
        (x == 5100),
        (x == 5101),
        (x == 5102),
        (x == 5103),
        (x.between(5104, 5902)),
        (x == 5999),
        (x.between(7001, 9348)),
        (x.between(9501, 9599)),
        (x == 9999)
    ]

    choices = [
        'Unknown',
        'Denmark',
        'Greenland',
        'Greenland',
        'Denmark',
        'Denmark',
        'Denmark',
        'Denmark',
        'Denmark',
        'Denmark',
        'Denmark',
        'Denmark',
        'Unknown',
        'Unknown',
        'Denmark',
        'Greenland',
        'Abroad',
        'Unknown',
        'Abroad',
        'Abroad',
        'Denmark',
        'Greenland',
        'Denmark'
    ]

    result = np.select(conditions, choices, default='Unknown')
    return result if len(result) > 1 else result[0]

def detect_ATC_status(df, colname="Disorder Codes"):
                """Return 'All', 'Some', or 'None' depending on ATC code presence."""
                if df.empty or colname not in df.columns:
                    return "None"
                col_values = df[colname].dropna().astype(str)
                if len(col_values) == 0:
                    return "None"
                if all(v.startswith("ATC") for v in col_values):
                    return "All"
                elif any(v.startswith("ATC") for v in col_values):
                    return "Some"
                else:
                    return "None"
                
def match_codes(series, codes, exact=False):
    if exact:
        return series.isin(codes)
    else:
        return series.apply(lambda d: any(d.startswith(p) for p in codes))
    
def generate_readme(flags_used, default_args, multiplePhenotypes=False, disclaimer_text="", additional_cols=[], selected_pickle=False, selected_PLINK=False, selected_FastGWA=False, Exclusions=False, phenotypes=[], ExDep=False):
    def parse_argstring_to_dict(argstring):
        #pattern = r"^(\w+):\s+(.+?)\s+\(default:\s+(.+?)\)$"
        arg_dict = {}
        
        for line in argstring.strip().splitlines():
            match = line.split(":") #re.match(pattern, line)
            if match:
                arg_name = match[0]
                value = match[1]
                value = value.split(" ")[1]
                #default_value = default_value.split(")")[0]
                #arg_dict[arg_name] = (value, default_value)
                arg_dict[arg_name] = (value)
        
        return arg_dict

    # Get argument from flags_used or fall back to default_args
    def get_arg(key):
        return parsed_flags_used.get(key) or parsed_default_flags.get(key, "UNKNOWN")

    parsed_flags_used = parse_argstring_to_dict(flags_used)
    parsed_default_flags = parse_argstring_to_dict(default_args)
    #print(f"[generate_readme] flags_used {flags_used}; parsed_flags: {parsed_flags_used}; default_args: {default_args}; parsed_default_flags: {parsed_default_flags}")
    
    # Extract all required values
    phenofile = get_arg("-g")
    iidcol = get_arg("--iidcol")
    sexcol = get_arg("--sexcol")
    birthdatecol = get_arg("--bdcol")
    statuscol = get_arg("--iidstatus")
    statuscoldate = get_arg("--iidstatusdate")

    curr_date = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    multiInfo = " of the first phenotype in the input" if multiplePhenotypes else ""

    # === HEADER ===
    readme = f"""# README

This file and the contents of the directory it is situated in, was generated by the script `get_pheno.py` on **{curr_date}** [1], which:

- Reads a list of phenotype definitions from the file `{phenofile}` and generates a Case/Control output.
- Outputs:
    - A TSV file with the results"""
    if selected_PLINK or selected_FastGWA:
        readme += "\n    - PLINK/FastGWA phenotype-based input"
    if selected_pickle:
        readme += "\n    - Python-native PICKLE output"

    readme += f"""

## General Information

- `MainPhenoName` refers to the main (first) phenotype requested. 
- Exclusions (Lifetime/Post/OneYearPrior) are currently only applied to the MainPheno.
- You can define exclusions separately for each phenotype like this (example based on MDD):

    MD   main=ICD10:F32;rule_out=ICD10:F25  
    Recurrent_MD    main=ICD:F33;rule_out=ICD10:F25

---

## Your requested phenotypes

{phenotypes}

---

## Your selected flags for this run

{flags_used}

---

## Output Columns (TSV)

| Column                           | Description                                                                                          |
|----------------------------------|------------------------------------------------------------------------------------------------------|
| {iidcol!s:<32} | Unique identifier for the individual                                                                 |
| diagnosis                        | Case/Control status{multiInfo!s:<82}|
| diagnoses                        | Codes used to generate the Case/Control status{multiInfo!s:<55}|
| in_dates                         | In-dates to each diagnosis code{multiInfo!s:<70}|
| out_dates                        | Out-dates to each diagnosis code{multiInfo!s:<69}|
| first_dx                         | First recorded diagnosis date{multiInfo!s:<72}|
| last_dx                          | Last recorded diagnosis date{multiInfo!s:<73}|
| n_diags                          | Number of diagnoses{multiInfo!s:<82}|
| n_unique_in_days                 | Unique days with recorded diagnoses{multiInfo!s:<66}|
| diagtype                         | Diagnosis type (if available)                                                                        |
| register                         | Register source (if available)                                                                       |"""

    # === PHENONAME (if multiple) ===
    if multiplePhenotypes:
        readme += """
| PhenoName(1-N)                   | Case/Control status for each phenotype                                                               |
| PhenoName_Codes(1-N)             | Codes used to define the phenotype                                                                   |
| PhenoName_In_Dates(1-N)          | In-dates per phenotype                                                                               |
| PhenoName_Out_Dates(1-N)         | Out-dates per phenotype                                                                              |
| PhenoName_earliest_date(1-N)     | Earliest diagnosis date per phenotype                                                                |
| PhenoName_latest_date(1-N)       | Latest diagnosis date per phenotype                                                                  |
| PhenoName_n_diags                | Number of diagnoses per phenotype                                                                    |
| PhenoName_n_unique_in_days       | Unique days of diagnosis per phenotype                                                               |"""
        for col in additional_cols:
            readme += f"\n| PhenoName_{col:<22} | Additional column {col} per phenotype                                                                    |"
        readme += """
| PhenoName_diagtype               | Diagnosis type info per phenotype [2]                                                                |
| PhenoName_register               | Source register per phenotype                                                                        |"""

    # === CORE DEMO INFO ===
    readme += f"""
| {sexcol:<32} | Gender (Male/Female)                                                                                 |
| {birthdatecol:<32} | Date of birth                                                                                        |
| dbds                             | TRUE/FALSE if from DBDS cohort (only in CHB/DBDS)                                                    |
| degen                            | TRUE/FALSE if from DEGEN cohort (only in CHB/DBDS)                                                   |
| Age_FirstDx                      | Age at first diagnosis                                                                               |"""
    
    # === IID Status ===
    if statuscol != "":
        readme += f"""
| {statuscol:<32} | Latest civil status (alive, dead, emigrated, etc.), see [3] for more information                     |"""    
    if statuscoldate != "":
        readme += f"""
| {statuscoldate:<32} | Date of latest civil status, see [3] for more information                                            |"""

    readme += """
| additional variables             | Other input columns (if any)                                                                         |"""

    # === EXCLUSIONS ===
    if Exclusions:
        readme += """
| ExclName(1-N)                    | Case/Control of the exclusion                                                                         |
| ExclName_Codes(1-N)              | Exclusion codes                                                                                      |
| ExclName_In_Dates(1-N)           | In-dates of exclusions                                                                               |
| ExclName_Out_Dates(1-N)          | Out-dates of exclusions                                                                              |
| ExclName_earliest_date(1-N)      | Earliest exclusion diagnosis                                                                         |
| ExclName_latest_date(1-N)        | Latest exclusion diagnosis                                                                           |
| ExclName_n_diags                 | Number of exclusion diagnoses                                                                         |
| ExclName_n_unique_in_days        | Unique days of a exclusion diagnoses                                                                    |"""
        for col in additional_cols:
            readme += f"\n| ExclName_{col:<23} | {col} status of the exclusions                                                             |"
        readme += """
| ExclName_diagtype                | Diagnosis type for each exclusion diagnosi                                                           |
| ExclName_register                | Register source for exclusions                                                                       |
| diagnoses_Level2_modifier        | Codes from exclusions that modified case/control status                                              |
| disorder_Level2_modifier         | Which exclusion altered the case/control                                                             |
| date_Level2_modifier             | Dates of exclusions that affected status                                                             |
| Level2_diagnoses                 | Resulting diagnoses after exclusions                                                                 |
| Level2_dates                     | Dates after exclusion                                                                                |
| Level2_ExclusionReason           | Reason for exclusion-based change                                                                    |
| Level2_FirstDx                   | First diagnosis after exclusion                                                                      |
| Level2_AgeExclusion              | Age-based exclusion status                                                                           |
| ExclName_Inflicted_changes_*     | All effects from exclusion (codes, dates, numbers)                                                   |
| MainPhenoName_diagnoses_in_percent_lost_due_to_ExclName | % of diagnoses lost to exclusion                                              |
| Level3_Age_FirstDx               | Age at diagnosis after Level3 exclusion                                                              |
| Level3_CaseControl               | Final case/control status after Level3                                                               |
| Level3_CaseControl_AgeExclusions | Age-based status after Level3                                                                        |
| Level3_Sankey                    | Sankey diagram (Level3) node label                                                                   |
| Level3_Sankey_data               | Sankey diagram data                                                                                  |
| Level3_Sankey_source             | Sankey source node                                                                                   |
| Level3_Sankey_target             | Sankey target node                                                                                   |
| Level3_Sankey_value              | Flow size between nodes                                                                              |"""

    # === FOOTER ===
    readme += f"""\n|----------------------------------|------------------------------------------------------------------------------------------------------|

---
"""
    # === EXCLUSIONS ===
    if ExDep:
        readme += """

## Phenotype Abbreviations
# If you used the --ExDepExcl flag you will find the following phenotypes given in your output file:
    Lifetime Exclusions:
        ID  : Intellectual Disability
        SCZ : Schizophrenia
        BPD : Bipolar Disorder  
    Time-Based (One year prior) Exclusions:
        AUD : Alcohol Use Disorder
        DUD : Drug Use Disorder
        MCI : Mild Cognitive Impairment
    Post onset Exclusions:
        DEM : Dementia
        CTI : Concurrent Terminal Illnesses
    Covariates:
        CP                  : Chronic Pain
        GAD                 : Generalized Anxiety Disorder
        PD                  : Panic Disorder
        Phobias
        PTSD                : Post-Traumatic Stress Disorder
        OCD                 : Obsessive Compulsive Disorder
        ADHD                : Attention-Deficite Hyperacitivity Disorder
        ASD                 : Autism-Spectrum Disorder
        Sleep_Disorder
        AD                  : Alzheimer's Disorder
        Pain                : Any type of Pain - Acute to Chronic
        Chronic_illness     : Any chronic illness
        Other_Mental_illness: All other F-Chapter codes that are not listed here
        BUL                 : Bulimia
        ED                  : Eating Disorders
        ANO                 : Anorexia
        ANX                 : Anxiety
        Dysthymia
        KenPain             : Kenneth Kendler's Pain definition
        Antidepressants
        Antipsychotics
        Mood_Stabilizers
        ECT                 : Codes for surgeries based on BRACA1
        Suicide_Attempt
        Suicide
        
---"""

    readme += f"""## Disclaimer

{disclaimer_text}

---

## Integrity Check

To ensure none of the files have been changed, run:

```bash
sha256sum -c ./checksums.sha256
```

[1]: https://github.com/MischaLundberg/GetPheno
[2]: https://www.esundhed.dk/Dokumentation/DocumentationExtended?id=5 (under t_diag and then c_diagtype)
[3]: https://www.dst.dk/da/Statistik/dokumentation/Times/cpr-oplysninger"""
    return(readme)


def remove_leading_icd(entry):
    """If the entry starts with any known ICD prefix, remove it once."""
    prefixes = ['ICD10:', 'ICD10-CM:', 'ICD9:', 'ICD9-CM:', 'ICD8:']
    res = entry
    if isinstance(entry, str):
        for prefix in prefixes:
            if entry.startswith(prefix):
                res = entry[len(prefix):]
            
    if verbose: 
        logger.debug(f"[remove_leading_icd] entry: {entry}; res: {res}")
    return res

def format_numeric(entry, Mode):
    """Formats a numeric entry as an ICD8 code."""
    # Set flag eM only if the mode is one of the specified ones.
    eM = Mode in DK_clusters
    entry_str = str(entry)
    integer_part, sep, decimal_part = entry_str.partition('.')
    # For the specified modes, pad the integer part to 3 digits.
    integer_formatted = integer_part.zfill(3) if eM and integer_part else integer_part
    # Pad the decimal part only if present and eM is True.
    decimal_formatted = decimal_part.zfill(2) if eM and decimal_part else decimal_part
    return f"{integer_formatted}.{decimal_formatted}"

def split_and_format(input_str, fill=False):
    # If input is numeric or a numeric string, return as string directly
    if isinstance(input_str, (int, float)) or (isinstance(input_str, str) and input_str.isnumeric()):
        return str(input_str)
    # Otherwise make sure it's a string
    input_str = str(input_str)
    # Use regex to split the string into letter, integer, and decimal parts
    match = SPLIT_PATTERN.match(input_str)
    if match:
        string_part = str(match.group(1))
        integer_part = str(match.group(2))
        decimal_part = str(match.group(3))
        if verbose:
            logger.debug(
                f"input: {input_str}; match: {match}; "
                f"string: {string_part}; integer: {integer_part}; decimal: {decimal_part}"
            )
        # Combine integer and decimal parts
        if decimal_part == 'None':
            combined_number = integer_part
        else:
            combined_number = integer_part + decimal_part
        # Ensure the combined number is 4 digits by adding trailing zeros if necessary
        if fill:
            combined_number = combined_number.ljust(4, '0')
        # Reconstruct the final string
        final_string = string_part + combined_number
        return final_string
    # If regex fails, just return the input as string (no error)
    return str(input_str)
    
# Function to get Memory usage
def usage():
    gc.collect()
    process = psutil.Process(os.getpid())
    mem_used = process.memory_info()[0] / float(1024 * 1024 * 1024) #2 ** 20)
    logger.info(f"Memory usage in GB: {str(mem_used)}")

def reformat_to_tsv(file):
    # Define the command to process the file
    command = f"""
    sed -e "s/\\[\\]//g" \
        -e "s/', '/,/g" \
        -e "s/\\['//g" \
        -e "s/'\\]//g" \
        -e "s/Timestamp('//g" \
        -e "s/ 00:00:00')//g" \
        -e "s/, /,/g" \
        -e "s/\\[//g" \
        -e "s/\\]//g" {file} > {file}.temp
    mv {file}.temp {file}
    """
    # Run the command
    subprocess.run(command, shell=True, check=True)

def _to_datetime_series(s: pd.Series, *, fmt: str, dayfirst: bool) -> pd.Series:
    """
    Robust parser:
    - trims whitespace and strips trailing '.0'
    - handles numeric YYYYMMDD (int/float)
    - if fmt given, try it first; then try a few common fallbacks
    """
    # Work on strings
    s_str = s.astype("string").str.strip().str.replace(r"\.0$", "", regex=True)

    out = pd.to_datetime(s_str, format=fmt, errors="coerce") if fmt else pd.to_datetime(
        s_str, errors="coerce", dayfirst=dayfirst
    )

    # Handle remaining NaT as possible YYYYMMDD numerics
    mask_nat = out.isna() & s_str.notna()
    if mask_nat.any():
        s_nat = s_str[mask_nat]
        # only digits and length 8  try %Y%m%d
        m8 = s_nat.str.fullmatch(r"\d{8}", na=False)
        if m8.any():
            out.loc[m8.index[m8]] = pd.to_datetime(s_nat[m8], format="%Y%m%d", errors="coerce")

    # Final gentle fallbacks for common human formats if still NaT and no explicit fmt
    if fmt is None:
        mask_nat = out.isna() & s_str.notna()
        if mask_nat.any():
            s_nat = s_str[mask_nat]
            for f in ("%Y-%m-%d", "%d/%m/%Y", "%m/%d/%Y"):
                try_more = pd.to_datetime(s_nat, format=f, errors="coerce")
                out.loc[try_more.notna().index[try_more.notna()]] = try_more[try_more.notna()]

    return out

def _as_list(x: Any) -> List[Any]:
    if isinstance(x, list):
        seq = x
    else:
        seq = [x]
    return [item for item in seq if str(item).strip() != ""]

def _to_dt_list(x: Any) -> List[pd.Timestamp]:
    arr = _as_list(x)
    if not arr:
        return []
    ts = pd.to_datetime(arr, errors="coerce")
    return [t for t in ts if not pd.isna(t)]

def normalize_iid_series(s, target="str"):
    """
    Normalize an IID column that may contain strings/floats like '100.0'.
    - target='int' -> pandas nullable Int64 (when all numeric)
    - target='str' -> pandas StringDtype (keeps leading zeros)
    NOTE: no longer prints to stdout; uses logger and does NOT mutate callers' data in-place.
    """
    s = pd.Series(s, dtype="string").str.strip().str.replace(r"\.0$", "", regex=True)

    if target == "str":
        return s

    # If any non-numeric values exist, fall back to string and warn
    if not s.str.fullmatch(r"\d+").all():
        logger.warning("Non-numeric IIDs detected, returning string-normalized series")
        return s

    out = pd.to_numeric(s, errors="coerce")
    return out.astype("Int64")

def normalize_iid_series_old(s, target="str"):
    """
    Normalize an IID column that may contain strings/floats like '100.0'.
    target='int' -> pandas nullable Int64
    target='str' -> string IDs (keeps leading zeros)
    """
    s = s.astype("string").str.strip().str.replace(r"\.0$", "", regex=True)

    if target == "str":
        return s

    # If any non-numeric values exist, fall back to string
    if not s.str.fullmatch(r"\d+").all():
        print(" Non-numeric IIDs detected, falling back to string normalization")
        return s

    out = pd.to_numeric(s, errors="coerce")
    return out.astype("Int64")

def normalize_iid_series_auto(s):
    s = pd.Series(s, dtype="string").str.strip().str.replace(r"\.0$", "", regex=True)
    if s.str.fullmatch(r"\d+").all():
        return pd.to_numeric(s, errors="coerce").astype("Int64")
    return s

def convert_if_not_datetime(val):
    # Handle missing values: return NaT for null values
    if pd.isnull(val):
        return pd.NaT
    # If already a pandas Timestamp or a Python datetime/date, just return it
    if isinstance(val, (pd.Timestamp, datetime, date)):
        return val
    # Otherwise, convert using the specified format with error coercion
    return pd.to_datetime(val, format=DateFormat, errors='coerce')

# Function to remove duplicates while preserving order
def remove_duplicates_preserve_order(lst):
    seen = set()
    result = []
    for item in lst:
        if item not in seen:
            result.append(item)
            seen.add(item)
    return result

# Function to generate random alphanumeric string
def generate_cpr_enc():
    characters = '0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ/+'
    return ''.join(random.choices(characters, k=40)) + "=="

# Function to generate random date between start_date and end_date
def generate_random_date(start_date, end_date):
    days_difference = (end_date - start_date).days
    random_days = random.randint(0, days_difference)
    return start_date + timedelta(days=random_days)

def setup_logger(script_name, to_console=True, to_file=None):
    logger = logging.getLogger(script_name)
    logger.setLevel(logging.DEBUG)  # master level, handlers will filter further

    # Common formatter
    formatter = logging.Formatter("[%(levelname)s] %(message)s")

    # Console handler
    if to_console:
        stream_handler = logging.StreamHandler(sys.stdout)
        stream_handler.setLevel(logging.INFO)
        stream_handler.setFormatter(formatter)
        logger.addHandler(stream_handler)

    # File handler
    if to_file is not None:
        fh = logging.FileHandler(to_file, mode="w")  # "a" if you want append
        fh.setLevel(logging.DEBUG)   # or INFO, depends on your needs
        fh.setFormatter(formatter)
        logger.addHandler(fh)

    return logger

# --------------------------------------------------------------------------------------
# Main functions 
# ------------------------------------------------------------------------------------
#need to handle double counts and exclusions
def main(lpr_file, pheno_request, stam_file, addition_information_file, use_predefined_exdep_exclusions, general_exclusions, diagnostic_col, 
         pheno_requestcol, iidcol, birthdatecol, sexcol, fsep, isep, jsep, gsep, outfile, exact_match, input_date_in_name, input_date_out_name, qced_iids, 
         ctype_excl, ctype_incl, ctype_col, lifetime_exclusions_file, post_exclusions_file, oneYearPrior_exclusions_file, exclCHBcontrols, Filter_YoB, 
         Filter_Gender, verbose_arg, Build_Test_Set, test_run, MatchFI, skip_icd_update, DateFormat_in, iidstatus_col, iidstatusdate, selectIIDs, remove_point_in_diag_request, 
         num_threads, main_pheno_name, BuildEntryExitDates, build_ophold, write_pickle, write_fastGWA_format, write_Plink2_format, lpr_cols_to_read_as_date, 
         stam_cols_to_read_as_date, MinMaxAge, ICDCM, load_precreated_phenotypes, RegisterRun, lowMem, batchsize, noLeadingICD, lpr2nd_file, lpr_recnummer, lpr2nd_recnummer, 
         diagnostic2nd_col, atc_file, atc_diag_col, atc_date, atc_datecols, runLPRonly, runPSYKonly, opholdsep, ophold_file, inifile, only_ICD8_arg, only_ICD9_arg, only_ICD10_arg, 
         BuildIndex, IndexDtypes, icdprefix, argstring, defaultargs, default_argstring):
    
    ## Add global variables
    global min_Age
    global max_Age
    global only_ICD10
    global only_ICD9
    global only_ICD8
    global cluster_run
    global verbose
    global dta_input
    global lifetime_exclusions
    global oneYearPrior_exclusions
    global post_exclusions
    global covariates
    global DateFormat
    global DayFirst
    global logger

    
    ## Initialize global variables based on userinput
    min_Age = int(MinMaxAge.split(',')[0])
    max_Age = int(MinMaxAge.split(',')[1])
    only_ICD10 = only_ICD10_arg
    only_ICD9 = only_ICD9_arg
    only_ICD8 = only_ICD8_arg
    verbose = verbose_arg
    DateFormat = DateFormat_in
    logger = setup_logger(script_name, to_console=verbose, to_file=outfile+".log")

    ## Initialize local variables
    num_threads = int(num_threads)
    dta_input=False
    runPSYKonly = False
    runLPRonly = False
    processed_ophold_file = "" 
    potential_lpr_cols_to_read_as_date = []
    atc_date_col = ""
    atc_cols_to_read_as_date = []
    cluster_run = "Default"
    h5_exist = False

    
    logger.info(disclaimer_text)

    if (Version(pd.__version__) < Version(pd_min_version)):
        logger.info(f"[main] ERROR: Current Pandas is at version {pd.__version__} but should be >={pd_min_version}")
        sys.exit()
    if (Version(pd.__version__) < Version(pd_recommended_version)):
        logger.info(f"[main] WARNING: Current Pandas is at version {pd.__version__} to work without warnings, it should be >={pd_recommended_version}")
        if not verbose:
            print(f"[main] WARNING: Current Pandas is at version {pd.__version__} to work without warnings, it should be >={pd_recommended_version}")
    if (Version(('{0[0]}.{0[1]}.{0[2]}'.format(sys.version_info))) < Version(python_min_version)):
        logger.info(f"[main] ERROR: Current Python is at version {('{0[0]}.{0[1]}.{0[2]}'.format(sys.version_info))} but should be >={python_min_version}")
        sys.exit()
    
    if RegisterRun:
        logger.info("[main] --RegisterRun is deprecated!")
    
    #TODO: Add instead of hardcoded.
    if atc_date != "":
        logger.info("[main] --atc_date is not yet in use!")
    #TODO: Add instead of hardcoded.
    if atc_datecols != "":
        logger.info("[main] --atc_datecols is not yet in use!")

    #TODO: Use MinMaxAge to set the variables for the exclusion criteria.
    
    if (Build_Test_Set):
        logger.info(f"[main] Generating a test dataset and store it in the directory selected with -o {outfile}")
        generate_test_dataset(outfile)
        logger.info("[main] Finished generating the test data. It is stored in the directory selected with -o.")
        sys.exit()
    

    if(not outfile.endswith((".tsv",".csv",".txt"))):
        outfile = outfile + ".tsv"
    #rename the output file to show that this is based on only the subset of ICD8/9/10 codes from the
    if(only_ICD8):
        outfile = outfile.rpartition('.')[0] + ".icd8.tsv"
    elif(only_ICD9):
        outfile = outfile.rpartition('.')[0] + ".icd9.tsv"
    elif(only_ICD10):
        outfile = outfile.rpartition('.')[0] + ".icd10.tsv"

    
    # Get the local hostname and set on which cluster we are working if known
    hostname = socket.gethostname()
    if ("cld065" in hostname or "dprhdbds" in hostname ):
        cluster_run = "CHB_DBDS"
        logger.info("[main] If not done, consider to use --DiagTypeExclusions \"H,M\", as this will exclude H (referral) and M (temporary) diagnoses.")
    if ("srvfsencrr" in hostname):
        cluster_run = "NCRR_DST"
        dta_input = True
        logger.info("[main] If not done, consider to use --DiagTypeExclusions \"H,M\", as this will exclude H (referral) and M (temporary) diagnoses.")
    if ("fe-ipsych-" in hostname or "cn-1" in hostname or "s21" in hostname or "dpibp" in hostname):
        cluster_run = "iPSYCH"
    if ("dpibp" in hostname):
        cluster_run = "IBP_computerome"
        logger.info("[main] If not done, consider to use --DiagTypeExclusions \"H,M\", as this will exclude H (referral) and M (temporary) diagnoses.")
    if ("srvngcrhbio1" in hostname):
        cluster_run = "IBP_DST"
        logger.info("[main] If not done, consider to use --DiagTypeExclusions \"H,M\", as this will exclude H (referral) and M (temporary) diagnoses.")
    if ("TOFILL" in hostname): #TODO
        cluster_run = "DBDS_DST"
        logger.info("[main] If not done, consider to use --DiagTypeExclusions \"H,M\", as this will exclude H (referral) and M (temporary) diagnoses.")

    
    # Based on cluster, set ini name if not given.
    if ((cluster_run in DK_clusters or cluster_run == "Default") and inifile == ""):
        inifile = "get_pheno.ini"


    # Load the configuration
    config = load_config(filename=inifile)
    if config is None or not os.path.exists(inifile):
        logger.info("[main] No configuration file found. Using built-in defaults and your settings.")
        # Set your defaults here or handle as needed.
    else:
        print(f"[main] Loading config file {inifile}")
        # For example, to load the NCRR_DST configuration:
        if config.has_section(cluster_run):
            # For list values, split by comma and strip any whitespace.
            if("-i" in defaultargs and config.has_option(cluster_run, "stam_file")):
                stam_file = config.get(cluster_run, "stam_file")
                argstring = argstring+(f"-i: {stam_file} (Set by loading config file)\n")
            if("--iDates" in defaultargs and config.has_option(cluster_run, "stam_cols_to_read_as_date")):
                stam_cols_to_read_as_date = [col.strip() for col in config.get(cluster_run, "stam_cols_to_read_as_date").split(",")]
                argstring = argstring+(f"--iDates: {stam_cols_to_read_as_date} (Set by loading config file)\n")
            if("-f" in defaultargs and config.has_option(cluster_run, "lpr_file")):
                lpr_file = config.get(cluster_run, "lpr_file")
                argstring = argstring+(f"-f: {lpr_file} (Set by loading config file)\n")
            if("--f2" in defaultargs and config.has_option(cluster_run, "lpr2nd_file")):
                lpr2nd_file = config.get(cluster_run, "lpr2nd_file")
                argstring = argstring+(f"--f2: {lpr2nd_file} (Set by loading config file)\n")
            if("--fDates" in defaultargs and config.has_option(cluster_run, "lpr_cols_to_read_as_date")):
                lpr_cols_to_read_as_date = [col.strip() for col in config.get(cluster_run, "lpr_cols_to_read_as_date").split(",")]
                argstring = argstring+(f"--fDates: {lpr_cols_to_read_as_date} (Set by loading config file)\n")
            if("-j" in defaultargs and config.has_option(cluster_run, "addition_information_file")):
                addition_information_file = config.get(cluster_run, "addition_information_file")
                argstring = argstring+(f"-j: {addition_information_file} (Set by loading config file)\n")
            if("--fcol" in defaultargs and config.has_option(cluster_run, "diagnostic_col")):
                diagnostic_col = config.get(cluster_run, "diagnostic_col")
                argstring = argstring+(f"--fcol: {diagnostic_col} (Set by loading config file)\n")
            if("--gcol" in defaultargs and config.has_option(cluster_run, "pheno_requestcol")):
                pheno_requestcol = config.get(cluster_run, "pheno_requestcol")
                argstring = argstring+(f"--gcol: {pheno_requestcol} (Set by loading config file)\n")
            if("--bdcol" in defaultargs and config.has_option(cluster_run, "birthdatecol")):
                birthdatecol = config.get(cluster_run, "birthdatecol")
                argstring = argstring+(f"--bdcol: {birthdatecol} (Set by loading config file)\n")
            if("--iidcol" in defaultargs and config.has_option(cluster_run, "iidcol")):
                iidcol = config.get(cluster_run, "iidcol")
                argstring = argstring+(f"--iidcol: {iidcol} (Set by loading config file)\n")
            if("--fsep" in defaultargs and config.has_option(cluster_run, "fsep")):
                fsep = config.get(cluster_run, "fsep")
                argstring = argstring+(f"--fsep: {fsep} (Set by loading config file)\n")
            if("--gsep" in defaultargs and config.has_option(cluster_run, "gsep")):
                gsep = config.get(cluster_run, "gsep")
                argstring = argstring+(f"--gsep: {gsep} (Set by loading config file)\n")
            if("--isep" in defaultargs and config.has_option(cluster_run, "isep")):
                isep = config.get(cluster_run, "isep") 
                argstring = argstring+(f"--isep: {isep} (Set by loading config file)\n")
            if("--jsep" in defaultargs and config.has_option(cluster_run, "jsep")):
                jsep = config.get(cluster_run, "jsep") 
                argstring = argstring+(f"--jsep: {jsep} (Set by loading config file)\n")
            if("--din" in defaultargs and config.has_option(cluster_run, "input_date_in_name")):
                input_date_in_name = config.get(cluster_run, "input_date_in_name")
                argstring = argstring+(f"--din: {input_date_in_name} (Set by loading config file)\n")
            if("--don" in defaultargs and config.has_option(cluster_run, "input_date_out_name")):
                input_date_out_name = config.get(cluster_run, "input_date_out_name")
                argstring = argstring+(f"--don: {input_date_out_name} (Set by loading config file)\n")
            if("--ge" in defaultargs and config.has_option(cluster_run, "general_exclusions")):
                general_exclusions = config.get(cluster_run, "general_exclusions")
                argstring = argstring+(f"--ge: {general_exclusions} (Set by loading config file)\n")
            if("--recnum" in defaultargs and config.has_option(cluster_run, "lpr_recnummer")):
                lpr_recnummer = config.get(cluster_run, "lpr_recnummer")
                argstring = argstring+(f"--recnum: {lpr_recnummer} (Set by loading config file)\n")
            if("--recnum2" in defaultargs and config.has_option(cluster_run, "lpr2nd_recnummer")):
                lpr2nd_recnummer = config.get(cluster_run, "lpr2nd_recnummer")
                argstring = argstring+(f"--recnum2: {lpr2nd_recnummer} (Set by loading config file)\n")
            if("--f2col" in defaultargs and config.has_option(cluster_run, "diagnostic2nd_col")):
                diagnostic2nd_col = config.get(cluster_run, "diagnostic2nd_col")
                argstring = argstring+(f"--f2col: {diagnostic2nd_col} (Set by loading config file)\n")
            if("--Ophold" in defaultargs and config.has_option(cluster_run, "ophold_file")):
                ophold_file = config.get(cluster_run, "ophold_file")
                argstring = argstring+(f"--Ophold: {ophold_file} (Set by loading config file)\n")
            if("--ophsep" in defaultargs and config.has_option(cluster_run, "opholdsep")):
                opholdsep = config.get(cluster_run, "opholdsep")
                argstring = argstring+(f"--ophsep: {opholdsep} (Set by loading config file)\n")
            if(config.has_option(cluster_run, "processed_ophold_file")):
                processed_ophold_file = config.get(cluster_run, "processed_ophold_file")
                argstring = argstring+(f"processed_ophold_file: {processed_ophold_file} (Set by loading config file)\n")
            if config.has_option(cluster_run, "runLPRonly"):    
                runLPRonly = config.get(cluster_run, "runLPRonly")
                argstring = argstring+(f"--LPR: {runLPRonly} (Set by loading config file)\n")
            if config.has_option(cluster_run, "runPSYKonly"):       
                runPSYKonly = config.get(cluster_run, "runPSYKonly")
                argstring = argstring+(f"--PSYK: {runPSYKonly} (Set by loading config file)\n")
            if("--DateFormat" in defaultargs and config.has_option(cluster_run, "DateFormat")):
                DateFormat = config.get(cluster_run, "DateFormat")
                argstring = argstring+(f"--DateFormat: {DateFormat} (Set by loading config file)\n")
            if("--removePointInDiagCode" in defaultargs and config.has_option(cluster_run, "remove_point_in_diag_request")):
                remove_point_in_diag_request = config.get(cluster_run, "remove_point_in_diag_request")
                argstring = argstring+(f"--removePointInDiagCode: {remove_point_in_diag_request} (Set by loading config file)\n")
            if("--noLeadingICD" in defaultargs and config.has_option(cluster_run, "noLeadingICD")):
                noLeadingICD = config.get(cluster_run, "noLeadingICD")
                argstring = argstring+(f"--noLeadingICD: {noLeadingICD} (Set by loading config file)\n")
            if("--noLeadingICD" in defaultargs and config.has_option(cluster_run, "remove_ICD_naming")):
                noLeadingICD = config.get(cluster_run, "remove_ICD_naming")
                argstring = argstring+(f"--noLeadingICD: {noLeadingICD} (Set by loading config file)\n")
            if("--sexcol" in defaultargs and config.has_option(cluster_run, "sexcol")):
                sexcol = config.get(cluster_run, "sexcol")
                argstring = argstring+(f"--sexcol: {sexcol} (Set by loading config file)\n")
            if("--atc" in defaultargs and config.has_option(cluster_run, "atc_file")):
                atc_file = config.get(cluster_run, "atc_file")
                argstring = argstring+(f"--atc: {atc_file} (Set by loading config file)\n")
            if("--atccol" in defaultargs and config.has_option(cluster_run, "atc_diag_col")):
                atc_diag_col = config.get(cluster_run, "atc_diag_col")
                argstring = argstring+(f"--atccol: {atc_diag_col} (Set by loading config file)\n")
            if("--atcdatecol" in defaultargs and config.has_option(cluster_run, "atc_date_col")):
                atc_date_col = config.get(cluster_run, "atc_date_col")
                argstring = argstring+(f"--atcdatecol: {atc_date_col} (Set by loading config file)\n")
            if("--atcDates" in defaultargs and config.has_option(cluster_run, "atc_cols_to_read_as_date")):
                atc_cols_to_read_as_date = config.get(cluster_run, "atc_cols_to_read_as_date")
                argstring = argstring+(f"--atcDates: {atc_cols_to_read_as_date} (Set by loading config file)\n")
            if("--DiagTypeExclusions" in defaultargs and config.has_option(cluster_run, "DiagTypeExclusions")):
                ctype_excl = config.get(cluster_run, "DiagTypeExclusions")
                argstring = argstring+(f"--DiagTypeExclusions: {ctype_excl} (Set by loading config file)\n")
            if("--DiagTypeInclusions" in defaultargs and config.has_option(cluster_run, "DiagTypeInclusions")):
                ctype_incl = config.get(cluster_run, "DiagTypeInclusions")
                argstring = argstring+(f"--DiagTypeInclusions: {ctype_incl} (Set by loading config file)\n")
            if("--DiagTypecol" in defaultargs and config.has_option(cluster_run, "DiagTypecol")):
                ctype_col = config.get(cluster_run, "DiagTypecol")
                argstring = argstring+(f"--DiagTypecol: {ctype_col} (Set by loading config file)\n")
            if("--lowmem" in defaultargs and config.has_option(cluster_run, "lowmem")):
                lowMem = config.get(cluster_run, "lowmem")
                argstring = argstring+(f"--lowmem: {lowMem} (Set by loading config file)\n")
            if("--batchsize" in defaultargs and config.has_option(cluster_run, "batchsize")):
                batchsize = config.get(cluster_run, "batchsize")
                argstring = argstring+(f"--batchsize: {batchsize} (Set by loading config file)\n")
            if("--icdprefix" in defaultargs and config.has_option(cluster_run, "icdprefix")):
                icdprefix = config.get(cluster_run, "icdprefix")
                argstring = argstring+(f"--icdprefix: {icdprefix} (Set by loading config file)\n")
            if("--iidstatus" in defaultargs and config.has_option(cluster_run, "iidstatus")):
                iidstatus = config.get(cluster_run, "iidstatus")
                argstring = argstring+(f"--iidstatus: {iidstatus} (Set by loading config file)\n")
            if("--iidstatusdate" in defaultargs and config.has_option(cluster_run, "iidstatusdate")):
                iidstatusdate = config.get(cluster_run, "iidstatusdate")
                argstring = argstring+(f"--iidstatusdate: {iidstatusdate} (Set by loading config file)\n")
            if("--IndexDtypes" in defaultargs and config.has_option(cluster_run, "IndexDtypes")):
                IndexDtypes = config.get(cluster_run, "IndexDtypes")
                argstring = argstring+(f"--IndexDtypes: {IndexDtypes} (Set by loading config file)\n")
            # Continue similarly for other sections or add logic to choose the right one.
    
    if IndexDtypes != "":
        index_dtypes = dtypes = json.loads(IndexDtypes) if IndexDtypes else None
    else:
        index_dtypes = {
                # integer columns (nullable Int64 allows NA)
                iidcol:        "int",
                "c_pattype":  "float",

                # string columns (pandas StringDtype allows NA)
                diagnostic_col:    "string",
                "c_diagtype": "string",
                "register":   "string",
                "source":     "string",
                # date columns (datetime64[ns] supports NaT)
                input_date_in_name:   "datetime64[ns]",
                input_date_out_name:    "datetime64[ns]"
            }
        
    # Fix separator issues
    if isep in ["\\t", r"\t"]:
        isep = "\t"
    if jsep in ["\\t", r"\t"]:
        jsep = "\t"
    if fsep in ["\\t", r"\t"]:
        fsep = "\t"
    
    if DateFormat.startswith("%d"):
        DayFirst=True
    logger.info(f"[main] DayFirst: {DayFirst}; DateFormat {DateFormat}")

    #TODO: replace when using our own files where we have the source indicated.
    if runLPRonly:
        file_paths = lpr_file.split(',') if ',' in lpr_file else [lpr_file]
        if (len(file_paths) == 2):
            lpr_file = file_paths[1] #Use the second file - by convention, this should be LPR
        if(lpr2nd_file != ""):
            file_paths = lpr2nd_file.split(',') if ',' in lpr2nd_file else [lpr2nd_file]
            if (len(file_paths) == 2):
                lpr2nd_file = file_paths[1]

    if runPSYKonly:
        file_paths = lpr_file.split(',') if ',' in lpr_file else [lpr_file]
        if (len(file_paths) == 2):
            lpr_file = file_paths[0] #Use the first file - by convention, this should be PSYK
        if(lpr2nd_file != ""):
            file_paths = lpr2nd_file.split(',') if ',' in lpr2nd_file else [lpr2nd_file]
            if (len(file_paths) == 2):
                lpr2nd_file = file_paths[0]

    if (test_run):
        use_predefined_exdep_exclusions = True
        #dbds_run = True
        #ipsych_run = False
        lpr_file = "./lpr_file.csv"
        stam_file = "./stam_file.csv"
        addition_information_file = "./addition_information_file.csv"
        pheno_request = "./sample_pheno_request.txt"
        iidcol = "cpr_enc"
        diagnostic_col = "diagnosis"
        pheno_requestcol = "diagnosis"
        fsep = ","
        gsep = "\t"
        input_date_in_name = "date_in"
        input_date_out_name = "date_out"
        general_exclusions = ""
        qced_iids = ""

    
    print(hostname+"; TestRun: "+str(test_run)+"; Cluster: "+cluster_run)

    # Set some potential columns that could contain dates
    potential_cols_to_read_as_date = [input_date_in_name,input_date_out_name,birthdatecol,"date_in","date_out","birthdate","date_of_birth","statd","fdato","d_inddto","d_uddto","fdato_m","fdato_f","statd_m","statd_f"]
    potential_lpr_cols_to_read_as_date = potential_cols_to_read_as_date
    if (not lpr_cols_to_read_as_date):
        logger.info("[main] no -f dates supplied, checking predefined colnames.")
    else:
        if not type(lpr_cols_to_read_as_date) == list:
            lpr_cols_to_read_as_date = lpr_cols_to_read_as_date.split(",")
        logger.info(f"[main] -f dates supplied, setting them now for later use: {lpr_cols_to_read_as_date}")
    lpr_cols_to_read_as_date = sorted(
        set(lpr_cols_to_read_as_date + potential_lpr_cols_to_read_as_date)
    )
    if (not stam_cols_to_read_as_date):
        logger.info("[main] no -i dates supplied, checking predefined colnames.")
    else:
        logger.info("[main] -i dates supplied, setting them now for later use.")
        if not type(stam_cols_to_read_as_date) == list:
            stam_cols_to_read_as_date = stam_cols_to_read_as_date.split(",")
    stam_cols_to_read_as_date = sorted(
        set(stam_cols_to_read_as_date + potential_cols_to_read_as_date)
    )

    if (lpr_file == "" or pheno_request == "" or stam_file == ""):
        logger.info("[main] ERROR: Either lpr_file , stam_file or pheno_request file is not given. Exiting")
        sys.exit()

    if verbose:
        logger.debug("[main] Mem before loading all input:")
        usage()

    if BuildIndex:
        print("[main] Building Index")
        index_diag_file(
            input_csv=lpr_file,
            separator=fsep,
            index_columns=[iidcol,diagnostic_col],
            dtypes = index_dtypes
        )
        if (atc_file != ""):
            index_diag_file(atc_file,fsep,atc_diag_col)
        if (lpr2nd_file != ""):
            logger.info("[main] WARNING: --BuildIndex only works with -f and --atc, but not with --f2.")
            sys.exit()
            #index_diag_file(lpr2nd_file,fsep,diagnostic2nd_col)
        sys.exit()


    # Load the pheno request file to do the check thereafter.
    # Check if the phenotype request is based on Codes or based on a already extracted phenotype that fits the specified set of columns.
    if (load_precreated_phenotypes):
        print("[main] Loading precreated phenotypes")
        # initialize the not needed dataframes
        df1 = pd.DataFrame() #lpr file
        df3 = pd.DataFrame() #stam file
        df4 = pd.DataFrame() #additional information file
        # Load input files
        casecontrol_df = pd.read_csv(pheno_request, engine='python', sep=fsep, dtype=object, dayfirst=DayFirst, date_format=DateFormat, parse_dates=lpr_cols_to_read_as_date)
        logger.info("#"*50)
        logger.info("#"*50)
        logger.info("[main] You are loading pre-created phenotype files to then let us do some manipulation and combining them.")
        logger.info("     This relies on certain data format assumptions:")
        logger.info(f"     We will use the following columns (you can change this through the adequate flag) iidcol={iidcol}, diagnostic_col={diagnostic_col}, pheno_requestcol={pheno_requestcol}, birthdatecol={birthdatecol}, sexcol={sexcol}, input_date_in_name={input_date_in_name}, input_date_out_name={input_date_out_name} ")
        logger.info("     Furthermore, when there are the following columns available, we will use them too: ...........")
        logger.info("     The format of your input to --LifetimeExclusion, --PostExclusion, and --OneyPriorExclusion is assumed to be in form of (tab between the two columns; no header)")
        logger.info("DISORDERNAME /path/to/file/for/DISORDERNAME")
        logger.info("#"*50)
        logger.info("#"*50)
        #TODO: add all options and integrate them into the workflow
        logger.info("[main] ERROR: This option is not yet implemented.")
        sys.exit()

        #use_predefined_exdep_exclusions
        #pheno_request, lifetime_exclusions_file, post_exclusions_file, oneYearPrior_exclusions_file, main_pheno_name, MinMaxAge, DateFormat_in
        minimal_viable_set_of_pheno_cols = [iidcol, diagnostic_col, pheno_requestcol, birthdatecol, sexcol, input_date_in_name, input_date_out_name ]
        # This requires to be a tab separated file with the phenotype file name to be in the first column
        ##  MDD /path/to/data/mdd.pheno.tsv

        # Read the files for pheno and exclusions 
        ## Process them like usually
    else:
        print(f"[main] Loading phenotypes from file {pheno_request}")
        in_pheno_codes, multi_inclusions, normalized_pheno = load_phenotypes(pheno_request=pheno_request, pheno_requestcol=pheno_requestcol, pheno_name=main_pheno_name, icdprefix=icdprefix, noLeadingICD=noLeadingICD, ICDCM=ICDCM, 
                                                           skip_icd_update=skip_icd_update, exact_match=exact_match, remove_point_in_diag_request=remove_point_in_diag_request)
        # assert isinstance(in_pheno_codes.iloc[0]["Disorder Codes"], list)
        ### START NEW
        # Safe accessor for first phenotype entry
        if isinstance(in_pheno_codes, dict):
            # Advanced format
            first_key = next(iter(in_pheno_codes))
            disorder_codes = in_pheno_codes[first_key]
            logger.info(f"[main] Using advanced phenotype format, first disorder = {first_key}, codes = {disorder_codes}")
        else:
            # Standard 2-column DataFrame format
            disorder_codes = in_pheno_codes.iloc[0]["Disorder Codes"]
            logger.info(f"[main] Using standard phenotype format, first disorder = {in_pheno_codes.iloc[0]['Disorder']}, codes = {disorder_codes}")
        ### END NEW
        logger.info("[main] Finished loading -g file")
        if not verbose:
            print(f"[main] Finished loading -g file")
        if verbose:
            logger.debug("[main] Mem after loading pheno_request input:")
            usage()
        # Load STAM file (-i)
        df3 = load_stam_file(stam_file, isep, birthdatecol, diagnostic_col, sexcol, stam_cols_to_read_as_date)
        logger.info("[main] Finished loading -i file(s)")
        if not verbose:
            print("[main] Finished loading -i file(s)")
        if verbose:
            logger.debug(f"[main] Header of your -i file: {df3.head(5)}")
        logger.info(df3.head(5))
        # Check if input is plausible (e.g. contains every IID only once)
        if (len(df3) > len(df3[iidcol].unique())):
            logger.info("[main] WARNING: Your input file -i contains duplicated IIDs. If this should not be the case, please check your input!")
        if verbose:
            logger.debug(df3.columns)
            logger.debug("[main] Mem after loading stam_file input:")
            usage()
        if (cluster_run in DK_clusters):
            if (build_ophold):
                try:
                    if dta_input:
                        ophold = pd.read_stata(ophold_file, sep=opholdsep, dtype=object)
                    else:
                        ophold = pd.read_csv(ophold_file, sep=opholdsep, dtype=object)
                except TypeError:
                    ophold = pd.read_csv(ophold_file, engine='python', sep=opholdsep, dtype=object)
                df3 = process_ophold(ophold, df3, "", processed_ophold_file, birthdatecol, iidcol, verbose)
            else:
                if (os.path.isfile(processed_ophold_file)):
                    try:
                        ophold = pd.read_csv(processed_ophold_file, sep=opholdsep, dtype=object)
                    except TypeError:
                        ophold = pd.read_csv(processed_ophold_file, engine='python', sep=opholdsep, dtype=object)
                    df3 = pd.merge(df3,ophold,how="left", on=iidcol)
        n_stam_iids = df3[iidcol].nunique()
        gc.collect()
        # Initialize the additional data file
        df4 = pd.DataFrame()
        # Load the fourth file as a DataFrame(should be a file with additional rows of information. This will not be merged but appended to df1) 
        if (addition_information_file != ''):
            df4 = load_stam_file(addition_information_file, jsep, birthdatecol, diagnostic_col, sexcol, stam_cols_to_read_as_date)
            if(("birthdate" in df3.columns) and "birthdate" in df4.columns):# or "birthdate" in df1.columns) and "birthdate" in df4.columns):
                df4.drop("birthdate",inplace=True, axis=1)
            if(("diagnosis" in df3.columns) and "diagnosis" in df4.columns):# or "diagnosis" in df1.columns) and "diagnosis" in df4.columns):
                df4.drop("diagnosis",inplace=True, axis=1)
        #in_pheno_codes, normalized_pheno = dict_update_icd_coding(in_pheno_codes, exact_match, skip_icd_update, remove_point_in_diag_request, ICDCM, noLeadingICD, icdprefix)
        pheno_request = in_pheno_codes
        if (use_predefined_exdep_exclusions):
            if not verbose:
                print("[main] Using predefined ExDEP Exclusions")
            lifetime_exclusions = pd.DataFrame(columns=['Disorder','Disorder Codes'])
            oneYearPrior_exclusions = pd.DataFrame(columns=['Disorder','Disorder Codes'])
            post_exclusions = pd.DataFrame(columns=['Disorder','Disorder Codes'])
            covariates = pd.DataFrame(columns=['Disorder','Disorder Codes'])
            #Lifetime exclusions
            lifetime_exclusions.loc[0] = ["ID", ID_Codes]
            lifetime_exclusions.loc[1] = ["SCZ", SCZ_Codes]
            lifetime_exclusions.loc[2] = ["BPD", BPD_Codes]
            lifetime_exclusions, normalized_lifetime = dict_update_icd_coding(lifetime_exclusions, exact_match, skip_icd_update, remove_point_in_diag_request, ICDCM, noLeadingICD, icdprefix)
            #One year prior exclusions
            oneYearPrior_exclusions.loc[0] = ["AUD", AUD_Codes]
            oneYearPrior_exclusions.loc[1] = ["DUD", DUD_Codes]
            oneYearPrior_exclusions.loc[2] = ["MCI", MCI_Codes]
            oneYearPrior_exclusions, normalized_prior = dict_update_icd_coding(oneYearPrior_exclusions, exact_match, skip_icd_update, remove_point_in_diag_request, ICDCM, noLeadingICD, icdprefix)
            #Post onset exclusions
            post_exclusions.loc[0] = ["DEM", DEM_Codes]
            post_exclusions.loc[1] = ["CTI", CTI_Codes]
            post_exclusions, normalized_post = dict_update_icd_coding(post_exclusions, exact_match, skip_icd_update, remove_point_in_diag_request, ICDCM, noLeadingICD, icdprefix)
            #Covariates
            covariates.loc[0] = ["CP", CP_Codes]
            covariates.loc[1] = ["GAD", GAD_Codes]
            covariates.loc[2] = ["PD", PD_Codes]
            covariates.loc[3] = ["Phobias", Phobias_Codes]
            covariates.loc[4] = ["PTSD", PTSD_Codes]
            covariates.loc[5] = ["OCD", OCD_Codes]
            covariates.loc[6] = ["ADHD", ADHD_Codes]
            covariates.loc[7] = ["ASD", ASD_Codes]
            covariates.loc[8] = ["Sleep_Disorder", Sleep_Disorder_Codes]
            covariates.loc[9] = ["AD", AD_Codes]
            covariates.loc[10] = ["Pain", Pain_Codes]
            covariates.loc[11] = ["Chronic_illness", Chronic_Codes]
            covariates.loc[12] = ["Other_Mental_illness", Other_Mental_Codes]
            covariates.loc[13] = ["BUL", BUL_Codes]
            covariates.loc[14] = ["ANO", ANO_Codes]
            covariates.loc[15] = ["ANX", ANX_Codes]
            covariates.loc[16] = ["ED", Eating_Disorder_Codes]
            covariates.loc[17] = ["Dysthymia", Dysthymia_Codes]
            covariates.loc[18] = ["KenPain", Ken_Pain_Codes]
            covariates.loc[19] = ["Antidepressants", Antidepressants_Codes]
            covariates.loc[20] = ["Antipsychotics", Antipsychotics_Codes]
            covariates.loc[21] = ["Mood_Stabilizers", Mood_stabilizers_Codes]
            covariates.loc[22] = ["ECT", ECT_Codes]
            covariates.loc[23] = ["Suicide_Attempt", SuicideAttempt_Codes]
            covariates.loc[24] = ["Suicide", SuicideCompleted_Codes]
            logger.info("[main] Running [dict_update_icd_coding]")
            if not verbose:
                print("[main] Running [dict_update_icd_coding]")
            covariates, normalized_covars = dict_update_icd_coding(covariates, exact_match, skip_icd_update, remove_point_in_diag_request, ICDCM, noLeadingICD, icdprefix)
            # Determine presence of ATC codes
            global ATC_Requested 
            # --- Apply to each DataFrame ---
            ATC_Status = {
                "lifetime_exclusions": detect_ATC_status(lifetime_exclusions),
                "oneYearPrior_exclusions": detect_ATC_status(oneYearPrior_exclusions),
                "post_exclusions": detect_ATC_status(post_exclusions),
                "covariates": detect_ATC_status(covariates)
            }
            print
            if ATC_Requested == "None" and ("All" in ATC_Status.values() or "Some" in ATC_Status.values()):
                ATC_Requested = "Some"

        if selectIIDs != "" and os.path.exists(selectIIDs):
            # read iid file
            iids = pd.read_csv(selectIIDs, header=None)[0]
            # normalize
            iids = normalize_iid_series(iids, target="str").tolist()
            # filter df3
            df3 = df3[df3[iidcol].astype("string").isin(iids)]

        else:
            iids = df3[iidcol].unique()  # Get unique IIDs from df3
            iids = normalize_iid_series(pd.Series(iids))
            #iids = pd.to_numeric(pd.Series(iids), errors="coerce").dropna().astype(int).tolist()

        if os.path.exists(lpr_file.rpartition('.')[0] + ".h5"):
            h5_exist = True
            h5_file = lpr_file.rpartition('.')[0] + ".h5"
            logger.info("[main] Identified an available h5 indexed input for the lpr -f file.")
            if not verbose:
                print("[main] Identified an indexed h5 input for -f file")
        pheno_requests = []
        pheno_requests_normalized = []
        pheno_requests_normalized.extend(set(map(str,normalized_pheno)))
        pheno_requests.extend(set(map(str, in_pheno_codes.iloc[0]["Disorder Codes"])))
        if not lifetime_exclusions.empty:
            pheno_requests.extend(set(map(str, lifetime_exclusions.iloc[0]["Disorder Codes"])))
            pheno_requests_normalized.extend(set(map(str, normalized_lifetime)))
        if not oneYearPrior_exclusions.empty:
            pheno_requests.extend(set(map(str, oneYearPrior_exclusions.iloc[0]["Disorder Codes"])))
            pheno_requests_normalized.extend(set(map(str, normalized_prior)))
        if not post_exclusions.empty:
            pheno_requests.extend(set(map(str, post_exclusions.iloc[0]["Disorder Codes"])))
            pheno_requests_normalized.extend(set(map(str, normalized_post)))
        if not covariates.empty:
            pheno_requests.extend(set(map(str, covariates.iloc[0]["Disorder Codes"])))
            pheno_requests_normalized.extend(set(map(str, normalized_covars)))
        if lifetime_exclusions.empty and oneYearPrior_exclusions.empty and post_exclusions.empty and covariates.empty:
            logger.info("[main] All posssible exclusions/covariates are empty (lifetime_exclusions,oneYearPrior_exclusions,post_exclusions,covariates)")
        logger.info(f"[main] Identified that {ATC_Requested} of the codes are ATC codes.")
        #flattened_pheno_requests = list(sorted(set(pheno_requests))) #Keep only unique entries and sort them
        #new 01.10.2025
        flattened_pheno_requests = list(sorted(set(pheno_requests_normalized))) #Keep only unique entries and sort them
        if h5_exist and not BuildEntryExitDates:
            logger.info(f"[main] ICD codes to call using the h5 indexed file: {flattened_pheno_requests}, based on: {pheno_requests}")
            del(pheno_requests)
            del(pheno_requests_normalized)
            if not lowMem:
                df1 = pd.DataFrame()
                logger.debug(f"[main] Before loading h5 with df1.head(5):{df1.head(5)}, df3.head(5):{df3.head(5)}; iidcol:{iidcol}")
                if selectIIDs != "":
                    IIDsToUse = iids
                else: 
                    IIDsToUse = []
                df1 = h5_load_df1(h5_file=h5_file, iids=IIDsToUse, iidcol=iidcol, flattened_pheno_requests=flattened_pheno_requests, exact_match=exact_match, BuildEntryExitDates=BuildEntryExitDates, 
                                  diagnostic_col=diagnostic_col, birthdatecol=birthdatecol, ctype_col=ctype_col)
                logger.info(f"[main] At the end of loading h5 with df1.head(5): {df1.head(5)}")
                gc.collect()
                logger.info(f"[main] Starting process_pheno_and_exclusions with df3: {df3.head(5)}")
                if not verbose:
                    print(f"[main] Starting process_pheno_and_exclusions with df3: {df3.head(5)}")
                process_pheno_and_exclusions(MatchFI=MatchFI, df1=df1, df3=df3, df4=df4, iidcol=iidcol, verbose=verbose, ctype_excl=ctype_excl, ctype_incl=ctype_incl, ctype_col=ctype_col, Filter_YoB=Filter_YoB, 
                                        Filter_Gender=Filter_Gender, use_predefined_exdep_exclusions=use_predefined_exdep_exclusions,
                                        cluster_run=cluster_run, exact_match=exact_match, skip_icd_update=skip_icd_update, 
                                        remove_point_in_diag_request=remove_point_in_diag_request, ICDCM=ICDCM, qced_iids=qced_iids, general_exclusions=general_exclusions, 
                                        multi_inclusions=multi_inclusions, in_pheno_codes=in_pheno_codes, pheno_requestcol=pheno_requestcol, diagnostic_col=diagnostic_col, 
                                        atc_diag_col=atc_diag_col, birthdatecol=birthdatecol, atc_date_col=atc_date_col, atc_cols_to_read_as_date=atc_cols_to_read_as_date, 
                                        atc_file=atc_file, fsep=fsep, BuildEntryExitDates=BuildEntryExitDates, lifetime_exclusions_file=lifetime_exclusions_file, 
                                        post_exclusions_file=post_exclusions_file, oneYearPrior_exclusions_file=oneYearPrior_exclusions_file, outfile=outfile, 
                                        write_Plink2_format=write_Plink2_format, write_fastGWA_format=write_fastGWA_format, write_pickle=write_pickle, n_stam_iids=n_stam_iids, 
                                        exclCHBcontrols=exclCHBcontrols, iidstatus_col=iidstatus_col, iidstatusdate=iidstatusdate, addition_information_file=addition_information_file, sexcol=sexcol, 
                                        input_date_in_name=input_date_in_name, input_date_out_name=input_date_out_name, append=False, icdprefix=icdprefix, noLeadingICD=noLeadingICD, 
                                        lifetime_exclusions=lifetime_exclusions, oneYearPrior_exclusions=oneYearPrior_exclusions, post_exclusions=post_exclusions, covariates=covariates)
            else:
                batch_size = int(batchsize)
                num_batches = int(np.ceil(len(iids) / batch_size))  # Calculate the number of batches
                first_write = True  # Control whether to overwrite or append to the file
                df3backup = df3.copy()
                for batch_num in range(num_batches):
                    df1 = pd.DataFrame()
                    # Get the current batch of IIDs
                    start_idx = batch_num * batch_size
                    end_idx = start_idx + batch_size
                    iid_batch = iids[start_idx:end_idx]
                    df3 = df3backup[df3backup[iidcol].isin(iid_batch)]
                    n_stam_iids=len(df3[iidcol])
                    logger.info(f"[main] Processing batch {batch_num + 1}/{num_batches} with {len(iid_batch)}|{n_stam_iids} IIDs... {iid_batch[:5]}")
                    iids = df3[iidcol].unique()  # Get unique IIDs from df3
                    iids = pd.to_numeric(pd.Series(iids), errors="coerce").dropna().astype(int).tolist()
                    logger.info(f"[main] Before loading current h5 batch {batch_num + 1} with df1.head(5):{df1.head(5)}; df3backup:{df3backup}; df3backup[df3backup[iidcol].isin(iid_batch)]:{df3backup[df3backup[iidcol].isin(iid_batch)].head(5)}; iid_batch: {iid_batch[:5]}; iidcol:{iidcol}")
                    if not verbose:
                        print(f"[main] Before loading current h5 batch {batch_num + 1} with df1.head(5):{df1.head(5)}; df3backup:{df3backup}; df3backup[df3backup[iidcol].isin(iid_batch)]:{df3backup[df3backup[iidcol].isin(iid_batch)].head(5)}; iid_batch: {iid_batch[:5]}; iidcol:{iidcol}")
                    df1 = h5_load_df1(h5_file=h5_file, iids=iids, iidcol=iidcol, flattened_pheno_requests=flattened_pheno_requests, exact_match=exact_match, BuildEntryExitDates=BuildEntryExitDates, 
                                  diagnostic_col=diagnostic_col, birthdatecol=birthdatecol, ctype_col=ctype_col)
                    logger.info(f"[main] At the end of loading current h5 batch {batch_num + 1} with df1.head(5):{df1.head(5)}")
                    gc.collect()
                    if len(df1) > 0:
                        logger.info(f"[main] Starting process_pheno_and_exclusions with df3:{df3.head(5)}; df3backup:{df3backup.head(5)}")
                        if not verbose:
                            print(f"[main] Starting process_pheno_and_exclusions with df3:{df3.head(5)}; df3backup:{df3backup.head(5)}")
                        process_pheno_and_exclusions(MatchFI=MatchFI, df1=df1, df3=df3, df4=df4, iidcol=iidcol, verbose=verbose, ctype_excl=ctype_excl, ctype_incl=ctype_incl, ctype_col=ctype_col, Filter_YoB=Filter_YoB, 
                                        Filter_Gender=Filter_Gender, use_predefined_exdep_exclusions=use_predefined_exdep_exclusions,
                                        cluster_run=cluster_run, exact_match=exact_match, skip_icd_update=skip_icd_update, 
                                        remove_point_in_diag_request=remove_point_in_diag_request, ICDCM=ICDCM, qced_iids=qced_iids, general_exclusions=general_exclusions, 
                                        multi_inclusions=multi_inclusions, in_pheno_codes=in_pheno_codes, pheno_requestcol=pheno_requestcol, diagnostic_col=diagnostic_col, 
                                        atc_diag_col=atc_diag_col, birthdatecol=birthdatecol, atc_date_col=atc_date_col, atc_cols_to_read_as_date=atc_cols_to_read_as_date, 
                                        atc_file=atc_file, fsep=fsep, BuildEntryExitDates=BuildEntryExitDates, lifetime_exclusions_file=lifetime_exclusions_file, 
                                        post_exclusions_file=post_exclusions_file, oneYearPrior_exclusions_file=oneYearPrior_exclusions_file, outfile=outfile, 
                                        write_Plink2_format=write_Plink2_format, write_fastGWA_format=write_fastGWA_format, write_pickle=write_pickle, n_stam_iids=n_stam_iids, 
                                        exclCHBcontrols=exclCHBcontrols, iidstatus_col=iidstatus_col, iidstatusdate=iidstatusdate, addition_information_file=addition_information_file, sexcol=sexcol, 
                                        input_date_in_name=input_date_in_name, input_date_out_name=input_date_out_name, append=first_write, icdprefix=icdprefix, noLeadingICD=noLeadingICD,
                                        lifetime_exclusions=lifetime_exclusions, oneYearPrior_exclusions=oneYearPrior_exclusions, post_exclusions=post_exclusions, covariates=covariates)
                    if first_write == True and len(df1) > 0:
                        first_write = False
                    elif len(df1) == 0:
                        #TODO IIDs without being case should still be included
                        logger.info(f"[main] Skipping batch {batch_num + 1} as there are no overlapping cases.")
                        if not verbose:
                            print(f"[main] Skipping batch {batch_num + 1} as there are no overlapping cases.")
        elif not lowMem:
            df1 = process_lpr_data(
                lpr_file, lpr2nd_file, dta_input, fsep, lpr_cols_to_read_as_date, 
                DateFormat, potential_lpr_cols_to_read_as_date, diagnostic_col, diagnostic2nd_col, 
                lpr_recnummer, lpr2nd_recnummer
            )
            
            df1 = finalize_lpr_data(df1=df1, diagnostic_col=diagnostic_col, birthdatecol=birthdatecol, ctype_col=ctype_col, verbose=verbose)
            
            diagnostic_col = "diagnosis"
            birthdatecol = "birthdate"
            if(exact_match):
                logger.info("[main] Updating the diagnostic codes to be all Uppercase to be able to run --eM")
                df1[diagnostic_col] = df1[diagnostic_col].apply(lambda x: x.upper() if isinstance(x, str) else x)

            if verbose:
                logger.debug(df1.columns)
                logger.debug("[main] Mem after loading lpr_file input:")
                usage()
            else:
                logger.info("[main] Finished loading -f file(s)")
                print("[main] Finished loading -f file(s)")

            # Check if input is plausible (e.g. contains every IID only once)
            if (len(df1) > len(df1[iidcol].unique())):
                logger.info("[main] WARNING: Your input file -f contains duplicated IIDs. This basically means, that each IID is given multiple times, potentially due to multiple diagnostic codes (then it is normal). If this should not be the case, please check your input!")
            
            if BuildEntryExitDates:
                df3 = BuildEntryExitDate(df1, df3, iidcol, input_date_in_name, input_date_out_name, verbose)
            gc.collect()
            logger.info(f"[main] Pheno codes requested: {in_pheno_codes}")
            logger.info(f"[main] Starting process_pheno_and_exclusions with df3:{df3.head(5)}")
            if not verbose:
                print(f"[main] Starting process_pheno_and_exclusions with df3:{df3.head(5)}")
            process_pheno_and_exclusions(MatchFI=MatchFI, df1=df1, df3=df3, df4=df4, iidcol=iidcol, verbose=verbose, ctype_excl=ctype_excl, ctype_incl=ctype_incl, ctype_col=ctype_col, Filter_YoB=Filter_YoB, 
                                         Filter_Gender=Filter_Gender, use_predefined_exdep_exclusions=use_predefined_exdep_exclusions,
                                         cluster_run=cluster_run, exact_match=exact_match, skip_icd_update=skip_icd_update, 
                                         remove_point_in_diag_request=remove_point_in_diag_request, ICDCM=ICDCM, qced_iids=qced_iids, general_exclusions=general_exclusions, 
                                         multi_inclusions=multi_inclusions, in_pheno_codes=in_pheno_codes, pheno_requestcol=pheno_requestcol, diagnostic_col=diagnostic_col, 
                                         atc_diag_col=atc_diag_col, birthdatecol=birthdatecol, atc_date_col=atc_date_col, atc_cols_to_read_as_date=atc_cols_to_read_as_date, 
                                         atc_file=atc_file, fsep=fsep, BuildEntryExitDates=BuildEntryExitDates, lifetime_exclusions_file=lifetime_exclusions_file, 
                                         post_exclusions_file=post_exclusions_file, oneYearPrior_exclusions_file=oneYearPrior_exclusions_file, outfile=outfile, 
                                         write_Plink2_format=write_Plink2_format, write_fastGWA_format=write_fastGWA_format, write_pickle=write_pickle, n_stam_iids=n_stam_iids, 
                                         exclCHBcontrols=exclCHBcontrols, iidstatus_col=iidstatus_col, iidstatusdate=iidstatusdate, addition_information_file=addition_information_file, sexcol=sexcol, 
                                         input_date_in_name=input_date_in_name, input_date_out_name=input_date_out_name, append=False, icdprefix=icdprefix, noLeadingICD=noLeadingICD,
                                        lifetime_exclusions=lifetime_exclusions, oneYearPrior_exclusions=oneYearPrior_exclusions, post_exclusions=post_exclusions, covariates=covariates)

        else:
            #TODO if lowMem.... 
            ## Here, it should loop over all individuals given in IIDs (below) by bins of  100.000 individuals at once. 
            ## This basically leads to the problem, that df1 needs to be loaded in parts, so only the current 100.000 iids. 
            ## These 100.000 iids will then need to be processed as if there were all iids loaded at once. This leads to the issue, that the method is right now assuming to have all individuals loaded at once, meaning it also filters for thos that are in df3 (stam file). 
            ## But we dont want to have to reload the stam file with every itteration as this file is not big and won't take up much memory.
            logger.info("[main] Running in low memory mode. Processing df1 in batches.")
            if not verbose:
                print("[main] Running in low memory mode. Processing df1 in batches.")
            iids = df3[iidcol].unique()  # Get unique IIDs from df3
            batch_size = int(batchsize)
            num_batches = int(np.ceil(len(iids) / batch_size))  # Calculate the number of batches
            first_write = True  # Control whether to overwrite or append to the file
            df3backup = df3.copy()
            # df1 = pd.DataFrame() # commented out 02.05.2025
            for batch_num in range(num_batches):
                df1 = pd.DataFrame()
                # Get the current batch of IIDs
                start_idx = batch_num * batch_size
                end_idx = start_idx + batch_size
                iid_batch = iids[start_idx:end_idx]
                df3 = df3backup[df3backup[iidcol].isin(iid_batch)]
                n_stam_iids=len(df3[iidcol])
                logger.info(f"[main] Processing batch {batch_num + 1}/{num_batches} with {len(iid_batch)} IIDs... {iid_batch[:5]}")
                # recnums_to_keep = []
                # Updated to filter on diagnostic col and requested phenotypes
                # df1 =  batch_load_lprfile(df = df1, lprfile = lpr_file, lpr_recnummer = lpr_recnummer, lpr2nd_file = lpr2nd_file, lpr2nd_recnummer = lpr2nd_recnummer,
                #             iidcol = iidcol, iid_batch = iid_batch, batch_num = batch_num, fsep = fsep,
                #             potential_lpr_cols_to_read_as_date = potential_lpr_cols_to_read_as_date, lpr_cols_to_read_as_date = lpr_cols_to_read_as_date,
                #             verbose = verbose, dta_input = dta_input, DateFormat = DateFormat, diagnostic_col = diagnostic_col, diagnostic2nd_col = diagnostic2nd_col)
                df1 =  batch_load_lprfile(df = df1, lprfile = lpr_file, lpr_recnummer = lpr_recnummer, lpr2nd_file = lpr2nd_file, lpr2nd_recnummer = lpr2nd_recnummer,
                            iidcol = iidcol, iid_batch = iid_batch, batch_num = batch_num, fsep = fsep,
                            potential_lpr_cols_to_read_as_date = potential_lpr_cols_to_read_as_date, lpr_cols_to_read_as_date = lpr_cols_to_read_as_date,
                            verbose = verbose, dta_input = dta_input, DateFormat = DateFormat, diagnostic_col = diagnostic_col, diagnostic2nd_col = diagnostic2nd_col,
                            filter_on_diagcol = True, diags = in_pheno_codes)
                gc.collect()
                if(exact_match):
                    logger.info("[main] Updating the diagnostic codes to be all Uppercase to be able to run --eM")
                    # Ensure only strings are processed and apply the transformation in one go
                    df1[diagnostic_col] = df1[diagnostic_col].apply(lambda x: x.upper() if isinstance(x, str) else x)
                    
                logger.info(f"[main] At the end of lowMem (no h5 file available) with df1.head(5):{df1.head(5)}")
                if BuildEntryExitDates:
                    df3 = BuildEntryExitDate(df1, df3, iidcol, input_date_in_name, input_date_out_name, verbose)
                logger.info(f"[main] Starting process_pheno_and_exclusions with df3:{df3.head(5)}")
                if not verbose:
                    print(f"[main] Starting process_pheno_and_exclusions with df3:{df3.head(5)}")
                process_pheno_and_exclusions(MatchFI=MatchFI, df1=df1, df3=df3, df4=df4, iidcol=iidcol, verbose=verbose, ctype_excl=ctype_excl, ctype_incl=ctype_incl, ctype_col=ctype_col, Filter_YoB=Filter_YoB, 
                                         Filter_Gender=Filter_Gender, use_predefined_exdep_exclusions=use_predefined_exdep_exclusions,
                                         cluster_run=cluster_run, exact_match=exact_match, skip_icd_update=skip_icd_update, 
                                         remove_point_in_diag_request=remove_point_in_diag_request, ICDCM=ICDCM, qced_iids=qced_iids, general_exclusions=general_exclusions, 
                                         multi_inclusions=multi_inclusions, in_pheno_codes=in_pheno_codes, pheno_requestcol=pheno_requestcol, diagnostic_col=diagnostic_col, 
                                         atc_diag_col=atc_diag_col, birthdatecol=birthdatecol, atc_date_col=atc_date_col, atc_cols_to_read_as_date=atc_cols_to_read_as_date, 
                                         atc_file=atc_file, fsep=fsep, BuildEntryExitDates=BuildEntryExitDates, lifetime_exclusions_file=lifetime_exclusions_file, 
                                         post_exclusions_file=post_exclusions_file, oneYearPrior_exclusions_file=oneYearPrior_exclusions_file, outfile=outfile, 
                                         write_Plink2_format=write_Plink2_format, write_fastGWA_format=write_fastGWA_format, write_pickle=write_pickle, n_stam_iids=n_stam_iids, 
                                         exclCHBcontrols=exclCHBcontrols, iidstatus_col=iidstatus_col, iidstatusdate=iidstatusdate, addition_information_file=addition_information_file, sexcol=sexcol, 
                                         input_date_in_name=input_date_in_name, input_date_out_name=input_date_out_name, append=first_write, icdprefix=icdprefix, noLeadingICD=noLeadingICD,
                                         lifetime_exclusions=lifetime_exclusions, oneYearPrior_exclusions=oneYearPrior_exclusions, post_exclusions=post_exclusions, covariates=covariates)
                if first_write == True and len(df1) > 0:
                    first_write = False
                elif len(df1) == 0:
                    #TODO IIDs without being case should still be included
                    logger.info(f"[main] Skipping batch {batch_num + 1} as there are no overlapping cases.")
                
    files_to_checksum = []
    if(os.path.exists(outfile)):
        files_to_checksum.append(f"{outfile}")    
    if(os.path.exists(os.path.splitext(os.path.basename(outfile))[0]+"sankey.tsv")):
        files_to_checksum.append(f"{os.path.splitext(os.path.basename(outfile))[0]}.sankey.tsv")
    if(write_fastGWA_format):
        files_to_checksum.append(f"{outfile}.fgwa.pheno")
    if(write_Plink2_format):
        files_to_checksum.append(f"{outfile}.plink2.pheno")    
    if(write_pickle):
        files_to_checksum.append(f"{outfile}.pickle")
        

    logger.info("[main] Finished the Phenotype definition(s). Producing checksums")
    if not verbose:
        print("[main] Finished the Phenotype definition(s). Producing checksums")

    checksum_file = outfile.rpartition('.')[0] + ".checksums.sha256"
    try:
        with open(checksum_file, "w") as f:
            subprocess.check_call(
            ["sha256sum"] + files_to_checksum,
            cwd=os.path.dirname(outfile),
            stdout=f
            )
        logger.info("[main] Checksums produced.")
        if not verbose:
            print("[main] Checksums produced.")
    except subprocess.CalledProcessError as e:
        logger.info(f"[main] ERROR: Encountered error while creating checksums: {e}")
        if not verbose:
            print(f"[main] ERROR: Encountered error while creating checksums: {e}")
    except:
        logger.info(f"[main] ERROR: Encountered error while creating checksums")
        if not verbose:
            print(f"[main] ERROR: Encountered error while creating checksums")

    
    logger.info(f"[main] Generating README: {outfile.rpartition('.')[0] + '.README.md'}")
    if not verbose:
        print(f"[main] Generating README: {outfile.rpartition('.')[0] + '.README.md'}")
    readme_file = outfile.rpartition('.')[0] + ".README.md"
    with open(readme_file, "w") as f:
        f.write(generate_readme(flags_used=argstring, default_args=default_argstring, multiplePhenotypes=multi_inclusions, disclaimer_text=disclaimer_text,
                                additional_cols=extra_cols_to_keep, selected_pickle=write_pickle, 
                                selected_PLINK=write_Plink2_format, selected_FastGWA=write_fastGWA_format, 
                                Exclusions=(use_predefined_exdep_exclusions or lifetime_exclusions_file != "" or post_exclusions_file != "" or oneYearPrior_exclusions_file != ""),
                                phenotypes=in_pheno_codes, ExDep=use_predefined_exdep_exclusions))
        f.write("\n")
    logger.info(disclaimer_text)
    logger.info(f"[main] Your Arguments used to start this program:\n{argstring}")
    if not verbose:
        print(disclaimer_text)
        print(f"[main] Your Arguments used to start this program:\n{argstring}")

# --- Indexing function with object dtype for strings to satisfy PyTables ---
def index_diag_file(
    input_csv: str,
    index_columns: List[str],
    chunksize: int = 2_000_000,
    complib: str = "zlib",
    complevel: int = 5,
    dtypes: Optional[Dict[str, str]] = None,
    table_name: str = "df",
    separator: str = ","
) -> None:
    """
    Build one .h5 per CSV, storing all columns with a consistent schema,
    defaulting string-like columns to object dtype (PyTables-friendly),
    and indexing only `index_columns`.

    Parameters
    ----------
    input_csv : str
        Path(s) to source CSV file(s), comma-separated if multiple.
    index_columns : List[str]
        Columns to index (e.g. ["c_iid","c_diagtype"]).
    chunksize : int
        Rows per chunk when streaming each CSV.
    complib : str
        Compression library for the HDF5 store.
    complevel : int
        Compression level (09).
    dtypes : Optional[Dict[str,str]]
        Override default dtypes. For string columns, use "object".
        Example: {"pnr": "Int64", "d_inddto": "datetime64[ns]", "source": "object"}
    table_name : str
        Key under which the table is stored in each HDF5 file.
    separator : str
        Field separator for CSV parsing.
    """
    paths = [p.strip() for p in input_csv.split(",")] if "," in input_csv else [input_csv]

    for path in paths:
        logger.info(f"[index_diag_file] Indexing file: {path}")
        h5_store = path.rpartition('.')[0] + '.h5'

        # 1) Read first chunk to infer schema
        reader = pd.read_csv(
            path,
            engine="python",
            compression="infer",
            sep=separator,
            chunksize=chunksize
        )
        try:
            first = next(reader)
        except StopIteration:
            print(f"   {path} is empty, skipping")
            continue

        cols = first.columns.tolist()

        # Default: object dtype for everything
        dtype_map: Dict[str, object] = {col: object for col in cols}

        # Override with user-specified dtypes
        if dtypes:
            for col, dt in dtypes.items():
                if col in dtype_map:
                    if dt.lower() in ("string", "object"):
                        dtype_map[col] = object
                    else:
                        dtype_map[col] = dt

        # Cast first chunk consistently
        first = first.astype(dtype_map, copy=False)

        # Force object columns explicitly to str
        for c, dt in dtype_map.items():
            if dt is object:
                first[c] = first[c].astype(str)

        # Fixed min_itemsize for object columns (safe for later chunks)
        min_itemsize = {c: 256 for c, dt in dtype_map.items() if dt is object}

        # 2) Create HDF5 and write
        with pd.HDFStore(h5_store, mode="w", complib=complib, complevel=complevel) as store:
            store.append(
                table_name,
                first,
                format="table",
                data_columns=index_columns,
                min_itemsize=min_itemsize
            )
            # Append remaining chunks
            for chunk in reader:
                chunk = chunk[cols].astype(dtype_map, copy=False)
                for c, dt in dtype_map.items():
                    if dt is object:
                        chunk[c] = chunk[c].astype(str)

                store.append(
                    table_name,
                    chunk,
                    format="table",
                    data_columns=index_columns,
                    min_itemsize=min_itemsize
                )

        print(f"   wrote indexed HDF5: {h5_store}")
    print("All files indexed.")

# --- Retrieval function with optimized filtering order ---

def select_by_iid_and_diag_old(
    h5_path,
    table_name,
    iidcol,
    iids,
    diagcol,
    diags,
    *,
    columns=None,
    chunksize=200_000,
    prefix_all=False,            # treat all diags as prefixes if True
):
    def _norm_list(x):
        if x is None:
            return None
        if isinstance(x, (pd.Series, pd.Index)):
            vals = x.tolist()
        elif isinstance(x, np.ndarray):
            vals = x.tolist()
        elif not hasattr(x, "__iter__") or isinstance(x, (str, bytes)):
            vals = [x]
        else:
            vals = list(x)
        # drop Nones/NaNs and trim
        out = []
        for v in vals:
            if v is None:
                continue
            s = str(v)
            if s == "nan":
                continue
            out.append(v)
        return out if out else None

    iids_list  = _norm_list(iids)
    diags_list = _norm_list(diags)

    exact_set, prefixes = set(), []
    if diags_list is not None:
        for d in map(str, diags_list):
            d = d.strip()
            if prefix_all or d.endswith("*"):
                prefixes.append(d[:-1] if d.endswith("*") else d)
            else:
                exact_set.add(d)
    logger.info(f"[select_by_iid_and_diag] diags using prefix search: {prefixes}; diags using exact search: {exact_set}")
    need = {iidcol, diagcol}
    cols = None if columns is None else list(dict.fromkeys([*columns, *need]))

    out = []
    with pd.HDFStore(h5_path, "r") as store:
        # Determine iid dtype from first chunk
        first_chunk = None
        for ch in store.select(table_name, columns=[iidcol], chunksize=1):
            first_chunk = ch
            break
        if first_chunk is None:
            return pd.DataFrame(columns=columns or [])

        iid_dtype = first_chunk[iidcol].dtype

        # Coerce iids to the H5 dtype
        if iids_list is not None:
            if pd.api.types.is_integer_dtype(iid_dtype) or pd.api.types.is_float_dtype(iid_dtype):
                # numeric column: coerce iids to numeric
                iids_norm = pd.to_numeric(pd.Series(iids_list), errors="coerce").dropna().astype(iid_dtype).tolist()
            else:
                # string/object column: coerce to stripped strings
                iids_norm = pd.Series(iids_list).astype("string").str.strip().tolist()
            iids_set = set(iids_norm)
        else:
            iids_set = None

        # Stream chunks
        for chunk in store.select(table_name, columns=cols, chunksize=chunksize):
            ## normalize the iid column early
            chunk[iidcol] = normalize_iid_series(chunk[iidcol], target="int")
            #IID mask
            if iids_set is None:
                iid_mask = pd.Series(True, index=chunk.index)
            else:
                # cast chunk iid to same dtype used above
                if pd.api.types.is_integer_dtype(iid_dtype) or pd.api.types.is_float_dtype(iid_dtype):
                    iid_series = pd.to_numeric(chunk[iidcol], errors="coerce").astype(iid_dtype)
                else:
                    iid_series = chunk[iidcol].astype("string").str.strip()
                iid_mask = iid_series.isin(iids_set)

            # DIAG mask
            s = chunk[diagcol].astype("string").str.strip()

            if diags_list is None:
                diag_mask = pd.Series(True, index=chunk.index)
            else:
                dmask = pd.Series(False, index=chunk.index)
                if exact_set:
                    dmask |= s.isin(exact_set)
                if prefixes:
                    dmask |= s.str.startswith(tuple(prefixes), na=False)
                diag_mask = dmask
            
            mask = iid_mask & diag_mask
            if mask.any():
                out.append(chunk.loc[mask, cols if cols is not None else chunk.columns])

        if out:
            return pd.concat(out, ignore_index=True)

        # empty but with real columns from store
        st = store.get_storer(table_name)
        fallback_cols = columns if columns is not None else [c for c in st.table.colnames if c != "index"]
        return pd.DataFrame(columns=fallback_cols)
    
def select_by_iid_and_diag_optimized(
    h5_path,
    table_name,
    iidcol,
    iids,
    diagcol,
    diags,
    *,
    columns=None,
    chunksize=200_000,
    prefix_all=False,
):
    """
    Reworked optimized reader that preserves the exact dtype / normalization
    behaviour of select_by_iid_and_diag_old while keeping the early
    diag-based chunk skipping optimisation.

    Behavioural invariants kept from the old implementation:
    - supplied iids are coerced to the store's iid dtype (numeric vs string)
    - chunk[iidcol] is normalized with normalize_iid_series(..., target='int')
      (i.e. exactly like the old function) before membership checks
    - diags==[] or iids==[] are treated as "no filter"
    - need columns (iidcol, diagcol) are ensured in requested columns
    """
    def _norm_list(x):
        if x is None:
            return None
        if isinstance(x, (pd.Series, pd.Index)):
            vals = x.tolist()
        elif isinstance(x, np.ndarray):
            vals = x.tolist()
        elif not hasattr(x, "__iter__") or isinstance(x, (str, bytes)):
            vals = [x]
        else:
            vals = list(x)
        # drop Nones/NaNs and trim
        out = []
        for v in vals:
            if v is None:
                continue
            s = str(v)
            if s == "nan":
                continue
            out.append(v)
        return out if out else None

    iids_list = _norm_list(iids)
    diags_list = _norm_list(diags)

    # treat empty lists as None (no filter)
    if iids_list == []:
        iids_list = None
    if diags_list == []:
        diags_list = None

    # Build diag exact/prefix sets (allow diags_list == None -> no diag filtering)
    exact_set, prefixes = set(), []
    if diags_list is not None:
        for d in map(str, diags_list):
            d = d.strip()
            if prefix_all or d.endswith("*"):
                prefixes.append(d[:-1] if d.endswith("*") else d)
            else:
                exact_set.add(d)

    # Ensure required columns are always requested from the store
    need = {iidcol, diagcol}
    if columns is None:
        cols = None
    else:
        cols = list(dict.fromkeys([*columns, *need]))

    out_chunks = []

    with pd.HDFStore(h5_path, "r") as store:
        # Determine iid dtype from first chunk/sample
        first_chunk = None
        try:
            for ch in store.select(table_name, columns=[iidcol], chunksize=1):
                first_chunk = ch
                break
        except Exception:
            first_chunk = None

        if first_chunk is None:
            return pd.DataFrame(columns=columns or [])

        iid_dtype = first_chunk[iidcol].dtype

        # capture sample dtypes from the stored table (use to coerce each chunk)
        sample_dtypes = first_chunk.dtypes.to_dict()
        logger.info(f"[select_by_iid_and_diag_optimized] Detected iid dtype: {iid_dtype}; sample dtypes: {sample_dtypes}")

        # Coerce supplied iids to same dtype as stored column (mirror old behavior)
        if iids_list is not None:
            if pd.api.types.is_integer_dtype(iid_dtype) or pd.api.types.is_float_dtype(iid_dtype):
                iids_norm = pd.to_numeric(pd.Series(iids_list), errors="coerce").dropna().astype(iid_dtype).tolist()
            else:
                iids_norm = pd.Series(iids_list).astype("string").str.strip().tolist()
            iids_set = set(iids_norm)
        else:
            iids_set = None

        # Stream chunks
        for chunk in store.select(table_name, columns=cols, chunksize=chunksize):
            # Attempt to apply the same dtypes observed in the sample chunk to each read chunk.
            # This makes chunk column dtypes consistent with the indexed store and avoids
            # # spurious NaN/NaT created by later ad-hoc coercions.
            try:
                cast_map = {c: sample_dtypes[c] for c in chunk.columns if c in sample_dtypes}
                if cast_map:
                    # use copy=False to avoid unnecessary copies; wrap in try as some casts may fail
                    chunk = chunk.astype(cast_map, copy=False)
            except Exception as e:
                logger.debug(f"[select_by_iid_and_diag_optimized] dtype cast failed for chunk: {e}")
                
             # --- DIAG filtering first (skip early if no diag match) ---
            if diags_list is None:
                diag_mask = pd.Series(True, index=chunk.index)
            else:
                s = chunk[diagcol].astype("string").str.strip()
                dmask = pd.Series(False, index=chunk.index)
                if exact_set:
                    dmask |= s.isin(exact_set)
                if prefixes:
                    dmask |= s.str.startswith(tuple(prefixes), na=False)
                diag_mask = dmask

            if not diag_mask.any():
                continue

            # restrict to diag-matching rows only (reduces work for IID filtering)
            sub = chunk.loc[diag_mask].copy()

            # --- IID filter: normalize exactly as old implementation did (mutate sub column)
            if iidcol in sub.columns:
                try:
                    # same call as in the old function (keeps same fallback behaviour)
                    sub[iidcol] = normalize_iid_series(sub[iidcol], target="int")
                except Exception:
                    # fallback: coerce to string and strip
                    sub[iidcol] = sub[iidcol].astype("string").str.strip()

            if iids_set is None:
                iid_mask = pd.Series(True, index=sub.index)
            else:
                # cast chunk iid to same dtype used above for iids_set
                if pd.api.types.is_integer_dtype(iid_dtype) or pd.api.types.is_float_dtype(iid_dtype):
                    iid_series = pd.to_numeric(sub[iidcol], errors="coerce").astype(iid_dtype)
                else:
                    iid_series = sub[iidcol].astype("string").str.strip()
                iid_mask = iid_series.isin(iids_set)

            if iid_mask.any():
                out_chunks.append(sub.loc[iid_mask, cols if cols is not None else sub.columns])

    if out_chunks:
        return pd.concat(out_chunks, ignore_index=True)

    # fallback: empty but with correct columns
    with pd.HDFStore(h5_path, "r") as store:
        st = store.get_storer(table_name)
        fallback_cols = columns if columns is not None else [c for c in st.table.colnames if c != "index"]
        return pd.DataFrame(columns=fallback_cols)

def select_by_iid_and_diag_optimized_working(
    h5_path,
    table_name,
    iidcol,
    iids,
    diagcol,
    diags,
    *,
    columns=None,
    chunksize=200_000,
    prefix_all=False,
):
    """
    Optimized retrieval that mirrors the behavior of select_by_iid_and_diag_old
    but keeps the chunk-by-chunk streaming / early diag-filtering approach.

    Key differences from the earlier broken version:
     - Coerces supplied iids to the same dtype as the stored iid column (numeric vs string)
       (matches select_by_iid_and_diag_old).
     - Normalizes the chunk iid column early (normalize_iid_series(..., target="int"))
       so membership checks behave identically to the old implementation.
     - Accepts diags=None as "no diag filter" (same as old).
    """
    def _norm_list(x):
        if x is None:
            return None
        if isinstance(x, (pd.Series, pd.Index)):
            vals = x.tolist()
        elif isinstance(x, np.ndarray):
            vals = x.tolist()
        elif not hasattr(x, "__iter__") or isinstance(x, (str, bytes)):
            vals = [x]
        else:
            vals = list(x)
        # drop Nones/NaNs and trim (keep same semantics as the old impl)
        out = []
        for v in vals:
            if v is None:
                continue
            s = str(v)
            if s == "nan":
                continue
            out.append(v)
        return out if out else None

    iids_list = _norm_list(iids)
    diags_list = _norm_list(diags)

    # Build diag exact/prefix sets (allow diags_list == None -> no diag filtering)
    exact_set, prefixes = set(), []
    if diags_list is not None:
        for d in map(str, diags_list):
            d = d.strip()
            if prefix_all or d.endswith("*"):
                prefixes.append(d[:-1] if d.endswith("*") else d)
            else:
                exact_set.add(d)

    need = {iidcol, diagcol}
    cols = None if columns is None else list(dict.fromkeys([*columns, *need]))
    out = []
    need = {iidcol, diagcol}

    with pd.HDFStore(h5_path, "r") as store:
        # Determine iid dtype from first chunk (kept for logging/awareness)
        first_chunk = None
        try:
            for ch in store.select(table_name, columns=[iidcol], chunksize=1):
                first_chunk = ch
                break
        except Exception:
            first_chunk = None

        if first_chunk is None:
            # nothing in store
            return pd.DataFrame(columns=columns or [])

        iid_dtype = first_chunk[iidcol].dtype

        # Coerce iids to the H5 dtype (mirror old behaviour)
        if iids_list is not None:
            if pd.api.types.is_integer_dtype(iid_dtype) or pd.api.types.is_float_dtype(iid_dtype):
                iids_norm = pd.to_numeric(pd.Series(iids_list), errors="coerce").dropna().astype(iid_dtype).tolist()
            else:
                iids_norm = pd.Series(iids_list).astype("string").str.strip().tolist()
            iids_set = set(iids_norm)
        else:
            iids_set = None

        # Stream chunks
        for chunk in store.select(table_name, columns=cols, chunksize=chunksize):
            # Normalize the iid column early (same as old)
            if iidcol in chunk.columns:
                try:
                    chunk[iidcol] = normalize_iid_series(chunk[iidcol], target="int")
                except Exception:
                    # fallback: coerce to string and strip
                    chunk[iidcol] = chunk[iidcol].astype("string").str.strip()

            # DIAG mask (allow diags_list == None => accept all)
            if diags_list is None:
                diag_mask = pd.Series(True, index=chunk.index)
            else:
                s = chunk[diagcol].astype("string").str.strip()
                dmask = pd.Series(False, index=chunk.index)
                if exact_set:
                    dmask |= s.isin(exact_set)
                if prefixes:
                    dmask |= s.str.startswith(tuple(prefixes), na=False)
                diag_mask = dmask

            # If no diag matches in this chunk, skip it early
            if not diag_mask.any():
                continue

            # Restrict to diag-matching rows
            chunk = chunk.loc[diag_mask].copy()

            # IID mask
            if iids_set is None:
                iid_mask = pd.Series(True, index=chunk.index)
            else:
                # cast chunk iid to same dtype used above for iids_set
                if pd.api.types.is_integer_dtype(iid_dtype) or pd.api.types.is_float_dtype(iid_dtype):
                    iid_series = pd.to_numeric(chunk[iidcol], errors="coerce").astype(iid_dtype)
                else:
                    iid_series = chunk[iidcol].astype("string").str.strip()
                iid_mask = iid_series.isin(iids_set)

            mask = iid_mask
            if mask.any():
                out.append(chunk.loc[mask, cols if cols is not None else chunk.columns])

    if out:
        return pd.concat(out, ignore_index=True)

    # fallback: empty but with real columns from store
    with pd.HDFStore(h5_path, "r") as store:
        st = store.get_storer(table_name)
        fallback_cols = columns if columns is not None else [c for c in st.table.colnames if c != "index"]
        return pd.DataFrame(columns=fallback_cols)

def select_by_iid_and_diag_optimized__(
    h5_path,
    table_name,
    iidcol,
    iids,
    diagcol,
    diags,
    *,
    columns=None,
    chunksize=200_000,
    prefix_all=False,
):
    def _norm_list(x):
        if x is None:
            return None
        if isinstance(x, (pd.Series, pd.Index)):
            vals = x.tolist()
        elif isinstance(x, np.ndarray):
            vals = x.tolist()
        elif not hasattr(x, "__iter__") or isinstance(x, (str, bytes)):
            vals = [x]
        else:
            vals = list(x)
        return [v for v in vals if str(v) != "nan" and v is not None]

    iids_list = _norm_list(iids)
    diags_list = _norm_list(diags)

    if diags_list is None:
        raise ValueError("[select_by_iid_and_diag_optimized] You must provide at least one diagnosis code to filter on")

    exact_set, prefixes = set(), []
    for d in map(str, diags_list):
        d = d.strip()
        if prefix_all or d.endswith("*"):
            prefixes.append(d[:-1] if d.endswith("*") else d)
        else:
            exact_set.add(d)

    need = {iidcol, diagcol}
    cols = None if columns is None else list(dict.fromkeys([*columns, *need]))
    out = []

    with pd.HDFStore(h5_path, "r") as store:
        # Determine iid dtype from first chunk (kept for logging/awareness)
        try:
            first_chunk = store.select(table_name, columns=[iidcol], stop=1)
            iid_dtype = first_chunk[iidcol].dtype
        except Exception:
            iid_dtype = None

        # Normalize supplied IIDs as strings to avoid dtype mismatch with stored data
        if iids_list is not None:
            iids_set = set(str(x).strip() for x in iids_list if x is not None and str(x).strip() != "")
        else:
            iids_set = None

        for chunk in store.select(table_name, columns=cols, chunksize=chunksize):
            # Step 1: Filter by diag
            s = chunk[diagcol].astype("string").str.strip()
            diag_mask = pd.Series(False, index=chunk.index)
            if exact_set:
                diag_mask |= s.isin(exact_set)
            if prefixes:
                diag_mask |= s.str.startswith(tuple(prefixes), na=False)

            if not diag_mask.any():
                continue  # No matching diag, skip

            chunk = chunk.loc[diag_mask].copy()

            # Step 2: Filter by IID if needed  compare as strings to avoid dtype mismatch
            if iids_set is not None:
                # normalize chunk IID column to string for membership check
                try:
                    chunk_iids = chunk[iidcol].astype("string").str.strip()
                except Exception:
                    # fallback: convert via map/str
                    chunk_iids = chunk[iidcol].apply(lambda x: str(x).strip())
                chunk = chunk[chunk_iids.isin(iids_set)]

            if not chunk.empty:
                out.append(chunk)

    if out:
        return pd.concat(out, ignore_index=True)

    # fallback: empty but with correct columns
    with pd.HDFStore(h5_path, "r") as store:
        st = store.get_storer(table_name)
        fallback_cols = columns if columns is not None else [c for c in st.table.colnames if c != "index"]
        return pd.DataFrame(columns=fallback_cols)   

def select_by_iid_and_diag_optimized_(
    h5_path,
    table_name,
    iidcol,
    iids,
    diagcol,
    diags,
    *,
    columns=None,
    chunksize=200_000,
    prefix_all=False,
):
    def _norm_list(x):
        if x is None:
            return None
        if isinstance(x, (pd.Series, pd.Index)):
            vals = x.tolist()
        elif isinstance(x, np.ndarray):
            vals = x.tolist()
        elif not hasattr(x, "__iter__") or isinstance(x, (str, bytes)):
            vals = [x]
        else:
            vals = list(x)
        return [v for v in vals if str(v) != "nan" and v is not None]

    iids_list = _norm_list(iids)
    diags_list = _norm_list(diags)

    if diags_list is None:
        raise ValueError("[select_by_iid_and_diag_optimized] You must provide at least one diagnosis code to filter on")

    exact_set, prefixes = set(), []
    for d in map(str, diags_list):
        d = d.strip()
        if prefix_all or d.endswith("*"):
            prefixes.append(d[:-1] if d.endswith("*") else d)
        else:
            exact_set.add(d)

    need = {iidcol, diagcol}
    cols = None if columns is None else list(dict.fromkeys([*columns, *need]))
    out = []

    with pd.HDFStore(h5_path, "r") as store:
        # Determine iid dtype from first chunk
        first_chunk = store.select(table_name, columns=[iidcol], stop=1)
        iid_dtype = first_chunk[iidcol].dtype

        # Normalize IIDs to appropriate dtype
        if iids_list is not None:
            if pd.api.types.is_numeric_dtype(iid_dtype):
                iids_norm = pd.to_numeric(pd.Series(iids_list), errors="coerce").dropna().astype(iid_dtype).tolist()
            else:
                iids_norm = pd.Series(iids_list).astype("string").str.strip().tolist()
            iids_set = set(iids_norm)
        else:
            iids_set = None

        for chunk in store.select(table_name, columns=cols, chunksize=chunksize):
            # Step 1: Filter by diag
            s = chunk[diagcol].astype("string").str.strip()
            diag_mask = pd.Series(False, index=chunk.index)
            if exact_set:
                diag_mask |= s.isin(exact_set)
            if prefixes:
                diag_mask |= s.str.startswith(tuple(prefixes), na=False)

            if not diag_mask.any():
                continue  # No matching diag, skip

            chunk = chunk.loc[diag_mask].copy()

            # Step 2: Normalize IIDs only for relevant rows
            #chunk[iidcol] = normalize_iid_series(chunk[iidcol], target="int").copy
            chunk.loc[:, iidcol] = normalize_iid_series(chunk[iidcol], target="int")

            # Step 3: Filter by IID if needed
            if iids_set is not None:
                chunk = chunk[chunk[iidcol].isin(iids_set)]

            if not chunk.empty:
                out.append(chunk)

    if out:
        return pd.concat(out, ignore_index=True)

    # fallback: empty but with correct columns
    with pd.HDFStore(h5_path, "r") as store:
        st = store.get_storer(table_name)
        fallback_cols = columns if columns is not None else [c for c in st.table.colnames if c != "index"]
        return pd.DataFrame(columns=fallback_cols)

# --- Retrieval function with conditional empty-list handling and mixed matching ---
def get_h5_cases(
    h5file: str,
    iids: list,
    iidcol: str,
    diags: list,
    diagcol: str,
    directmapping: bool = True,
    table_name: str = "df",
    is_advanced_pheno: bool = False
) -> pd.DataFrame:
    """
    Retrieve rows from an HDF5 table filtered by ID and diagnosis lists,
    with support for empty-list (no filter), mixed exact, and prefix matching.
    Behavior:
    - If iids is empty, include all IDs (no iid filter).
    - If diags is empty, include all diagnoses (no diag filter).
    - If directmapping=False: all diags are treated as prefixes (strip '*').
    - If directmapping=True: diagnostics without '*' are exact matches;
      those with '*' are treated as prefixes.
    Parameters
    ----------
    h5file : str
        Path to the .h5 file containing the indexed table.
    iids : list
        List of values to filter on the iidcol column. Empty list means no filter.
    iidcol : str
        Column name for the first filter (e.g. "iidcol").
    diags : list
        List of values or prefixes (with '*' for wildcard) to filter on the diagcol column.
        Empty list means no filter.
    diagcol : str
        Column name for the second filter (e.g. "diagcol").
    directmapping : bool, default True
        If False, all diags are treated as prefixes. If True,
        diags with '*' are prefixes and others are exact.
    table_name : str, default "df"
        Key of the table in the HDF5 store.
    Returns
    -------
    pd.DataFrame
        DataFrame containing rows matching the combined filters.
    """
    # Helper: build tuple literal for store select
    def make_tuple_literal(vals):
        tup = "(" + ",".join(repr(v) for v in vals)
        if len(vals) == 1:
            tup += ","
        tup += ")"
        return tup
    #TODO: Update it to also be able to process the advanced phenotype coding. 
    logger.info(f"[get_h5_cases] H5 path: {os.path.abspath(h5file)} exists? {os.path.exists(h5file)}")

    # if is_advanced_pheno:
    #     if isinstance(diags,list):
    #         for sublist in diags:
    #             diags_dict = parse_pheno_rules(sublist)
    #             print(f"[get_h5_cases] diags:{diags}; sublist:{sublist}; diags_dict: {diags_dict}")
    #     else:
    #         diags_dict = parse_pheno_rules(sublist)
    #         print(f"[get_h5_cases] diags:{diags}; diags_dict: {diags_dict}")
    print(f"[get_h5_cases] diags before sanitization: {diags}")
    if diags:
        sanitized = []
        for entry in diags:
            if isinstance(entry, list):
                sanitized.extend(entry)       # pull items out of sublists
            else:
                sanitized.append(entry)
        print(f"[get_h5_cases] sanitized before directmapping check: {sanitized}")
        logger.info(f"[get_h5_cases] sanitized: {sanitized}; diags: {diags}")
        # now overwrite
        diags = [str(d) for d in sanitized]  # coerce everything to str
    # Step 1: on-disk selection by iids if provided
    df = select_by_iid_and_diag_optimized(h5_path=h5file, table_name=table_name, iidcol=iidcol, iids=iids, diagcol=diagcol, diags=diags, prefix_all=(not directmapping))
    try:
        logger.info(f"[get_h5_cases] df.head(5): {df.head(5)}; Identified {len(df.loc[1])} overlapping entries with the selected diags.")
    except:
        logger.info(f"[get_h5_cases] df.head(5): {df.head(5)}.")
    return df

def h5_load_df1(h5_file, iids, iidcol, flattened_pheno_requests, exact_match, BuildEntryExitDates, diagnostic_col, birthdatecol, ctype_col):
    logger.info(f"""[h5_load_df1] config for get_h5_cases used: h5_file= {h5_file}
            \tiids={iids[:10]}
            \tiidcol={iidcol}
            \tdiags={flattened_pheno_requests}
            \tdiagcol={diagnostic_col}
            \tdirectmapping={exact_match}""")
    cases = get_h5_cases(
        h5file = h5_file,
        iids = iids,
        iidcol = iidcol,
        diags = flattened_pheno_requests,
        diagcol = diagnostic_col,
        directmapping = exact_match)
    logger.info(f"[h5_load_df1] h5 based cases: {cases.head(5)}")
    df1 = finalize_lpr_data(df1=cases, diagnostic_col=diagnostic_col, birthdatecol=birthdatecol, ctype_col=ctype_col, verbose=verbose)

    diagnostic_col = "diagnosis"
    birthdatecol = "birthdate"
    
    if verbose:
        logger.debug(df1.columns)
        logger.debug("[h5_load_df1] Mem after loading lpr_file input:")
        usage()
    else:
        logger.info("[h5_load_df1] Finished loading -f file(s)")

    # Check if input is plausible (e.g. contains every IID only once)
    if (len(df1) > len(df1[iidcol].unique())):
        logger.info("[h5_load_df1] WARNING: Your input file -f contains duplicated IIDs. This basically means, that each IID is given multiple times, potentially due to multiple diagnostic codes (then it is normal). If this should not be the case, please check your input!")
    if BuildEntryExitDates:
        logger.info("[h5_load_df1] WARNING: BuildEntryExitDates not yet implemented when using h5 indexed files.")
    return(df1)

def generate_test_dataset(odir):
    # Generate File1
    np.random.seed(0)
    num_entries = 1000
    cpr_enc = [generate_cpr_enc() for _ in range(num_entries)]
    sex = np.random.choice(['M', 'F'], num_entries)
    birthdate = [generate_random_date(datetime(1950, 1, 1), datetime(2000, 12, 31)) for _ in range(num_entries)]
    df1 = pd.DataFrame({'cpr_enc': cpr_enc,
                        'sex': sex,
                        'birthdate': birthdate})
    # Generate dbds, degen_old, degen_new
    choices = [True, False]
    df1['dbds'] = np.random.choice(choices, num_entries)
    df1['degen_old'] = np.random.choice(choices, num_entries)
    df1['degen_new'] = np.random.choice(choices, num_entries)
    # Generate File2
    dates_in = [generate_random_date(b, datetime.now()) for b in birthdate]
    dates_out = [generate_random_date(d, datetime.now()) for d in dates_in]
    dates_in2 = [generate_random_date(b, datetime.now()) for b in birthdate]
    dates_out2 = [generate_random_date(d, datetime.now()) for d in dates_in2]
    types = ['A', 'B', '+', 'G', 'H']
    diagnosis = ['ICD10:DR074', 'ICD10:DZ016', 'ICD10:DS011', 'ICD10:DR075', 'ICD10:DR021', 
                'ICD10:DF330', 'ICD10:DF32', 'ICD10:DF320', 'ICD10:DF321', 'ICD10:DF322', 
                'ICD10:DF323', 'ICD10:DF328', 'ICD10:DF329', 'ICD10:DF33', 'ICD10:DF330', 
                'ICD10:DF331', 'ICD10:DF332', 'ICD10:DF333', 'ICD10:DF334', 'ICD10:DF338', 
                'ICD10:DF339', 'ICD10:DF70', 'ICD10:DF700', 'ICD10:DF701', 'ICD10:DF708', 
                'ICD10:DF709', 'ICD10:DF71', 'ICD10:DF710', 'ICD10:DF122', 'ICD10:DF132', 
                'ICD10:DF142', 'ICD10:DF152', 'ICD10:DF162', 'ICD10:DF182', 'ICD10:DF00', 
                'ICD10:DF000', 'ICD10:DF001', 'ICD10:DF002', 'ICD10:DF009']
    source = ['lpr_diag', 'lpr_psyk_diag', 'lpr3_diagnoses']
    type_diagnosis = np.random.choice(diagnosis, num_entries)
    type_diagnosis2 = np.random.choice(diagnosis, num_entries)
    type_values = np.random.choice(types, num_entries)
    source_values = np.random.choice(source, num_entries)
    in_pheno_codes = pd.DataFrame({'cpr_enc': cpr_enc,
                        'source': source_values,
                        'date_in': dates_in,
                        'date_out': dates_out,
                        'type': type_values,
                        'diagnosis': type_diagnosis})
    in_pheno_codes2 = pd.DataFrame({'cpr_enc': cpr_enc,
                                'source': source_values,
                                'date_in': dates_in2,
                                'date_out': dates_out2,
                                'type': type_values,
                                'diagnosis': type_diagnosis2})
    in_pheno_codes = pd.concat([in_pheno_codes, in_pheno_codes2], ignore_index=True)
    # Generate File3
    c_status = [random.choice([0, 10, 20, 30, 50, 70, 80, 90]) for _ in range(num_entries)]
    d_foddato = birthdate
    d_status_hen_start = [generate_random_date(out, datetime.now()) for out in dates_out]
    c_kon = np.random.choice(['K', 'M'], num_entries)
    df3 = pd.DataFrame({'cpr_enc': cpr_enc,
                        'C_STATUS': c_status,
                        'D_STATUS_HEN_START': d_status_hen_start,
                        'D_FODDATO': d_foddato,
                        'C_KON': c_kon})
    # Save to CSV files
    in_pheno_codes.to_csv("./lpr_file.csv", index=False)
    df1.to_csv("./stam_file.csv", index=False)
    df3.to_csv("./addition_information_file.csv", index=False)
    test_mdd_pheno = "diagnosis\nICD10:DF32\nICD10:DF32.0\nICD10:DF32.1\nICD10:DF32.2\nICD10:DF32.3\nICD10:DF32.8\nICD10:DF32.9\nICD10:DF33\nICD10:DF33.0\nICD10:DF33.1\nICD10:DF33.2\nICD10:DF33.3\nICD10:DF33.4\nICD10:DF33.8\nICD10:DF33.9"
    test_mdd_pheno_df = pd.DataFrame({'diagnosis': test_mdd_pheno.split('\n')})
    # Save the DataFrame to a CSV file
    test_mdd_pheno_df.to_csv("./sample_pheno_request.txt", index=False)

def batch_load_lprfile(df, lprfile, lpr_recnummer, lpr2nd_file, lpr2nd_recnummer,
                       iidcol, iid_batch, batch_num, fsep,
                       potential_lpr_cols_to_read_as_date, lpr_cols_to_read_as_date,
                       verbose, dta_input, DateFormat, diagnostic_col, diagnostic2nd_col, filter_on_diagcol=False, diags=""):
    """
    Refactored batch loader that now calls process_lpr_data to load and merge the LPR files.
    
    Parameters:
      df : DataFrame
          The accumulator DataFrame to which the loaded data will be appended.
      lprfile : str
          The primary LPR file (or comma-separated list of files).
      lpr_recnummer : str
          Identifier used in merging diagnosis data.
      lpr2nd_file : str
          The secondary LPR file(s) (or comma-separated list) if provided.
      lpr2nd_recnummer : str
          Identifier for the secondary diagnosis.
      iidcol, iid_batch : (unused)
          Legacy parameters that are no longer needed.
      batch_num : int
          Current batch index (for verbose messaging).
      fsep : str
          The file separator.
      potential_lpr_cols_to_read_as_date : list
          List of columns to try to parse as dates.
      lpr_cols_to_read_as_date : list
          List of columns to explicitly read as dates.
      verbose : bool
          If True, print progress messages.
      dta_input : object
          Input to be passed to load_lpr_file (see process_lpr_data).
      DateFormat : str
          Format for parsing dates.
      diagnostic_col : str
          Primary diagnostic column name.
      diagnostic2nd_col : str
          Secondary diagnostic column name.
    
    Returns:
      df : DataFrame
          The updated DataFrame with loaded LPR data appended.
    """
    if verbose:
        logger.debug(f"[batch_load_lprfile] Loading LPR files in batch loop {batch_num + 1}")
    if filter_on_diagcol:
        df1_rows_to_keep = load_mapping_rows(lprfile, diagnostic_col, diags, fsep)
    else:
        df1_rows_to_keep = load_mapping_rows(lprfile, iidcol, iid_batch, fsep)
    temp_file = str(uuid.uuid4())[:4]+".filtered_temp.csv"
    build_temp_file(lprfile, df1_rows_to_keep, temp_file=temp_file, verbose=verbose)
    lprfile = temp_file
    del df1_rows_to_keep

    #Get the recnums from lprfile to exctract these from the lpr2nd_file
    if lpr2nd_file != "":
        lpr_recnummer_batch = pd.read_csv(lprfile, engine='python', sep = fsep, usecols = [lpr_recnummer])[lpr_recnummer].unique()
        logger.info(lpr_recnummer_batch)
        df1_rows_to_keep = load_mapping_rows(lpr2nd_file, lpr2nd_recnummer, lpr_recnummer_batch, fsep)
        temp_file = str(uuid.uuid4())[:4]+".filtered2nd_temp.csv"
        build_temp_file(lpr2nd_file, df1_rows_to_keep, temp_file=temp_file, verbose=verbose)
        lpr2nd_file = temp_file
        del df1_rows_to_keep
    # Delegate all file loading and merging to process_lpr_data.
    df_new = process_lpr_data(
        lpr_file=lprfile,
        lpr2nd_file=lpr2nd_file,
        dta_input=dta_input,
        fsep=fsep,
        lpr_cols_to_read_as_date=lpr_cols_to_read_as_date,
        DateFormat=DateFormat,
        potential_lpr_cols_to_read_as_date=potential_lpr_cols_to_read_as_date,
        diagnostic_col=diagnostic_col,
        diagnostic2nd_col=diagnostic2nd_col,
        lpr_recnummer=lpr_recnummer,
        lpr2nd_recnummer=lpr2nd_recnummer
    )

    if not verbose:
        os.remove(lprfile)
        os.remove(lpr2nd_file)

    # Append the newly loaded data to the existing DataFrame.
    df = pd.concat([df, df_new], ignore_index=True, sort=False)
    del df_new
    gc.collect()
    if verbose:
        logger.debug(f"[batch_load_lprfile] Finished loading batch {batch_num + 1}. DataFrame shape: {df.shape}")
    return df

def load_mapping_rows(file_path, iidcol, target_iids, fsep="\t", suffix_search=False):
    """
    Load only row indices of lines matching IIDs from a file.

    Parameters:
    - file_path: str, path to the CSV/TSV file
    - iidcol: str, column name containing IIDs
    - target_iids: list of IIDs to retain (will be coerced to string)
    - fsep: field separator (default: tab)
    - suffix_search: if True, uses string suffix matching instead of exact

    Returns:
    - list of int: line numbers to load (1-based, including header as line 0)
    """
    try:
        iids_df = pd.read_csv(file_path, sep=fsep, engine='python', usecols=[iidcol], dtype=str)
    except ValueError as ve:
        logger.error(f"[load_mapping_rows] Column loading failed: {ve}")
        tmp = pd.read_csv(file_path, sep=fsep, nrows=2)
        logger.info(f"[load_mapping_rows] Available columns: {tmp.columns.tolist()}")
        raise

    target_iids = [str(iid) for iid in target_iids]  # Normalize to strings

    if suffix_search:
        match_mask = iids_df[iidcol].astype(str).str.startswith(tuple(target_iids))
    else:
        match_mask = iids_df[iidcol].isin(target_iids)

    # Rows to load: +1 to convert from zero-based index to line numbers (+1 for header)
    matching_rows = [0] + (match_mask[match_mask].index + 1).tolist()

    logger.info(f"[load_mapping_rows] Found {len(matching_rows)-1} matching rows. First few: {matching_rows[:10]}")
    return matching_rows

def load_mapping_rows_old(file_path, iidcol, target_iids, fsep="\t", suffix_search=False):
    """
    Load only rows mapping to a list of IIDs from a file.
    
    Parameters:
    - file_path: str, path to the CSV file
    - iidcol: str, column name containing IIDs
    - target_iids: list, list of IIDs to map
    
    Returns:
    - pd.DataFrame: DataFrame containing only the matching rows
    """
    # Step 1: Load only the `iidcol` column
    try:
        iids_df = pd.read_csv(file_path, engine='python', sep=fsep, usecols=[iidcol])
    except ValueError as ve:
        logger.info(f"Got the following error: {ve}")
        iids_df = pd.read_csv(file_path, engine='python', sep=fsep, nrows=2)
        logger.info(f"Available columns are: {iids_df.columns}")
        exit()

    # Ensure `iidcol` and `target_iids` are of the same type
    iids_df[iidcol] = iids_df[iidcol].astype(str)  # Cast column to string
    target_iids = [str(iid) for iid in target_iids]  # Cast target_iids to string
    
    # Step 2: Find the row indices for the target IIDs
    if suffix_search:
        matching_rows = [0] + (iids_df[iids_df[iidcol].startswith(target_iids)].index + 1).tolist()
    else:
        matching_rows = [0] + (iids_df[iids_df[iidcol].isin(target_iids)].index + 1).tolist()
    logger.info(f"[load_mapping_rows] Identifying rows to load from df1 based on the current set of iids: matching_rows: {matching_rows[:10]}; iids in df1: {iids_df[:10]}; iids to grep: {target_iids[:10]}")    
    return matching_rows

def build_temp_file(file_path, row_indices, temp_file="filtered_temp.csv", index_file="row_indices.txt", verbose=False):
    """
    Filter rows from a large CSV file using sed with a list of row indices.

    Parameters:
    - file_path: str, path to the CSV file
    - row_indices: list, list of row indices to extract (1-based indexing)
    - temp_file: str, path to save the filtered temporary file

    Returns:
    - None (creates the filtered temp file)
    """

    if index_file == "row_indices.txt":
        index_file = str(uuid.uuid4())[:4]+".row_indices.csv"
    # Write the row indices to a temporary file (convert to 1-based indexing)
    with open(index_file, "w") as f:
        f.write("\n".join([str(i + 1) for i in row_indices]))  # +1 for 1-based indexing

    # Use awk to extract rows with matching line numbers
    awk_command = f"awk 'NR==FNR{{lines[$1]; next}} FNR in lines' {index_file} {file_path} > {temp_file}"
    if verbose:
        logger.debug(f"[build_temp_file] Running awk command: {awk_command}")
    subprocess.run(awk_command, shell=True, check=True)

    # Clean up the temporary index file
    if not verbose:
        os.remove(index_file)

def process_pheno_and_exclusions(MatchFI, df3, df1, iidcol, verbose, ctype_excl, ctype_incl, ctype_col, Filter_YoB, Filter_Gender, use_predefined_exdep_exclusions, cluster_run, 
                                 exact_match, skip_icd_update, remove_point_in_diag_request, ICDCM, qced_iids, general_exclusions, multi_inclusions, 
                                 in_pheno_codes, pheno_requestcol, diagnostic_col, atc_diag_col, birthdatecol, atc_date_col, atc_cols_to_read_as_date, atc_file, fsep, 
                                 BuildEntryExitDates, lifetime_exclusions_file, post_exclusions_file, oneYearPrior_exclusions_file, outfile, write_Plink2_format, 
                                 write_fastGWA_format, write_pickle, n_stam_iids, exclCHBcontrols, iidstatus_col, iidstatusdate, addition_information_file, sexcol, input_date_in_name, 
                                 input_date_out_name, append, df4, icdprefix, noLeadingICD, lifetime_exclusions, oneYearPrior_exclusions, post_exclusions, covariates):
    global extra_cols_to_keep
    logger.info(f"[process_pheno_and_exclusions] Starting with df1.head(5):{df1.head(5)}\nAnd df3.head(5): {df3.head(5)}")
    if MatchFI:
        logger.info("[process_pheno_and_exclusions] Restraining df1 and df3 (-f and -i) to only overlapping IIDs. This can be avoided by using --MatchFI")
        # use only those IIDs that are given in df3 and df1. 
        df3_N_before = str(df3[iidcol].nunique())
        df1_N_before = str(df1[iidcol].nunique())
        if df3_N_before > 0:
            df3 = df3[df3[iidcol].isin(df1[iidcol])]
            df1 = df1[df1[iidcol].isin(df3[iidcol])]
            logger.info(f"[process_pheno_and_exclusions] After Restraining df1 {str(df1[iidcol].nunique())}({df1_N_before}) and df3 {str(df3[iidcol].nunique())}({df3_N_before}); now(before)")
        else:
            logger.info(f"[process_pheno_and_exclusions] WARNING: df3 had {df3_N_before} entries. Skipping this step and keeping all df1 entries. Check if you did load the correct df3 (-i) file.")
    else:
        logger.info("[process_pheno_and_exclusions] Restraining df1 to IIDs overlapping with df3 (-f to -i). As df3 (-i) is supposed to supply the information like Birthdate and so on and it would not make much sense to go forward without this information.")
        # use only those IIDs that are given in df3. 
        if df3[iidcol].nunique() > 0:
            df1_N_before = str(df1[iidcol].nunique())
            df1 = df1[df1[iidcol].isin(df3[iidcol])]
            logger.info(f"[process_pheno_and_exclusions] After Restraining df1 {str(df1[iidcol].nunique())}({df1_N_before}) and df3 ({str(df3[iidcol].nunique())}); now(before)")
        else:
            logger.info(f"[process_pheno_and_exclusions] WARNING: df3 had {df3[iidcol].nunique()} entries. Skipping this step and keeping all df1 entries. Check if you did load the correct df3 (-i) file.")
    gc.collect()
    if verbose:
        logger.debug(f"[process_pheno_and_exclusions] df1.head(5):{df1.head(5)}; df3.head(5):{df3.head(5)}")
        logger.debug(f"[process_pheno_and_exclusions] df1.columns:{df1.columns}; df3.columns:{df3.columns}")
    logger.info(f"[process_pheno_and_exclusions] diagnostic_col: {diagnostic_col}")
    if diagnostic_col != "diagnosis":
        if diagnostic_col in df1.columns:
            df1.rename(columns={diagnostic_col: "diagnosis"}, inplace=True)
        if diagnostic_col in df3.columns:
            df3.rename(columns={diagnostic_col: "diagnosis"}, inplace=True)
        if verbose:
            logger.debug(f"[process_pheno_and_exclusions] After renaming \"{diagnostic_col}\" to \"diagnosis\". df1.columns:{df1.columns}; df3.columns:{df3.columns}")
    diagnostic_col = "diagnosis"
    if verbose:
        logger.debug("[process_pheno_and_exclusions] Mem after loading all input:")
        usage()
    if (ctype_excl != ""):
        if (cluster_run in DK_clusters):
            ctype_excllusions = ctype_excl.split(",")
            len_before = len(df1)
            if ("diagtype" in df1.columns):
                df1 = df1.loc[~df1["diagtype"].isin(ctype_excllusions)]
                len_after = len(df1)
                logger.info(f"[process_pheno_and_exclusions] Excluded {len_before-len_after} Diagnoses from the main input file due to ctype_excllusions {ctype_excl}. This does not reflect case/control diagnoses; This is only a general information.")
        else:
            logger.info("[process_pheno_and_exclusions] WARNING: You selected to exclude c_types but you are not running this method on a CHB/DBDS Server!")
    if (ctype_incl != ""):
        if (cluster_run in DK_clusters):
            ctype_inclusions = ctype_incl.split(",")
            len_before = len(df1)
            if ("diagtype" in df1.columns):
                df1 = df1.loc[df1["diagtype"].isin(ctype_inclusions)]
                len_after = len(df1)
                logger.info(f"[process_pheno_and_exclusions] Excluded {len_before-len_after} Diagnoses from the main input file due to ctype_inclusions {ctype_incl}. This does not reflect case/control diagnoses; This is only a general information.")
        else:
            logger.info("[process_pheno_and_exclusions] WARNING: You selected to include only certain c_types but you are not running this method on a CHB/DBDS Server!")
    if (Filter_YoB != ""):
        if (cluster_run in DK_clusters):
            Filter_YoB = Filter_YoB
            if ("birthdate" in df3.columns):
                iids_to_keep = df3.loc[df3['birthdate'] > Filter_YoB,iidcol]
                len_before = len(df1)            
                df1 = df1.loc[df1[iidcol].isin(iids_to_keep)]
                len_after = len(df1)
                logger.info(f"[process_pheno_and_exclusions] Excluded {len_before-len_after} of {len_before} IIDs due to Birthdate before selected date {Filter_YoB}.")
            del iids_to_keep
        else:
            logger.info("WARNING: Selected filter is not implemented yet! [process_pheno_and_exclusions] ")
    if (Filter_Gender != ""):
        if (cluster_run in DK_clusters):
            iids_to_keep = df3.loc[df3['sex'] == Filter_Gender,iidcol]
            len_before = len(df1)            
            df1 = df1.loc[df1[iidcol].isin(iids_to_keep)]
            len_after = len(df1)
            logger.info(f"[process_pheno_and_exclusions] Excluded {len_before-len_after} of {len_before} IIDs due to Gender filter {Filter_Gender}.")
            del iids_to_keep
        else:
            logger.info("[process_pheno_and_exclusions] WARNING: Selected filter is not implemented yet!")
    if (len(df1) == 0):
        logger.info("[process_pheno_and_exclusions] ERROR: No IIDs left after initial Filtering. Consider using different Filters. Exiting")
        if not verbose:
            print("[process_pheno_and_exclusions] ERROR: No IIDs left after initial Filtering. Consider using different Filters. Exiting")
        exit()
    gc.collect()
    if (qced_iids != ""):
        try:
            logger.info(f"[process_pheno_and_exclusions] Filtering now for QC'ed Individuals ({qced_iids})")
            if ("fam" in qced_iids):
                qced_iids_to_keep = pd.read_csv(qced_iids, engine='python', sep = "\t", header = None, dtype=str)
            else:
                qced_iids_to_keep = pd.read_csv(qced_iids, engine='python', sep = None, header = 'infer', dtype=str)
            qced_iids_to_keep.rename(columns={qced_iids_to_keep.columns[0]: iidcol}, inplace=True)
            if verbose:
                logger.debug("[process_pheno_and_exclusions] Assuming the first column contains the IID")
            rows_to_drop = df1[~df1[iidcol].isin(qced_iids_to_keep[iidcol])] 
            if len(rows_to_drop) != 0:
                tmp = rows_to_drop[iidcol].copy()
                tmp.drop_duplicates(inplace=True)
                n_exclusions = len(tmp)
                del tmp
            else:
                n_exclusions = 0
            del rows_to_drop
            df1 = df1[df1[iidcol].isin(qced_iids_to_keep[iidcol])]
            logger.info(f"[process_pheno_and_exclusions] Dropping {str(n_exclusions)} Individual(s) due to QC ({qced_iids})")
        except Exception as e:
            logger.info(f"[process_pheno_and_exclusions] An error occured while loading or processing the QC file. This step will now be skipped.\nHead of the file: {qced_iids_to_keep.head(5)}\nError message: {e}")
            qced_iids = ""
    if (general_exclusions != ""):
        try:
            logger.info(f"[process_pheno_and_exclusions] Filtering now for general exclusion Individuals ({general_exclusions})")
            # Load the IIDs that should be excluded from file as a DataFrame
            iids_to_exclude = pd.read_csv(general_exclusions, engine='python', sep=" ", dtype=str)
            if verbose:
                logger.debug(f"[process_pheno_and_exclusions] iids_to_exclude.head(5): {iids_to_exclude.head(5)}")
            iids_to_exclude.rename(columns={ iids_to_exclude.columns[0]: iidcol}, inplace=True)
            # Identify rows in df1 that have matching rows in iids_to_exclude (where iids_to_exclude values are not NaN)
            rows_to_drop = df1[df1[iidcol].isin(iids_to_exclude[iidcol])]
            if len(rows_to_drop) != 0:
                tmp = rows_to_drop[iidcol].copy()
                tmp.drop_duplicates(inplace=True)
                n_exclusions = len(tmp)
                del tmp
            else:
                n_exclusions = 0
            del rows_to_drop
            df1 = df1[~df1[iidcol].isin(iids_to_exclude[iidcol])]
            logger.info(f"[process_pheno_and_exclusions] Dropping {str(n_exclusions)} Individual(s) due to general exclusion (depending on input file, but could be e.g. due to withdrawal of consent)")
            if verbose:
                logger.debug("[process_pheno_and_exclusions] Mem after building general exclusions and excluding these from the base file:")
                usage()
        except Exception as e:
            logger.info(f"[process_pheno_and_exclusions] An error occured while loading or processing the General exclusion file. This step will now be skipped.\nError message: {e}")
            general_exclusions = ""
    gc.collect()
    if (len(df1) == 0):
        logger.info("[process_pheno_and_exclusions] Error: No IIDs left after general exclusion Filtering. Consider using different Filters. Exiting")
        exit()
    logger.info(f"[process_pheno_and_exclusions] Original in_pheno_codes: {in_pheno_codes}")
    values_to_match = set(map(str, in_pheno_codes.iloc[0]["Disorder Codes"]))
    if (diagnostic_col in df1.columns and diagnostic_col != "diagnosis"):
        df1.rename(columns={diagnostic_col: "diagnosis"},inplace=True)
    if (True):
        logger.info(f"[process_pheno_and_exclusions] Phenotype codes to map: {values_to_match}")
        logger.info(f"[process_pheno_and_exclusions] Original in_pheno_codes: {in_pheno_codes}")
        logger.info("[process_pheno_and_exclusions] Mem after removing iids not passing QC:")
        usage()
    # Use boolean indexing to extract rows from the first file that match the values 
    logger.info("[process_pheno_and_exclusions] ## Build initial Phenotype cases")
    gc.collect()
    logger.info(f"[process_pheno_and_exclusions] ATC_Requested: {ATC_Requested}")
    tmp_cases_df = pd.DataFrame()
    logger.info("[process_pheno_and_exclusions] NB: Using multiple phenotypes at once can't handle (as of now) ATC codes other than within CHB/DBDS.")
    # Set if dbds_run or not
    dbds_run = True if "CHB_DBDS" == cluster_run else False
    # Extra cols when dbds_run=True (only if they exist)
    extra_cols_to_keep = ["admissiontype", "pattype", "diagtype", "source", "register"] if cluster_run in DK_clusters else []
    if (ATC_Requested == "All" or ATC_Requested == "Some") and cluster_run in DK_clusters:
        extra_cols_to_keep.extend(['apk','packsize','vnr'])
    logger.info(f"[process_pheno_and_exclusions] cluster_run: {cluster_run}; extra_cols_to_keep: {extra_cols_to_keep}")
    if not verbose:
        print(f"[process_pheno_and_exclusions] cluster_run: {cluster_run}; extra_cols_to_keep: {extra_cols_to_keep}")

    if (ATC_Requested == "All" or ATC_Requested == "Some"):
        if not cluster_run in DK_clusters:
            print(f"[process_pheno_and_exclusions] WARNING: You are using ATC codes outside of our predefined and tested clusters. The stability is not yet tested.")
        atc_df1 = load_stam_file(stam_file= atc_file, isep=fsep, birthdatecol="", diagnostic_col=atc_diag_col, sexcol = "", stam_cols_to_read_as_date = atc_cols_to_read_as_date)
        logger.info(f"[process_pheno_and_exclusions] after loading atc_df1 using load_stam_file atc_df1.columns: {atc_df1.columns}\n \"birthdate\" in atc_df1.columns: {'birthdate' in atc_df1.columns}")
        if("birthdate" in atc_df1.columns):
            atc_df1.rename(columns={"birthdate":atc_date_col},inplace=True)
            logger.info(f"[process_pheno_and_exclusions] after renaming birthdate column to {atc_date_col} atc_df1.columns: {atc_df1.columns}")
        atc_df1 = finalize_lpr_data(df1=atc_df1, diagnostic_col=atc_diag_col, birthdatecol=atc_date_col, ctype_col=ctype_col, verbose=verbose)
        if("birthdate" in atc_df1.columns):
            atc_df1.rename(columns={"birthdate":atc_date_col},inplace=True)
            logger.info(f"[process_pheno_and_exclusions] after renaming birthdate column to {atc_date_col} atc_df1.columns: {atc_df1.columns}")
        if("diagnosis" in atc_df1.columns and atc_diag_col != "diagnosis"):
            atc_df1.rename(columns={"diagnosis":atc_diag_col},inplace=True)
            logger.info(f"[process_pheno_and_exclusions] after renaming diagnosis column to {atc_diag_col} atc_df1.columns: {atc_df1.columns}")
        logger.info(f"[process_pheno_and_exclusions] after finalize_lpr_data; atc_df1.columns: {atc_df1.columns}")
        logger.info(f"[process_pheno_and_exclusions] atc_df1.head(5): {atc_df1.head(5)}")
        if (qced_iids != ""):
            if (len(qced_iids_to_keep) > 0):
                logger.info(f"[process_pheno_and_exclusions] qced_iids_to_keep.head(5): {qced_iids_to_keep.head(5)}")
                atc_df1 = atc_df1[atc_df1[iidcol].isin(qced_iids_to_keep[iidcol])]
        if (general_exclusions != ""):
            if (len(iids_to_exclude) > 0):
                logger.info(f"[process_pheno_and_exclusions] iids_to_exclude.head(5): {iids_to_exclude.head(5)}")
                atc_df1 = atc_df1[~atc_df1[iidcol].isin(iids_to_exclude[iidcol])]
        logger.info(f"[process_pheno_and_exclusions] len(atc_df1.index): {len(atc_df1.index)}")
        logger.info(f"[process_pheno_and_exclusions] atc_df1.head(5): {atc_df1.head(5)}")
    
    for InclusionReason in in_pheno_codes['Disorder']:
        logger.info(f"[process_pheno_and_exclusions] Building CaseControl list regarding {InclusionReason}")
        if not verbose:
            print(f"[process_pheno_and_exclusions] Building CaseControl list regarding {InclusionReason}")
        values_to_match = ""
        values_to_match = set(str(value) for value in in_pheno_codes.loc[in_pheno_codes['Disorder'] == InclusionReason, "Disorder Codes"].iloc[0])
        logger.info(f"[process_pheno_and_exclusions] values_to_match: {values_to_match}")
        if InclusionReason == 'ATC' or ATC_Requested == "All":
            logger.info(f"[process_pheno_and_exclusions] Identified that you are running all ATC call based on the following Name: {InclusionReason}")
            filtered_df = build_phenotype_cases(    
                df1=atc_df1, exact_match=exact_match,  values_to_match=values_to_match, 
                diagnostic_col=atc_diag_col, birthdatecol=birthdatecol,
                iidcol=iidcol, input_date_in_name=atc_date_col, input_date_out_name=atc_date_col,
                verbose=verbose, Covariates=True, Covar_Name=InclusionReason, skip_icd_update = skip_icd_update, 
                remove_point_in_diag_request = remove_point_in_diag_request, 
                ICDCM = ICDCM, noLeadingICD = noLeadingICD, icdprefix = icdprefix
            )
            # del atc_df1
        if ATC_Requested == "Some":
            atc_values_to_match = []
            non_atc_values_to_match = []
            atc_values_to_match = [value for value in values_to_match if str(value).startswith('ATC')]
            non_atc_values_to_match = [value for value in values_to_match if not str(value).startswith('ATC')]
            if cluster_run in DK_clusters:
                atc_values_to_match = [value for value in values_to_match if not str(value).startswith('ICD')]
                non_atc_values_to_match = [value for value in values_to_match if str(value).startswith('ICD')]
            if atc_values_to_match and not non_atc_values_to_match:
                filtered_df = build_phenotype_cases(    
                    df1=atc_df1, exact_match=exact_match,  values_to_match=atc_values_to_match, 
                    diagnostic_col=atc_diag_col, birthdatecol=birthdatecol,
                    iidcol=iidcol, input_date_in_name=atc_date_col, input_date_out_name=atc_date_col,
                    verbose=verbose, Covariates=True, Covar_Name=InclusionReason, skip_icd_update = skip_icd_update, 
                    remove_point_in_diag_request = remove_point_in_diag_request, 
                    ICDCM = ICDCM, noLeadingICD = noLeadingICD, icdprefix = icdprefix
                )
                logger.info(f"[process_pheno_and_exclusions] Identified to map only ATC codes for your phenotype {InclusionReason} and identified {str(len(filtered_df.index))} cases from atc_df1 for the following codes: {atc_values_to_match}; values_to_match {values_to_match} ")
            elif non_atc_values_to_match and not atc_values_to_match:
                filtered_df = build_phenotype_cases(    
                    df1=df1, exact_match=exact_match,  values_to_match=non_atc_values_to_match, 
                    diagnostic_col=diagnostic_col, birthdatecol=birthdatecol,
                    iidcol=iidcol, input_date_in_name=input_date_in_name, input_date_out_name=input_date_out_name,
                    verbose=verbose, Covariates=True, Covar_Name=InclusionReason, skip_icd_update = skip_icd_update, 
                    remove_point_in_diag_request = remove_point_in_diag_request, 
                    ICDCM = ICDCM, noLeadingICD = noLeadingICD, icdprefix = icdprefix
                )
                logger.info(f"[process_pheno_and_exclusions] Identified to map only NON-ATC codes for your phenotype {InclusionReason} and identified {str(len(filtered_df.index))} cases from df1 for the following codes: {non_atc_values_to_match}; values_to_match {values_to_match} ")
            elif non_atc_values_to_match and atc_values_to_match:
                filtered_df = build_phenotype_cases(    
                    df1=[df1,atc_df1], exact_match=exact_match,  values_to_match=[non_atc_values_to_match,atc_values_to_match], 
                    diagnostic_col=[diagnostic_col,atc_diag_col], birthdatecol=birthdatecol,
                    iidcol=iidcol, input_date_in_name=[input_date_in_name,atc_date_col], input_date_out_name=[input_date_out_name,atc_date_col],
                    verbose=verbose, Covariates=True, Covar_Name=InclusionReason, skip_icd_update = skip_icd_update, 
                    remove_point_in_diag_request = remove_point_in_diag_request, 
                    ICDCM = ICDCM, noLeadingICD = noLeadingICD, icdprefix = icdprefix
                )
                logger.info(f"[process_pheno_and_exclusions] Identified to map NON-ATC and ATC codes for your phenotype {InclusionReason} and identified {str(len(filtered_df.index))} cases from [df1,atc_df1] for the following codes: {[non_atc_values_to_match,atc_values_to_match]}; values_to_match {values_to_match} ")
            else:
                logger.info(f"[process_pheno_and_exclusions] WARNING: No values to match given for your phenotype {InclusionReason}; [non_atc_values_to_match,atc_values_to_match] {[non_atc_values_to_match,atc_values_to_match]}; values_to_match {values_to_match} ")
        else:
            filtered_df = build_phenotype_cases(    
                df1=df1, exact_match=exact_match,  values_to_match=values_to_match, 
                diagnostic_col=diagnostic_col, birthdatecol=birthdatecol,
                iidcol=iidcol, input_date_in_name=input_date_in_name, input_date_out_name=input_date_out_name,
                verbose=verbose, Covariates=True, Covar_Name=InclusionReason, skip_icd_update = skip_icd_update, 
                remove_point_in_diag_request = remove_point_in_diag_request, 
                ICDCM = ICDCM, noLeadingICD = noLeadingICD, icdprefix = icdprefix
            )
            logger.info(f"[process_pheno_and_exclusions] Identified to map only NON-ATC codes and identified {str(len(filtered_df.index))} cases from df1 for the following codes: {values_to_match}")
        cols_to_keep = [[iidcol, "diagnosis", "diagnoses", "in_dates", "out_dates", "first_dx", "last_dx", "n_diags", "n_unique_in_days"] + extra_cols_to_keep ]
        # Flatten the list if needed (in case cols_to_keep is nested)
        if isinstance(cols_to_keep, list) and any(isinstance(i, list) for i in cols_to_keep):
            cols_to_keep = [item for sublist in cols_to_keep for item in sublist]

        # Filter columns that actually exist in filtered_df
        available_cols = [col for col in cols_to_keep if col in filtered_df.columns]

        # Keep only those columns
        filtered_df = filtered_df[available_cols] 
        logger.info(f"[process_pheno_and_exclusions] Identified {str(len(filtered_df.index))} of Cases for {InclusionReason}")
        if not verbose:
            print(f"[process_pheno_and_exclusions] Identified {str(len(filtered_df.index))} of Cases for {InclusionReason}")
        logger.info(f"[process_pheno_and_exclusions] filtered_df.columns: {filtered_df.columns}")
        if not filtered_df.empty:
            if tmp_cases_df.empty:
                tmp_cases_df = filtered_df.copy()

                # Define base column mapping: original  new name with InclusionReason prefix
                base_column_map = {
                    "diagnosis": InclusionReason,
                    "diagnoses": InclusionReason + "_Codes",
                    "in_dates": InclusionReason + "_In_Dates",
                    "out_dates": InclusionReason + "_Out_Dates",
                    "first_dx": InclusionReason + "_earliest_date",
                    "last_dx": InclusionReason + "_latest_date",
                    "n_diags": InclusionReason + "_n_diags",
                    "n_unique_in_days": InclusionReason + "_n_unique_in_days"
                }

                # Copy each base column into new InclusionReason-prefixed column
                for original_col, new_col in base_column_map.items():
                    if original_col in tmp_cases_df.columns:
                        tmp_cases_df[new_col] = tmp_cases_df[original_col].copy()

                # Handle extra covariate columns
                for col in extra_cols_to_keep:
                    if col in tmp_cases_df.columns:
                        tmp_cases_df[InclusionReason + '_' + col] = tmp_cases_df[col].copy()
                
            else:
                rename_map = {
                    "diagnosis": InclusionReason,
                    "diagnoses": InclusionReason + "_Codes",
                    "in_dates": InclusionReason + "_In_Dates",
                    "out_dates": InclusionReason + "_Out_Dates",
                    "first_dx": InclusionReason + '_earliest_date',
                    "last_dx": InclusionReason + '_latest_date',
                    "n_diags": InclusionReason + '_n_diags',
                    "n_unique_in_days": InclusionReason + '_n_unique_in_days'
                }

                filtered_df.rename(columns=rename_map, inplace=True)
                
                # Start with known columns to keep
                filtered_df_cols_to_keep = list(rename_map.values()) + [InclusionReason]
                filtered_df_cols_to_keep.append(iidcol)
                
                # Optionally include covariates
                for col in extra_cols_to_keep:
                    if col in filtered_df.columns:
                        new_col_name = InclusionReason + '_' + col
                        filtered_df.rename(columns={col: new_col_name}, inplace=True)
                        filtered_df_cols_to_keep.append(new_col_name)
                # Filter columns
                filtered_df = filtered_df[filtered_df_cols_to_keep]

                # Merge into tmp_cases_df (assumes tmp_cases_df exists)
                tmp_cases_df = tmp_cases_df.merge(filtered_df, on=iidcol, how='outer')
                
            for suffix in ["", "_In_Dates", "_Out_Dates"]:
                col = InclusionReason + suffix
                if col in tmp_cases_df.columns:
                    tmp_cases_df[col] = tmp_cases_df[col].fillna("")
        else:
            # Define the fixed suffixes
            base_suffixes = [
                "", "_Codes", "_In_Dates", "_Out_Dates",
                "_earliest_date", "_latest_date", "_n_diags", "_n_unique_in_days"
            ]

            # Set empty strings for each fixed column
            for suffix in base_suffixes:
                col = InclusionReason + suffix
                tmp_cases_df[col] = ""

            # Set empty strings for each of the extra columns
            for col in extra_cols_to_keep:
                tmp_cases_df[InclusionReason + '_' + col] = ""
    
        del filtered_df
    if (ATC_Requested == "All" or ATC_Requested == "Some"):
        try:
            del atc_df1
        except:
            logger.error(f"Could not delete atc_df1; Dataframe that is based on your ATC input.")
    if (qced_iids != ""):
        del qced_iids_to_keep
    if (general_exclusions != ""):
        del iids_to_exclude
    #Housekeeping
    del in_pheno_codes
    gc.collect()
    if (len(tmp_cases_df) == 0):
        logger.info("[process_pheno_and_exclusion] Error: No Cases found. Are your input diagnostic codes given in the phenotype file? Exiting")
        logger.info("[process_pheno_and_exclusion] Phenotype codes to map: ")
        logger.info(values_to_match)
        if not verbose:
            print("[process_pheno_and_exclusion] Error: No Cases found. Are your input diagnostic codes given in the phenotype file? Exiting")
            print("[process_pheno_and_exclusion] Phenotype codes to map: ")
            print(values_to_match)
        exit()
    if (input_date_out_name == input_date_in_name):
        input_date_out_name = 'date_out'
    
    casecontrol_df = tmp_cases_df.copy()
    if ("diagnosis" in casecontrol_df.columns):
        logger.info(f"[process_pheno_and_exclusion] Description of DIAGNOSIS: {casecontrol_df.diagnosis.value_counts()}; {casecontrol_df.diagnosis.head(5)}")
    else:
        logger.info(f"[process_pheno_and_exclusion] Could not find \"diagnosis\" in columns: {casecontrol_df.columns}")
    del tmp_cases_df
    gc.collect()
    if verbose:
        logger.debug("[process_pheno_and_exclusion] Mem after building casecontrol_df and cleaning up tmp_cases_df and tmp_controls_df:")
        usage()
    if (not cluster_run == "CHB_DBDS" and input_date_in_name != "date_in" and not isinstance(df1, type(None))):
        #Change colname of indate to 'date_in'
        if (input_date_in_name in df1.columns and "date_in" in df1.columns):
            logger.info(f"[process_pheno_and_exclusions] WARNING: There exists a column \"date_in\" within your -f input as well as the column {input_date_in_name} which will be renamed to \"date_in\". This can lead to issues, thus we will rename the original column \"date_in\" to \"date_in_original\".")
            df1.rename(columns={"date_in": "date_in_original"}, inplace=True)
        df1.rename(columns={input_date_in_name: "date_in"}, inplace=True)
        input_date_in_name = "date_in"
    if not use_predefined_exdep_exclusions and lifetime_exclusions_file == "" and post_exclusions_file == "" and oneYearPrior_exclusions_file == "":
        #del df1
        if verbose:
            logger.debug("[process_pheno_and_exclusion] Mem after building tmp_cases_df and tmp_controls_df; AND deleting df1:")
            usage()
    if (lifetime_exclusions_file != "" and use_predefined_exdep_exclusions.empty):
            multi_exclusions = False
            if not verbose:
                logger.info("[process_pheno_and_exclusions] Loading phenotypes for Lifetime exclusions.")
            lifetime_exclusions , notneeded = load_phenotypes(pheno_request=lifetime_exclusions_file, pheno_name="LifeExcl", icdprefix=icdprefix, noLeadingICD=noLeadingICD, 
                                                  ICDCM=ICDCM, skip_icd_update=skip_icd_update, exact_match=exact_match, remove_point_in_diag_request=remove_point_in_diag_request)
            assert isinstance(lifetime_exclusions.iloc[0]["Disorder Codes"], list)
            del(notneeded)
    if (oneYearPrior_exclusions_file != "" and use_predefined_exdep_exclusions.empty):
            multi_exclusions = False
            if not verbose:
                logger.info("[process_pheno_and_exclusions] Loading phenotypes for One Year Prior exclusions.")
            oneYearPrior_exclusions , notneeded = load_phenotypes(pheno_request=lifetime_exclusions_file, pheno_name="OneYearPriorExcl", icdprefix=icdprefix, noLeadingICD=noLeadingICD, 
                                                  ICDCM=ICDCM, skip_icd_update=skip_icd_update, exact_match=exact_match, remove_point_in_diag_request=remove_point_in_diag_request)
            assert isinstance(oneYearPrior_exclusions.iloc[0]["Disorder Codes"], list)
            del(notneeded)
    if (post_exclusions_file != "" and use_predefined_exdep_exclusions.empty):
            multi_exclusions = False
            if not verbose:
                logger.info("[process_pheno_and_exclusions] Loading phenotypes for Post-Onset exclusions.")
            post_exclusions , notneeded = load_phenotypes(pheno_request=lifetime_exclusions_file, pheno_name="PostExcl", icdprefix=icdprefix, noLeadingICD=noLeadingICD, 
                                                  ICDCM=ICDCM, skip_icd_update=skip_icd_update, exact_match=exact_match, remove_point_in_diag_request=remove_point_in_diag_request)
            assert isinstance(post_exclusions.iloc[0]["Disorder Codes"], list)
            del(notneeded)
    
    # Append CaseControl Dataframe with information regarding the ExDEP exclusions
    if (not lifetime_exclusions.empty):
        for ExclusionReason in lifetime_exclusions['Disorder']:
            logger.info(f"[process_pheno_and_exclusions] Building ExDEP exclusions regarding the LIFETIME exclusion {ExclusionReason}")
            values_to_match = ""
            values_to_match = set(str(value) for value in lifetime_exclusions.loc[lifetime_exclusions['Disorder'] == ExclusionReason, "Disorder Codes"].iloc[0])
            casecontrol_df = build_ExDEP_exclusions(casecontrol_df=casecontrol_df, df1=df1, diagnostic_col=diagnostic_col, iidcol=iidcol, birthdatecol=birthdatecol, 
                                                    input_date_in_name=input_date_in_name, input_date_out_name=input_date_out_name, diag_df=values_to_match, diag=ExclusionReason, 
                                                    exact_match=exact_match, verbose=verbose, get_earliest_date_from_data=True, dbds_run=dbds_run, extra_cols_to_keep=extra_cols_to_keep, 
                                                    skip_icd_update = skip_icd_update, remove_point_in_diag_request = remove_point_in_diag_request, ICDCM = ICDCM, noLeadingICD = noLeadingICD, icdprefix = icdprefix)
            #logger.info(f"Description of {ExclusionReason}: {casecontrol_df[ExclusionReason].value_counts()}; {casecontrol_df[ExclusionReason].head(5)}")
    if (not oneYearPrior_exclusions.empty):
        for ExclusionReason in oneYearPrior_exclusions['Disorder']:
            logger.info(f"[process_pheno_and_exclusions] Building exclusions regarding the 1Y PRIOR exclusion {ExclusionReason}")
            values_to_match = ""
            values_to_match = set(str(value) for value in oneYearPrior_exclusions.loc[oneYearPrior_exclusions['Disorder'] == ExclusionReason, "Disorder Codes"].iloc[0])
            casecontrol_df = build_ExDEP_exclusions(casecontrol_df=casecontrol_df, df1=df1, diagnostic_col=diagnostic_col, iidcol=iidcol, birthdatecol=birthdatecol, 
                                                    input_date_in_name=input_date_in_name, input_date_out_name=input_date_out_name, diag_df=values_to_match, diag=ExclusionReason, 
                                                    exact_match=exact_match, verbose=verbose, get_earliest_date_from_data=True, dbds_run=dbds_run, extra_cols_to_keep=extra_cols_to_keep, 
                                                    skip_icd_update = skip_icd_update, remove_point_in_diag_request = remove_point_in_diag_request, ICDCM = ICDCM, noLeadingICD = noLeadingICD, icdprefix = icdprefix)
            #logger.info(f"Description of {ExclusionReason}: {casecontrol_df[ExclusionReason].value_counts()}; {casecontrol_df[ExclusionReason].head(5)}")
    if (not post_exclusions.empty):
        for ExclusionReason in post_exclusions['Disorder']:
            logger.info(f"[process_pheno_and_exclusions] Building exclusions regarding the POST exclusion {ExclusionReason}")
            values_to_match = ""
            values_to_match = set(str(value) for value in post_exclusions.loc[post_exclusions['Disorder'] == ExclusionReason, "Disorder Codes"].iloc[0])
            casecontrol_df = build_ExDEP_exclusions(casecontrol_df=casecontrol_df, df1=df1, diagnostic_col=diagnostic_col, iidcol=iidcol, birthdatecol=birthdatecol, 
                                                    input_date_in_name=input_date_in_name, input_date_out_name=input_date_out_name, diag_df=values_to_match, diag=ExclusionReason, 
                                                    exact_match=exact_match, verbose=verbose, get_earliest_date_from_data=True, dbds_run=dbds_run, extra_cols_to_keep=extra_cols_to_keep, 
                                                    skip_icd_update = skip_icd_update, remove_point_in_diag_request = remove_point_in_diag_request, ICDCM = ICDCM, noLeadingICD = noLeadingICD, icdprefix = icdprefix)
            #logger.info(f"Description of {ExclusionReason}: {casecontrol_df[ExclusionReason].value_counts()}; {casecontrol_df[ExclusionReason].head(5)}")
    if (not covariates.empty):
        for ExclusionReason in covariates['Disorder']:
            logger.info(f"[process_pheno_and_exclusions] Building Cases regarding the Covariates {ExclusionReason}")
            values_to_match = ""
            values_to_match = set(str(value) for value in covariates.loc[covariates['Disorder'] == ExclusionReason, "Disorder Codes"].iloc[0])
            casecontrol_df = build_ExDEP_exclusions(casecontrol_df=casecontrol_df, df1=df1, diagnostic_col=diagnostic_col, iidcol=iidcol, birthdatecol=birthdatecol, 
                                                    input_date_in_name=input_date_in_name, input_date_out_name=input_date_out_name, diag_df=values_to_match, diag=ExclusionReason, 
                                                    exact_match=exact_match, verbose=verbose, get_earliest_date_from_data=True, dbds_run=dbds_run, extra_cols_to_keep=extra_cols_to_keep, 
                                                    skip_icd_update = skip_icd_update, remove_point_in_diag_request = remove_point_in_diag_request, ICDCM = ICDCM, noLeadingICD = noLeadingICD, icdprefix = icdprefix)
            #logger.info(f"Description of {ExclusionReason}: {casecontrol_df[ExclusionReason].value_counts()}; {casecontrol_df[ExclusionReason].head(5)}")
    del df1
    if verbose:
        print ("[process_pheno_and_exclusions] Mem after building all ExDEP exclusions and deleteing df1.")
        usage()
    gc.collect()
    
    if verbose:
        print ("[process_pheno_and_exclusions] Mem after building potential Exclusions and Covariates and deleteing df1.")
        usage()
        logger.debug(casecontrol_df.head(5))
        logger.debug(df3.head(5))
    # Add information from stam_file and keep also IIDs that are not given in df1 (lpr_file).
    if casecontrol_df.empty:
        logger.info("[process_pheno_and_exclusions] casecontrol_df is empty")
        casecontrol_df = pd.DataFrame(columns=[iidcol,"diagnosis","diagnoses","in_dates","out_dates","first_dx","n_diags","n_unique_in_days"])
        result_df = pd.merge(casecontrol_df, df3, on=iidcol, how='outer')
        result_df["diagnosis"] = "Control"
    else:
        result_df = pd.merge(casecontrol_df, df3, on=iidcol, how='outer')
        logger.info(f"[process_pheno_and_exclusions] casecontrol_df.columns: {casecontrol_df.columns}\nresult_df.columns {result_df.columns}")
    if verbose:
        logger.debug(result_df.head(5))
    print(result_df.head(5))
    print("\"MDD\" in result_df.columns: ",("MDD" in result_df.columns))
    result_df.loc[(result_df["diagnosis"] != "Case") & 
              (result_df["diagnosis"] != "Control") & 
              (result_df["diagnosis"] != "Case_Excluded"), "diagnosis"] = "Control"
    if (not("birthdate" in result_df.columns) and (birthdatecol in result_df.columns)):
        result_df.rename(columns={birthdatecol:"birthdate"}, inplace=True)
        if verbose:
            logger.debug(f"[process_pheno_and_exclusions] Updating colname of result_df from {birthdatecol} to birthdate")
    if ("birthdate_x" in result_df.columns):
        if verbose:
            logger.debug("[process_pheno_and_exclusions] Updating colname of result_df from birthdate_x to birthdate")
        result_df.rename(columns={"birthdate_x":"birthdate"}, inplace=True)
    if (not("birthdate" in result_df.columns) and (birthdatecol+"_x" in result_df.columns)):
        result_df.rename(columns={birthdatecol+"_x":"birthdate"}, inplace=True)
        if verbose:
            logger.debug(f"[process_pheno_and_exclusions] Updating colname of result_df from {birthdatecol}_x to birthdate")
    if (not("birthdate" in result_df.columns) and (birthdatecol in result_df.columns)):
        result_df.rename(columns={birthdatecol:"birthdate"}, inplace=True)
        if verbose:
            logger.debug(f"[process_pheno_and_exclusions] Updating colname of result_df from {birthdatecol} to birthdate")
    del df3
    if verbose:
        logger.debug("[process_pheno_and_exclusions] Mem after deleting df3, updating tmp_controls_df, building dx_result_df, casecontrol_df and result_df:")
        usage()
    gc.collect()
    if verbose:
        logger.debug(result_df.head(5))
        logger.debug(birthdatecol + " in result_df: " + str('birthdate' in result_df.columns))
        logger.debug(f"result_df: ({str(result_df[iidcol].nunique())} rows) - {result_df.columns}")
    if ("date_in_x" in result_df.columns and not "first_dx" in result_df.columns):
            result_df.rename(columns={"date_in_x": "first_dx"}, inplace=True)
    if ("in_dates_x" in result_df.columns and not "in_dates" in result_df.columns):
            result_df.rename(columns={"in_dates_x": "in_dates"}, inplace=True)
    if ("date_in_y" in result_df.columns and not "first_dx" in result_df.columns):
            result_df.rename(columns={"date_in_y": "first_dx"}, inplace=True)
    if ("in_dates_y" in result_df.columns and not "in_dates" in result_df.columns):
            result_df.rename(columns={"in_dates_y": "in_dates"}, inplace=True)
    if ("date_in_x" in result_df.columns and "first_dx" in result_df.columns):
            result_df.drop("date_in_x", inplace=True)
    if ("in_dates_x" in result_df.columns and "in_dates" in result_df.columns):
            result_df.drop("in_dates_x", inplace=True)
    logger.info(result_df.columns)
    if ("date_in_y" in result_df.columns and "first_dx" in result_df.columns):
            result_df.drop("date_in_y", inplace=True, axis=1)
    if ("in_dates_y" in result_df.columns and "in_dates" in result_df.columns):
            result_df.drop("in_dates_y", inplace=True, axis=1)
    # Calculate Age at first diagnosis; Convert date columns to datetime objects
    result_df['temp_birthdate'] = result_df['birthdate'].copy()
    result_df['temp_first_dx'] = result_df['first_dx'].copy()
    result_df['Age_FirstDx'] = 0
    if verbose:
        logger.debug(f"[process_pheno_and_exclusions] {result_df.head(5)}")
        logger.debug(f"[process_pheno_and_exclusions] {result_df['diagnosis'].unique()}")  # Check unique values in 'diagnosis' column
        logger.debug(f"[process_pheno_and_exclusions] {result_df.dtypes}")  # Check data types of columns
        logger.debug(f"[process_pheno_and_exclusions] {result_df[['diagnosis', 'temp_first_dx', 'temp_birthdate']].head()}")
        logger.debug(f"[process_pheno_and_exclusions] {result_df['temp_birthdate'].head()}")
    # Calculate age for 'Case' rows
    case_indices = np.where(result_df['diagnosis'] == 'Case')[0]
    case_rows = result_df.iloc[case_indices]
    # Identify case rows
    logger.info(f"[process_pheno_and_exclusions] case_rows.head(5):{case_rows.head(5)}")
    logger.info(f"[process_pheno_and_exclusions] type(case_rows):{type(case_rows)}")
    logger.info(f"{case_rows.shape}")
    logger.info(f"[process_pheno_and_exclusions] result_df.shape:{result_df.shape}")
    logger.info(f"{result_df.loc[case_indices, 'temp_first_dx'].head(5)}")
    logger.info(f"{result_df.loc[case_indices, 'temp_birthdate'].head(5)}")
    logger.info(f"{type(result_df.loc[case_indices, 'temp_first_dx'])}")
    logger.info(f"{type(result_df.loc[case_indices, 'temp_birthdate'])}")
    if len(case_rows) > 0:
    # Calculate the age at first diagnosis
        try:
            # Use the helper function to convert each element
            first_dx_converted = result_df.loc[case_indices, 'temp_first_dx'].apply(convert_if_not_datetime)
            birthdate_converted = result_df.loc[case_indices, 'temp_birthdate'].apply(convert_if_not_datetime)
            # Compute age in years (approximate)
            age_first_dx = (first_dx_converted - birthdate_converted).dt.days // 365
        except Exception as e:
            logger.info(f"[process_pheno_and_exclusions] WARNING while processing date columns: {e}. Attempting to fix formats...")

            # Convert to datetime, force errors to NaT
            result_df['temp_first_dx'] = pd.to_datetime(result_df['temp_first_dx'], errors='coerce')
            result_df['temp_birthdate'] = pd.to_datetime(result_df['temp_birthdate'], errors='coerce')

            # Ensure both are timezone-naive
            result_df['temp_first_dx'] = result_df['temp_first_dx'].dt.tz_localize(None)
            result_df['temp_birthdate'] = result_df['temp_birthdate'].dt.tz_localize(None)

            # Drop rows with NaT values if necessary
            mask_valid = result_df[['temp_first_dx', 'temp_birthdate']].notna().all(axis=1)

            age_first_dx = (
                result_df.loc[case_indices & mask_valid, 'temp_first_dx'] - 
                result_df.loc[case_indices & mask_valid, 'temp_birthdate']
            ).dt.days // 365

        # Assign the calculated values to the DataFrame
        result_df.loc[case_indices, 'Age_FirstDx'] = age_first_dx
        logger.info(f"[process_pheno_and_exclusions] result_df.loc[case_indices, [\"diagnosis\", \"temp_first_dx\", \"temp_birthdate\"]].head(): {result_df.loc[case_indices, ['diagnosis', 'temp_first_dx', 'temp_birthdate']].head()}")
        result_df.drop("temp_first_dx", inplace=True, axis=1)
        result_df.drop("temp_birthdate", inplace=True, axis=1)
    if verbose:
        logger.debug(result_df.head(5))
    # Create a DataFrame with unique combinations of ID and DX 
    final_df = result_df.drop_duplicates(subset=iidcol)
    if verbose:
        logger.debug(f"[process_pheno_and_exclusions] final_df: ({str(final_df[iidcol].nunique())} rows) - {final_df.columns}")
    del result_df
    if (sexcol != "sex" and sexcol in final_df.columns and not "sex" in final_df.columns ):
        # Rename sex column 
        final_df.rename(columns={sexcol: 'sex'}, inplace=True)
    elif (sexcol != "sex" and "sex" in final_df.columns and sexcol in final_df.columns ):
        #drop the duplicated column
        final_df.drop(sexcol, inplace=True, axis=1)
    if verbose:
        logger.debug("[process_pheno_and_exclusions] Mem after deleting result_df:")
        usage()
    else:
        gc.collect()
    if (iidstatus_col != ""):
        if (iidstatus_col in df4):
            df4.rename(columns={iidstatus_col: "C_STATUS"}, inplace=True)
    if (iidstatusdate != ""):
        if (iidstatus_col in df4):
            df4.rename(columns={iidstatus_col: "C_STATUS_DATE"}, inplace=True)
    if (addition_information_file != ''):
        logger.info(f"df4 cols: {df4.columns}")
        try:
            if(len(df4[iidcol]) > df4[iidcol].nunique()):
                logger.info("[process_pheno_and_exclusions] Warning: df4 contains duplicated iids.")
            # Add the person information
            final_df =  pd.merge(final_df, df4, on=iidcol, how='left')
            if verbose:
                logger.debug(f"[process_pheno_and_exclusions] Merged df4 ({addition_information_file}) to final_df ({str(final_df[iidcol].nunique())} rows)")
                logger.debug("[process_pheno_and_exclusions] Mem after merging with df4 and deleting df4 (right before saving output file):")
                usage()
        except Exception as e:
            logger.info(f"[process_pheno_and_exclusions] Error in df4 processing: {e}")
        del df4

    cols_to_use = [iidcol,'diagnosis','diagnoses','first_dx','in_dates','birthdate','Age_FirstDx']
    if (not lifetime_exclusions.empty):
        for ExclusionReason in lifetime_exclusions['Disorder']:
            cols_to_use = sorted(
                    set(cols_to_use + [ExclusionReason,ExclusionReason+"_In_Dates",ExclusionReason+"_Out_Dates"])
                )
    if (not oneYearPrior_exclusions.empty):
        for ExclusionReason in oneYearPrior_exclusions['Disorder']:
            cols_to_use = sorted(
                    set(cols_to_use + [ExclusionReason,ExclusionReason+"_In_Dates",ExclusionReason+"_Out_Dates"])
                )
    if (not post_exclusions.empty):
        for ExclusionReason in post_exclusions['Disorder']:
            cols_to_use = sorted(
                    set(cols_to_use + [ExclusionReason,ExclusionReason+"_In_Dates",ExclusionReason+"_Out_Dates"])
                )
    tmp_final_df = final_df[cols_to_use].copy()
    if (not lifetime_exclusions.empty or not oneYearPrior_exclusions.empty or not post_exclusions.empty):
        final_df = final_df.merge(Exclusion_interpreter(data=tmp_final_df[tmp_final_df["diagnosis"] == "Case"].copy(), min_Age=min_Age, max_Age=max_Age, lifetime_exclusions=lifetime_exclusions, oneYearPrior_exclusions=oneYearPrior_exclusions, post_exclusions=post_exclusions, verbose=verbose), on=iidcol, how='left')
    del(tmp_final_df)
    
    if (lifetime_exclusions_file != "" or oneYearPrior_exclusions_file != "" or post_exclusions_file != "" or not lifetime_exclusions.empty or not oneYearPrior_exclusions.empty or not post_exclusions.empty):
        logger.info("[process_pheno_and_exclusions] Interpreting the exclusions.")
        if not verbose:
            print("[process_pheno_and_exclusions] Interpreting the exclusions.")
        if verbose:
            logger.debug(f"[process_pheno_and_exclusions] final_df: ({str(final_df[iidcol].nunique())} rows) - {final_df.columns}")
        final_df.fillna('', inplace=True)
        final_df.replace(np.nan, '') 
        logger.info(f"[process_pheno_and_exclusion] [oneYearPrior_exclusions, post_exclusions, covariates, lifetime_exclusions]:{[oneYearPrior_exclusions, post_exclusions, covariates, lifetime_exclusions]}")
        if not verbose:
            print(f"[process_pheno_and_exclusion] [oneYearPrior_exclusions, post_exclusions, covariates, lifetime_exclusions]:\noneYearPrior_exclusions: {oneYearPrior_exclusions}\npost_exclusions: {post_exclusions}\ncovariates: {covariates}\nlifetime_exclusions: {lifetime_exclusions}")
        disorder_sources = [oneYearPrior_exclusions, post_exclusions, covariates, lifetime_exclusions]
        if (ctype_col in final_df.columns):
            base_cols = [iidcol,'diagnosis','diagnoses','first_dx','in_dates','birthdate',ctype_col,'Age_FirstDx']
        else:
            base_cols = [iidcol,'diagnosis','diagnoses','first_dx','in_dates','birthdate','Age_FirstDx']
        cols = columns_for_disorders(final_df, iidcol, disorder_sources)
        tmp_final_df = final_df[cols].copy()
        final_df.loc[final_df['Level2_AgeExclusion'].isna(),'Level2_AgeExclusion'] = "FALSE"
        final_df.fillna('', inplace=True)
        final_df.loc[(final_df["diagnosis"] == "Control"),"Level3_CaseControl"] = "Control"
        final_df.loc[(final_df['diagnosis'] == "Control"),'Level3_Age_FirstDx'] = 0
        final_df.loc[:,"Level3_CaseControl_AgeExclusions"] = "Control"
        final_df.loc[(final_df["diagnosis"] == "Case"),"Level3_CaseControl_AgeExclusions"] = "Case_Excluded"
        if verbose:
            logger.debug(final_df.head(5))
            logger.debug(f"[process_pheno_and_exclusions] {final_df[[iidcol,'diagnosis','diagnoses','Level3_CaseControl','date_Level2_modifier']].head(10)}")
        # Check types
        logger.info(f"""[process_pheno_and_exclusions] \n
                    {final_df["Level3_CaseControl"].dtypes} \n
                    {final_df["Level2_AgeExclusion"].dtypes} \n
                    {final_df["Level3_CaseControl_AgeExclusions"].dtypes} \n
                    {final_df["Level3_CaseControl"].value_counts()} \n
                    {final_df["Level2_AgeExclusion"].value_counts()} \n
                    {final_df["Level3_CaseControl_AgeExclusions"].value_counts()} \n
                    {final_df[["Level3_CaseControl","Level2_AgeExclusion"]].value_counts()}  \n
                    {final_df.loc[
                        (final_df["Level3_CaseControl"] == "Case") & 
                        (final_df["Level2_AgeExclusion"] == "FALSE"),["Level3_CaseControl", "Level2_AgeExclusion", "Level3_CaseControl_AgeExclusions"]].head(5)}""")
        try:
            final_df.loc[
                (final_df["Level3_CaseControl"] == "Case") & 
                (final_df["Level2_AgeExclusion"] == "FALSE"),
                "Level3_CaseControl_AgeExclusions"] = "Case"
        except Exception as e:
            logger.info(f"[process_pheno_and_exclusions] Error: {e}")
        logger.info(final_df.loc[
                (final_df["Level3_CaseControl"] == "Case") & 
                (final_df["Level2_AgeExclusion"] == "FALSE"),["Level3_CaseControl", "Level2_AgeExclusion", "Level3_CaseControl_AgeExclusions"]].head(5))
        final_df.loc[final_df['diagnosis'] == "Control",'Level2_FirstDx'] = ""
        final_df.loc[final_df['Level2_AgeExclusion'].isna(),'Level2_AgeExclusion'] = "TRUE"
        final_df.replace(np.nan, '') 
        if verbose:
            logger.debug(final_df.head(5))
            logger.debug(final_df[[iidcol,'diagnosis','diagnoses','Level3_CaseControl','date_Level2_modifier']].head(10))
        final_df = Build_sankey_data(final_df,outfile,iidcol,verbose)
        # Reformat data for output into comma separates strings isntead of list

        final_df["diagnoses_Level2_modifier"] = final_df.apply(lambda row: ','.join(row["diagnoses_Level2_modifier"]), axis=1)
        final_df["date_Level2_modifier"] = final_df.apply(lambda row: ','.join(row["date_Level2_modifier"]), axis=1)
        final_df['disorder_Level2_modifier'] = final_df.apply(lambda row: ','.join(row['disorder_Level2_modifier']), axis=1)
        final_df['Level2_ExclusionReason'] = final_df['Level2_ExclusionReason'].apply(lambda lst: ','.join(map(str, sorted(lst))))
        final_df['Level2_FirstDx'] = final_df['Level2_FirstDx'].apply(lambda lst: ','.join(map(str, sorted(lst))))
        #final_df['Level2_ExclusionReason'] = final_df['Level2_ExclusionReason'].apply(sorted)
        #final_df['Level2_FirstDx'] = final_df['Level2_FirstDx'].apply(sorted)
    if 'dbds' in final_df.columns:
        n_before = len(final_df.index)
        if("degen_new" in final_df.columns):
            final_df = final_df.loc[~((final_df['dbds'] == "FALSE") & (final_df['degen_new'] == "FALSE") & (final_df['degen_old'] == "FALSE")),].copy()
        else:
            final_df = final_df.loc[~((final_df['dbds'] == "FALSE") & (final_df['degen'] == "FALSE")),].copy()
        n_after = len(final_df.index)
        logger.info(f"[process_pheno_and_exclusions] Excluding {str(n_before - n_after)} Individuals as they are not part of CHB or DBDS.")
        if("degen_new" in final_df.columns):
            final_df.loc[(((final_df['degen_new'] == "TRUE") | (final_df['degen_old'] == "TRUE")) & (final_df['diagnosis'] == "Control")), 'diagnosis'] = "Non-case"
        else:
            final_df.loc[((final_df['degen'] == "TRUE") & (final_df['diagnosis'] == "Control")), 'diagnosis'] = "Non-case"
    if 'dbds' in final_df.columns and exclCHBcontrols:
        iids_to_exclude_CHB_controls = final_df.loc[(final_df['diagnosis'] == "Control") & (final_df['degen_new'] == "TRUE") & (final_df['dbds'] == "FALSE"), iidcol]
        all_controls = len(final_df.loc[((final_df['diagnosis'] == "Control") | (final_df['diagnosis'] == "Non-case")), iidcol])
        final_df = final_df[~final_df[iidcol].isin(iids_to_exclude_CHB_controls)]
        logger.info(f"[process_pheno_and_exclusions] Excluding {str(len(iids_to_exclude_CHB_controls))} CHB controls (of {all_controls} controls), as we are not allowed to use them.")
    # Printing a warning, that one should take care and check that the used phenotype is well within their protocol and otherwise they will need to exclude all CHB Non-cases.
    print_CHB_warning = False
    if 'degen_new' in final_df.columns:
        logger.info(f"[process_pheno_and_exclusions] {any(((final_df['degen_new'] == True) | (final_df['degen_old'] == True)) & (final_df['diagnosis'] == 'Control'))}")
        logger.info(f"[process_pheno_and_exclusions] {len(final_df.loc[(((final_df['degen_new'] == 'TRUE') | (final_df['degen_old'] == 'TRUE')) & (final_df['diagnosis'] == 'Control')),])}")
        # Count the occurrences of "Case" and "Control" in the "diagnosis" column
        if any(((final_df['degen_new'] == "TRUE") | (final_df['degen_old'] == "TRUE")) & (final_df['diagnosis'] == "Control")):
            temp_df = final_df.copy()
            # Update the diagnosis in the temporary dataframe for display purposes
            temp_df.loc[temp_df['diagnosis'] == "Control", 'diagnosis'] = "Non-case"
            counts = temp_df['diagnosis'].value_counts()
            del(temp_df) 
            logger.info(f"""[process_pheno_and_exclusions] Case - Non-case counts\n
                        {counts}""")
            print_CHB_warning = True
        else:
            # Count the occurrences of "Case" and "Control" in the "diagnosis" column
            counts = final_df['diagnosis'].value_counts()
            logger.info(f"""[process_pheno_and_exclusions] Case-Control counts\n
                        {counts}""")
        # Count the occurrences of "Case" and "Control" in the "diagnosis" column
        counts = final_df.loc[final_df['dbds'] == "TRUE",'diagnosis'].value_counts()
        logger.info(f"""[process_pheno_and_exclusions] Case-Control counts for DBDS only\n
                    {counts}""")
        # Count the occurrences of "Case" and "Control" in the "diagnosis" column
        counts = final_df.loc[final_df['degen_new'] == "TRUE",'diagnosis'].value_counts()
        logger.info(f"""[process_pheno_and_exclusions] Case - Non-case counts for CHB only\n
                    {counts}""")
        # Count the occurrences of "Case" and "Control" in the "diagnosis" column
        if (len(final_df.loc[(final_df['degen_new'] == "TRUE") & (final_df['dbds'] == "TRUE"),'diagnosis']) > 0):
            counts = final_df.loc[(final_df['degen_new'] == "TRUE") & (final_df['dbds'] == "TRUE"),'diagnosis'].value_counts()
            logger.info(f"""[process_pheno_and_exclusions] Case-Control counts for IIDs in CHB and DBDS\n
                    {counts}""")
        else:
            logger.info("[process_pheno_and_exclusions] No overlapping IIDs between CHB and DBDS")
    else:
        # Count the occurrences of "Case" and "Control" in the "diagnosis" column
        counts = final_df['diagnosis'].value_counts()
        logger.info(f"""[process_pheno_and_exclusions] Case-Control counts\n
                    {counts}""")
        if not verbose:
            print(f"""[process_pheno_and_exclusions] Case-Control counts\n
                    {counts}""")
    if (use_predefined_exdep_exclusions):
        # Count the occurrences of "Case" and "Control" in the "diagnosis" column
        counts = final_df['Level3_CaseControl'].value_counts()
        logger.info(f"""[process_pheno_and_exclusions] Case-Control counts based on ExDEP Exclusions\n
                    {counts}""")
        if not verbose:
            print(f"""[process_pheno_and_exclusions] Case-Control counts based on ExDEP Exclusions\n
                    {counts}""")
    if (lifetime_exclusions_file != "" or post_exclusions_file != "" or oneYearPrior_exclusions_file != ""):
        # Count the occurrences of "Case" and "Control" in the "diagnosis" column
        counts = final_df['Level3_CaseControl'].value_counts()
        logger.info(f"""[process_pheno_and_exclusions] Case-Control counts based on Exclusions\n
                    {counts}""")
        if not verbose:
            print(f"""[process_pheno_and_exclusions] Case-Control counts based on Exclusions\n
                    {counts}""")
    if (print_CHB_warning):
        logger.info(50*"#")
        logger.info(50*"!")
        logger.info("[process_pheno_and_exclusions] Warning: Please take care and check that the requested phenotype is well within the underlying protocol!\nOtherwise you will need to exclude all CHB Non-cases to comply with the restricitions.")
        logger.info(50*"!")
        logger.info(50*"#")
    n_final_iids = final_df[iidcol].nunique()
    logger.info(f"[process_pheno_and_exclusions] The input STAM file had {n_stam_iids} IIDs listed. The Output file of this script has {n_final_iids} IIDs listed.")
    if "stat_x" in final_df.columns and not "stat" in final_df.columns:
        final_df.rename(columns={"stat_x":"stat"},inplace=True)
        if "stat_y" in final_df.columns:
            final_df.drop("stat_y", inplace=True, axis=1)
    if "statd_x" in final_df.columns and not "statd" in final_df.columns:
        final_df.rename(columns={"statd_x":"statd"},inplace=True)
        if "statd_y" in final_df.columns:
            final_df.drop("statd_y", inplace=True, axis=1)
    final_df_ = final_df.copy()
    final_df = final_df_.copy()
    del(final_df_)
    if "fkode" in final_df.columns:
        logger.info("[process_pheno_and_exclusions] Updating Information about DK born or not. This uses the selected country numbers used by Oleguer Plana-Ripoll https://osf.io/zhfyp/files/2cyvs on the fkode/fkode_m/fkode_f. Find more details here https://www.dst.dk/da/Statistik/dokumentation/Times/cpr-oplysninger/foedreg-kode")
        try:
            final_df['both_parents_DK'] = False
            final_df['fkode_m'] = pd.to_numeric(final_df['fkode_m'], errors='coerce')
            final_df['fkode_f'] = pd.to_numeric(final_df['fkode_f'], errors='coerce')
            final_df['fkode'] = pd.to_numeric(final_df['fkode'], errors='coerce')
            final_df.loc[
                    (BirthCountry_DK(final_df['fkode_m']) == "Denmark") & 
                    (BirthCountry_DK(final_df['fkode_f']) == "Denmark") , 
                    'both_parents_DK'
                ] = True
            final_df._consolidate_inplace()
            final_df['DK_born'] = False
            final_df.loc[(BirthCountry_DK(final_df['fkode']) == "Denmark"), 'DK_born'] = True
        except Exception as e:
            logger.info(f"""[process_pheno_and_exclusions] An error occurred while mapping final_df['fkode','fkode_m','fkode_f'].\n
                        Head of the file:\n{final_df[['fkode','fkode_m', 'fkode_f']].head(5)}\n
                        Error message: {e}""")
    write_mode = 'w'
    write_header = True
    if append:
        write_mode = 'a'
        write_header = False
    if verbose:
        logger.debug(f"[process_pheno_and_exclusions] append is set to {append}. write_mode is set to {write_mode}. write_header is set to {write_header}.")
    # Compute the result and save it to a new file 
    final_df.to_csv(outfile, sep="\t", index=False, quoting=False, header=write_header, mode=write_mode)
    reformat_to_tsv(outfile)
    logger.info(f"[process_pheno_and_exclusions] You can find your output file here {outfile}")
    if write_pickle:
        logger.info("[process_pheno_and_exclusions] Building PICKLE file.")
        with open(outfile+'.pickle', 'wb') as handle:
            pickle.dump(final_df, handle, protocol=pickle.HIGHEST_PROTOCOL)
    if iidcol != "IID":
        final_df['IID'] = final_df[iidcol].copy()
    if write_fastGWA_format:
        logger.info("[process_pheno_and_exclusions] Building FastGWA phenotype file.")
        # Create the 'FID' column as a copy of 'IID'
        final_df['FID'] = final_df['IID'].copy()
        # Map 'diagnosis' to 'CaseControl' (0/1 coding)
        logger.info("[process_pheno_and_exclusions] Mapping CaseControl status to 0/1")
        final_df['CaseControl'] = final_df['diagnosis'].map({
            'Case': "1", 
            'Control': "0", 
            'Non-case': "0", 
            'Case_Excluded': "0"
        })
        fgwa = final_df[['FID', 'IID', 'CaseControl']]
        fgwa.to_csv(outfile + ".fgwa.pheno", sep="\t", mode=write_mode,
                    index=False, quoting=csv.QUOTE_NONE, header=write_header)

    if write_Plink2_format:
        logger.info("[process_pheno_and_exclusions] Building PLINK2 phenotype file.")
        # Create the '#FID' column as a copy of 'IID'
        final_df['#FID'] = final_df['IID'].copy()
        # Map 'diagnosis' to 'CaseControl' (1/2 coding)
        logger.info("[process_pheno_and_exclusions] Mapping CaseControl status to 1/2")
        final_df['CaseControl'] = final_df['diagnosis'].map({
            'Case': "2", 
            'Control': "1", 
            'Non-case': "1", 
            'Case_Excluded': "1"
        })
        plink2 = final_df[['#FID', 'IID', 'CaseControl']]
        plink2.to_csv(outfile + ".plink2.pheno", sep="\t", mode=write_mode,
                    index=False, quoting=csv.QUOTE_NONE, header=write_header)
   
def build_ExDEP_exclusions(
    casecontrol_df: pd.DataFrame,
    df1: pd.DataFrame,
    diagnostic_col: str,
    iidcol: str,
    birthdatecol: str,
    input_date_in_name: str,
    input_date_out_name: str,
    diag_df: pd.DataFrame,
    diag: str,
    exact_match: bool,
    verbose: bool,
    get_earliest_date_from_data: bool,   # kept for API compatibility (unused here)
    dbds_run: bool = False,
    update_diag_col_name_to_diag: bool = False,   # kept for API compatibility (unused here)
    extra_cols_to_keep: list = [],
    skip_icd_update: bool = False, 
    remove_point_in_diag_request: bool = False, 
    ICDCM: bool = False, 
    noLeadingICD: bool = False, 
    icdprefix: str = "",
) -> pd.DataFrame:
    """
    Build exclusions for a given disorder `diag` and merge into `casecontrol_df`.

    Expects build_phenotype_cases(...) to return at least:
      - iidcol
      - 'diagnoses'
      - 'in_dates'
      - 'out_dates'
      - 'first_dx'
    and (for dbds_run=True): 'type', 'source', 'register'.

    Returns updated casecontrol_df with added columns:
      diag, diag+'_In_Dates', diag+'_Out_Dates', diag+'_earliest_date'
      (plus optional DBDS meta columns if present when dbds_run=True).
    """

    def _select_existing(df: pd.DataFrame, wanted: Sequence[str]) -> pd.DataFrame:
        # preserve original order; ignore missing columns
        cols = df.columns.intersection(wanted)
        logger.info(f"[_select_existing] cols:{cols}")
        return df.loc[:, cols]

    # Build phenotype frame once
    if verbose:
        logger.debug("[build_ExDEP_exclusions] Running build_phenotype_cases...")
    filtered_df = build_phenotype_cases(    
        df1=df1, exact_match=exact_match,  values_to_match=diag_df, 
        diagnostic_col=diagnostic_col, birthdatecol=birthdatecol,
        iidcol=iidcol, input_date_in_name=input_date_in_name, input_date_out_name=input_date_out_name,
        verbose=verbose, Covariates=True, Covar_Name=diag, skip_icd_update = skip_icd_update, 
        remove_point_in_diag_request = remove_point_in_diag_request, 
        ICDCM = ICDCM, noLeadingICD = noLeadingICD, icdprefix = icdprefix
    )

    logger.info(f"[build_ExDEP_exclusions] filtered_df.columns: {filtered_df.columns}\n filtered_df.head(5): {filtered_df.head(5)}")
    if verbose:
        logger.debug(f"[build_ExDEP_exclusions] filtered_df columns -> {list(filtered_df.columns)}")

    # Columns we need in all cases
    base_cols = [iidcol, "diagnoses", "in_dates", "out_dates", "first_dx"]

    logger.info(f"[build_ExDEP_exclusions] _select_existing(filtered_df, {base_cols + extra_cols_to_keep}; base_cols {base_cols} + extra_cols_to_keep {extra_cols_to_keep})")

    # Keep only what exists (and in a stable order)
    filtered_df = _select_existing(filtered_df, base_cols + extra_cols_to_keep)
    logger.info(f"[build_ExDEP_exclusions] After [_select_existing] filtered_df.columns: {filtered_df.columns}\n filtered_df.head(5): {filtered_df.head(5)}")
    
    if verbose:
        logger.debug(f"[build_ExDEP_exclusions] filtered_df.head(5):\n{filtered_df.head(5)}")
        logger.info(f"[build_ExDEP_exclusions] Identified {filtered_df.shape[0]} rows for {diag}")

    # Ensure casecontrol_df has the IID column
    if casecontrol_df is None or casecontrol_df.empty:
        if verbose:
            logger.debug("[build_ExDEP_exclusions] casecontrol_df empty  initializing from df1 IIDs")
        casecontrol_df = df1[[iidcol]].drop_duplicates().reset_index(drop=True)
    elif iidcol not in casecontrol_df.columns:
        if verbose:
            logger.debug("[build_ExDEP_exclusions] Adding IID column from df1 (outer join of IIDs)")
        casecontrol_df = (
            casecontrol_df.merge(
                df1[[iidcol]].drop_duplicates(),
                on=iidcol, how="outer"
            )
        )

    # If we found cases, rename and merge; else fill empty columns
    if not filtered_df.empty:
        # Renames
        rename_map = {
            "diagnoses": diag,
            "in_dates": f"{diag}_In_Dates",
            "out_dates": f"{diag}_Out_Dates",
            "first_dx": f"{diag}_earliest_date",
            "last_dx": f"{diag}_latest_date",
        }
        filtered_df = filtered_df.rename(columns=rename_map)
        for col in extra_cols_to_keep:
            if col in filtered_df.columns:
                filtered_df.rename(columns={col:f"{diag}_{col}"},inplace=True)
        # Merge onto casecontrol_df
        casecontrol_df = casecontrol_df.merge(filtered_df, on=iidcol, how="outer")

        # Fill empties for the main three list-like cols
        for col in [diag, f"{diag}_In_Dates", f"{diag}_Out_Dates"]:
            if col in casecontrol_df.columns:
                casecontrol_df[col] = casecontrol_df[col].where(casecontrol_df[col].notna(), "")
        logger.info(f"[build_ExDEP_exclusions] [if not filtered_df.empty] filtered_df.columns: {filtered_df.columns}\ncasecontrol_df.columns: {casecontrol_df.columns}")
    else:
        # Ensure the expected columns exist with empty defaults
        for col in [diag, f"{diag}_In_Dates", f"{diag}_Out_Dates", f"{diag}_earliest_date"]:
            if col not in casecontrol_df.columns:
                casecontrol_df[col] = "" if col != f"{diag}_earliest_date" else pd.NaT
        for col in [diag, f"{diag}_In_Dates", f"{diag}_Out_Dates", f"{diag}_latest_date"]:
            if col not in casecontrol_df.columns:
                casecontrol_df[col] = "" if col != f"{diag}_latest_date" else pd.NaT
        for col in extra_cols_to_keep:
            col=f"{diag}_{col}"
            if col not in casecontrol_df.columns:
                casecontrol_df[col] = "" if col != f"{diag}_earliest_date" else pd.NaT
        for col in extra_cols_to_keep:
            col=f"{diag}_{col}"
            if col not in casecontrol_df.columns:
                casecontrol_df[col] = "" if col != f"{diag}_latest_date" else pd.NaT
        logger.info(f"[build_ExDEP_exclusions] [if filtered_df.empty] filtered_df.columns: {filtered_df.columns}\ncasecontrol_df.columns: {casecontrol_df.columns}")
    
    return casecontrol_df


def merge_IIDs(
    tmp_result_df: pd.DataFrame,
    diagnostic_col: str,
    birthdatecol: str,
    input_date_in_name: str,
    input_date_out_name: str,
    iidcol: str,
    verbose: bool,
    Cases: bool,
    Covariates: bool = False,
    BuildEntryExitDates: bool = False,
) -> pd.DataFrame:
    """
    Merge diagnoses for each IID, computing first/last diagnosis dates,
    lists of codes and dates, and counts.

    Returns a DataFrame with one row per IID.
    """

    if verbose:
        logger.debug(
            f"[merge_IIDs] Cases={Cases}, Covariates={Covariates}, "
            f"in={input_date_in_name}, out={input_date_out_name}, "
            f"BuildEntryExitDates={BuildEntryExitDates}"
        )

    if tmp_result_df.empty:
        if verbose:
            logger.debug("[merge_IIDs] Input empty  returning empty DataFrame")
        return tmp_result_df

    # --- Step 1: Pre-select relevant columns
    cols = [iidcol, diagnostic_col, input_date_in_name, input_date_out_name]
    if birthdatecol in tmp_result_df.columns:
        cols.append(birthdatecol)
    if "pattype" in tmp_result_df.columns:
        cols.append("pattype")
    if "admissiontype" in tmp_result_df.columns:
        cols.append("admissiontype")
    if "diagtype" in tmp_result_df.columns:
        cols.append("diagtype")
    if "source" in tmp_result_df.columns:
        cols.append("source")
    if "register" in tmp_result_df.columns:
        cols.append("register")
    logger.info(f"[merge_IIDs] cols to keep:{cols}")
    cols =  list(set(cols)) 
    cols = [c for c in cols if c in tmp_result_df.columns]
    tmp_result_df = tmp_result_df[cols].copy()

    # --- Step 2: Normalize date_in/date_out column names
    def _normalize_dates(df: pd.DataFrame) -> pd.DataFrame:
        # Clean up suffixes from merges
        if "date_in_y" in df.columns:
            logger.info("[merge_IIDs] WARNING: dropping date_in_y")
            df = df.drop(columns=["date_in_y"])
        if "date_in_x" in df.columns:
            if "date_in" not in df.columns:
                logger.info("[merge_IIDs] WARNING: renaming date_in_xdate_in")
                df["date_in"] = df["date_in_x"]
            df = df.drop(columns=["date_in_x"])
        # Ensure we have date_in
        if input_date_in_name != "date_in" and "date_in" not in df.columns:
            if verbose:
                logger.debug(f"[merge_IIDs] Renaming {input_date_in_name}date_in")
            df["date_in"] = df[input_date_in_name]
            df = df.drop(columns=[input_date_in_name])
        # Ensure we have date_out
        if (
            input_date_out_name != input_date_in_name
            and input_date_out_name != "date_out"
            and "date_out" not in df.columns
        ):
            if verbose:
                logger.debug(f"[merge_IIDs] Renaming {input_date_out_name}date_out")
            df["date_out"] = df[input_date_out_name]
            df = df.drop(columns=[input_date_out_name])
        elif "date_out" not in df.columns:
            if verbose:
                logger.debug("[merge_IIDs] No date_out  duplicating date_in")
            df["date_out"] = df["date_in"]
        return df

    tmp_result_df = _normalize_dates(tmp_result_df)

    # --- Step 3: Group by IID and compute aggregations
    grouped = tmp_result_df.groupby(iidcol, group_keys=False)
    first_dates = grouped["date_in"].min().rename("first_dx").reset_index()
    last_dates = grouped["date_out"].max().rename("last_dx").reset_index()

    merges = [first_dates, last_dates]

    if Cases or Covariates:
        dx_list = grouped[diagnostic_col].apply(list).rename("diagnoses").reset_index()
        merges.append(dx_list)
    in_dates = grouped["date_in"].apply(list).rename("in_dates").reset_index()
    out_dates = grouped["date_out"].apply(list).rename("out_dates").reset_index()
    merges.extend([in_dates, out_dates])

    if "pattype" in tmp_result_df.columns:
        types = grouped["pattype"].apply(list).reset_index()
    if "admissiontype" in tmp_result_df.columns:
        types = grouped["admissiontype"].apply(list).reset_index()
    if "source" in tmp_result_df.columns:
        sources = grouped["source"].apply(list).reset_index()
        merges.append(sources)
    if "diagtype" in tmp_result_df.columns:
        #types = grouped["diagtype"].apply(list).rename("c_diagtype").reset_index()
        types = grouped["diagtype"].apply(list).reset_index()
        merges.append(types)
    if "register" in tmp_result_df.columns:
        types = grouped["register"].apply(list).reset_index()
        merges.append(types)

    if Cases:
        n_diags = grouped["date_in"].count().rename("n_diags").reset_index()
        n_unique = grouped["date_in"].nunique().rename("n_unique_in_days").reset_index()
        merges.extend([n_diags, n_unique])

    # --- Step 4: Merge all summaries
    id_dx_date_df = merges[0]
    for m in merges[1:]:
        id_dx_date_df = id_dx_date_df.merge(m, on=iidcol, how="outer")
    logger.info(f"[merge_IIDs] id_dx_date_df.head(5): {id_dx_date_df.head(5)}/n id_dx_date_df.columns: {id_dx_date_df.columns}")
    # --- Step 5: Add Case/Control flag
    tmp_result_df = tmp_result_df[[iidcol]].drop_duplicates()
    tmp_result_df["diagnosis"] = "Case" if Cases else "Control"

    # Merge back
    dx_result_df = tmp_result_df.merge(id_dx_date_df, on=iidcol, how="left").drop_duplicates(subset=[iidcol])

    if verbose:
        logger.debug(f"[merge_IIDs] Output shape={dx_result_df.shape}, dup IIDs? {dx_result_df[iidcol].duplicated().any()}")

    # --- Step 6: Optionally merge entry/exit
    if BuildEntryExitDates:
        logger.info("[merge_IIDs] WARNING: BuildEntryExitDates not fully implemented here")
        # dx_result_df = dx_result_df.merge(Entry_Exit_date_df, on=iidcol)

    return dx_result_df

def expand_ranges_old(code_list):
        expanded = []
        for code in code_list:
            code = code.strip()
            if '-' in code:
                # Example: T36-T50
                match = re.match(r"^([A-Z]+)(\d+)-\1(\d+)$", code)
                if match:
                    prefix, start, end = match.groups()
                    expanded += [f"{prefix}{i}" for i in range(int(start), int(end) + 1)]
                else:
                    # No prefix? Try generic numeric (e.g., 36-50)
                    match = re.match(r"^(\d+)-(\d+)$", code)
                    if match:
                        start, end = match.groups()
                        expanded += [str(i) for i in range(int(start), int(end)+1)]
                    else:
                        # Can't parse, keep original
                        expanded.append(code)
            else:
                expanded.append(code)
        return expanded

def expand_ranges(code_list):
    """
    Expand shorthand ranges in a list (or single string) of code tokens.

    Handles:
      - "T36-T38"                 -> ["T36","T37","T38"]
      - "36-38"                   -> ["36","37","38"]
      - "ICD10:T36-ICD10:T38"     -> ["ICD10:T36","ICD10:T37","ICD10:T38"]
      - "ICD10:T36-T38"           -> ["ICD10:T36","ICD10:T37","ICD10:T38"]
    Accepts either a list/Series of tokens or a single comma-separated string.
    Falls back to returning the original token when it cannot be parsed.
    """
    expanded: List[str] = []

    def _parse_side(s: str):
        s = s.strip()
        if not s:
            return None
        pref = ""
        rest = s
        if ":" in s:
            parts = s.split(":", 1)
            pref = parts[0].strip() + ":"
            rest = parts[1].strip()
        # allow letters part that can be 1-2 letters (e.g., T or DF) and numeric (with optional decimal)
        m = re.match(r"^([A-Za-z]{1,2})(\d+)(?:\.(\d+))?$", rest)
        if m:
            letters, num, _ = m.groups()
            return pref, letters, int(num)
        m2 = re.match(r"^(\d+)$", rest)
        if m2:
            return pref, "", int(m2.group(1))
        return None

    # Normalize input: if single string that contains commas, split on commas first
    if isinstance(code_list, str):
        if "," in code_list:
            tokens = [t.strip() for t in code_list.split(",") if t.strip() != ""]
        else:
            tokens = [code_list]
    elif code_list is None:
        return []
    else:
        try:
            tokens = list(code_list)
        except Exception:
            tokens = [code_list]

    for token in tokens:
        if token is None:
            continue
        code = str(token).strip()
        if not code:
            continue

        # If there's no dash, keep as-is
        if "-" not in code:
            expanded.append(code)
            continue

        # split only on the first dash to allow colons on the right side
        left_raw, right_raw = map(str.strip, code.split("-", 1))

        # Pure numeric range: "36-38"
        if left_raw.isdigit() and right_raw.isdigit():
            start, end = int(left_raw), int(right_raw)
            if start <= end:
                expanded.extend([str(i) for i in range(start, end + 1)])
            else:
                expanded.append(code)
            continue

        # Try parsing both sides structurally
        lp = _parse_side(left_raw)
        rp = _parse_side(right_raw)
        if lp and rp:
            lpref, lletters, lnum = lp
            rpref, rletters, rnum = rp

            # If letter part matches (or both empty numeric), build range
            if lletters == rletters:
                if lnum <= rnum:
                    use_pref = lpref or rpref or ""
                    if lletters:
                        expanded += [f"{use_pref}{lletters}{i}" for i in range(lnum, rnum + 1)]
                    else:
                        expanded += [str(i) for i in range(lnum, rnum + 1)]
                    continue
                else:
                    expanded.append(code)
                    continue
            else:
                # Different letter parts (rare) -> cannot safely expand -> keep original
                expanded.append(code)
                continue

        # Back-compat simple pattern "T36-T38" (no colon)
        m = re.match(r"^([A-Za-z]{1,2})(\d+)-\1(\d+)$", code.replace(" ", ""))
        if m:
            letters, start_s, end_s = m.groups()
            s_i, e_i = int(start_s), int(end_s)
            if s_i <= e_i:
                expanded += [f"{letters}{i}" for i in range(s_i, e_i + 1)]
                continue

        # Generic numeric fallback
        mnum = re.match(r"^(\d+)-(\d+)$", code)
        if mnum:
            s_i, e_i = map(int, mnum.groups())
            if s_i <= e_i:
                expanded += [str(i) for i in range(s_i, e_i + 1)]
                continue

        # Unparseable -> keep as-is
        expanded.append(code)

    return expanded

def expand_ranges_(code_list):
    """
    Expand shorthand ranges in a list (or single string) of code tokens.
    Supported forms:
      - "T36-T38"                 -> ["T36","T37","T38"]
      - "36-38"                   -> ["36","37","38"]
      - "ICD10:T36-ICD10:T38"     -> ["ICD10:T36","ICD10:T37","ICD10:T38"]
      - "ICD10:T36-T38"           -> ["ICD10:T36","ICD10:T37","ICD10:T38"]
    Falls back to returning the original token when it cannot be parsed.
    """
    expanded: List[str] = []
    def _parse_side(s: str):
        s = s.strip()
        if not s:
            return None
        pref = ""
        rest = s
        if ":" in s:
            parts = s.split(":", 1)
            pref = parts[0].strip() + ":"
            rest = parts[1].strip()
        m = re.match(r"^([A-Za-z]+)(\d+)(?:\.(\d+))?$", rest)
        if m:
            letters, num, _ = m.groups()
            return pref, letters, int(num)
        m2 = re.match(r"^(\d+)$", rest)
        if m2:
            return pref, "", int(m2.group(1))
        return None
    # Accept a single string or any iterable (list/tuple/Series). Avoid iterating characters for a plain string.
    if isinstance(code_list, str):
        tokens = [code_list]
    elif code_list is None:
        return []
    else:
        try:
            tokens = list(code_list)
        except Exception:
            tokens = [code_list]
    for token in tokens:
        if token is None:
            continue
        code = str(token).strip()
        if not code:
            continue
        # If there's no dash, keep as-is
        if "-" not in code:
            expanded.append(code)
            continue
        # split only on the first dash to allow colons on the right side
        left_raw, right_raw = map(str.strip, code.split("-", 1))
        # Pure numeric range: "36-38"
        if left_raw.isdigit() and right_raw.isdigit():
            start, end = int(left_raw), int(right_raw)
            if start <= end:
                expanded.extend([str(i) for i in range(start, end + 1)])
            else:
                expanded.append(code)
            continue
        # Try parsing both sides
        lp = _parse_side(left_raw)
        rp = _parse_side(right_raw)
        if lp and rp:
            lpref, lletters, lnum = lp
            rpref, rletters, rnum = rp
            # If letter part matches (or both empty numeric), build range
            if lletters == rletters:
                if lnum <= rnum:
                    use_pref = lpref or rpref or ""
                    if lletters:
                        expanded.extend([f"{use_pref}{lletters}{i}" for i in range(lnum, rnum + 1)])
                    else:
                        expanded.extend([str(i) for i in range(lnum, rnum + 1)])
                else:
                    expanded.append(code)
                continue
            else:
                # Different letter parts (rare) -> fallback to attempt using left letter
                if lnum <= rnum and lletters:
                    use_pref = lpref or rpref or ""
                    expanded.extend([f"{use_pref}{lletters}{i}" for i in range(lnum, rnum + 1)])
                    continue
        # Back-compat simple pattern "T36-T38" (no colon)
        m = re.match(r"^([A-Za-z]+)(\d+)-\1(\d+)$", code.replace(" ", ""))
        if m:
            letters, start_s, end_s = m.groups()
            s_i, e_i = int(start_s), int(end_s)
            if s_i <= e_i:
                expanded.extend([f"{letters}{i}" for i in range(s_i, e_i + 1)])
                continue
        # Generic numeric fallback
        mnum = re.match(r"^(\d+)-(\d+)$", code)
        if mnum:
            s_i, e_i = map(int, mnum.groups())
            if s_i <= e_i:
                expanded.extend([str(i) for i in range(s_i, e_i + 1)])
                continue
        # Unparseable -> keep as-is
        expanded.append(code)
    return expanded
# covariates = pd.DataFrame(columns=['Disorder','Disorder Codes'])
# covariates.loc[0] = ["Suicide_Attempt", SuicideAttempt_Codes]
# covariates.loc[1] = ["Suicide", SuicideCompleted_Codes]
# covariates2, normalized_covars = dict_update_icd_coding(covariates, exact_match, skip_icd_update, remove_point_in_diag_request, ICDCM, noLeadingICD, icdprefix)


def parse_pheno_rules(rule_str,exact_match=False,skip_icd_update=False,remove_point_in_diag_request=False,ICDCM=False,noLeadingICD=False,icdprefix=""):
    """
    Parses a rule string like:
      'main=F33;sub=T36-T50,T52-T60;rule_out=S59'
    into a dictionary with expanded code lists.
    """
    rule = {
        'main': [], 'sub': [], 'rule_out': [],
        'main_exact': [], 'sub_exact': [], 'rule_out_exact': [], 'ranges':[]
    }
    logger.info(f"[parse_pheno_rules] trying to split {rule_str}")
    if(";" in rule_str):
        for part in rule_str.split(';'):
            if '=' not in part:
                continue
            key, val = part.split('=')
            key = key.strip()
            val_list = [v.strip() for v in val.split(',') if v.strip()]
            #rule[key] = expand_ranges(val_list)
            updated = update_icd_coding(
                            data=expand_ranges(val_list),
                            eM=exact_match,
                            skip=skip_icd_update,
                            remove_point_in_diag_request=remove_point_in_diag_request,
                            ICDCM=ICDCM,
                            noLeadingICD=noLeadingICD,
                            icdprefix=icdprefix,
                        )
            logger.info(f"[parse_pheno_rules] in if; {rule_str} into val_list {val_list} and updated to {updated}")
            rule[key] = updated
    else:
        val_list = [v.strip() for v in rule_str.split(',') if v.strip()]
        updated = update_icd_coding(
                            data=expand_ranges(val_list),
                            eM=exact_match,
                            skip=skip_icd_update,
                            remove_point_in_diag_request=remove_point_in_diag_request,
                            ICDCM=ICDCM,
                            noLeadingICD=noLeadingICD,
                            icdprefix=icdprefix,
                        )
        logger.info(f"[parse_pheno_rules] in else; {rule_str} into val_list {val_list} and updated to {updated}")
        rule['ranges'] = updated
    print(f"[parse_pheno_rules] Successfully split {rule_str} into {rule}")
    logger.info(f"[parse_pheno_rules] Successfully split {rule_str} into {rule}")
    return rule

def detect_advanced_cases(df, rule_dict, input_date_in_name, exact_match, diagcol, iidcol):
    main_df = df[df['type'].isin(['A', 'G', 'C'])]
    sub_df = df[df['type'].isin(['+', 'B'])]
    all_df = df.copy()
    ruleout_ids = set()

    # --- Collect rule_out IIDs
    if rule_dict.get('rule_out'):
        ruleout_ids |= set(all_df[match_codes(all_df[diagcol], rule_dict['rule_out'])][iidcol].unique())
    if rule_dict.get('rule_out_exact'):
        ruleout_ids |= set(all_df[match_codes(all_df[diagcol], rule_dict['rule_out_exact'], exact=True)][iidcol].unique())
    if rule_dict.get('ranges'):
        ranges_ids |= set(all_df[match_codes(all_df[diagcol], rule_dict['ranges'], exact=exact_match)][iidcol].unique())

    # --- Parse main values and detect which require +sub
    raw_main = rule_dict.get('main', [])
    main_exact = rule_dict.get('main_exact', [])

    # Split into +sub-required and normal
    mains_with_sub = [m.replace('+sub','') for m in raw_main if m.endswith('+sub')]
    mains_normal   = [m for m in raw_main if not m.endswith('+sub')]

    matched_frames = []

    # --- Handle mains that require +sub
    for m in mains_with_sub:
        main_match = map_cases(values_to_match=m, exact_match=exact_match, df1=main_df, diagcol=diagcol, cols=None)
        sub_match = map_cases(values_to_match=rule_dict.get('sub', []), exact_match=exact_match, df1=sub_df, diagcol=diagcol, cols=None)
        #main_match = get_matches(main_df, [m])
        #sub_match = get_matches(sub_df, rule_dict.get('sub', []))
        if not main_match.empty and not sub_match.empty:
            both = pd.merge(main_match, sub_match, on=[iidcol, input_date_in_name])
            cols_to_drop = list(both.columns)
            for col in cols_to_drop:
                if col.endswith("_x"):
                    both.drop(columns=[col], inplace = True)
                    new_col_name = col.split("_x")[0]
                    both.rename(columns={new_col_name+"_y": new_col_name}, inplace=True)
                matched_frames.append(both)

    # --- Handle mains without +sub (union logic)
    if mains_normal or main_exact:
        main_match = map_cases(values_to_match=mains_normal, exact_match=exact_match, df1=main_df, diagcol=diagcol, cols=None)
        #main_match = get_matches(main_df, mains_normal)        
        if main_exact:
            main_match = pd.concat([main_match, map_cases(values_to_match=main_exact, exact_match=exact_match, df1=main_df, diagcol=diagcol, cols=None)])
            #main_match = pd.concat([main_match, get_matches(main_df, main_exact, exact=True)])
        sub_match = map_cases(values_to_match=rule_dict.get('sub', []), exact_match=exact_match, df1=sub_df, diagcol=diagcol, cols=None)
        #sub_match = get_matches(sub_df, rule_dict.get('sub', []))
        combined = pd.concat([main_match, sub_match]).drop_duplicates()
        matched_frames.append(combined)

    if ranges_ids:
        matched_frames.append(ranges_ids)

    # --- Merge all
    if matched_frames:
        matched = pd.concat(matched_frames).drop_duplicates()
    else:
        matched = pd.DataFrame(columns=['IID', input_date_in_name, 'diagnosis'])

    # --- Apply rule_out
    matched = matched[~matched['IID'].isin(ruleout_ids)]

    return matched

def advanced_map_cases(values_to_match, df1, exact_match, diagnostic_col, input_date_in_name, iidcol, skip_icd_update = False, remove_point_in_diag_request = False, ICDCM = False, noLeadingICD = False, icdprefix = False):
    # Handle advanced logic if detected
    if isinstance(values_to_match, str) and 'main=' in values_to_match:
        # Advanced rule handling
        rule_dict = dict_update_icd_coding(curr_codes = values_to_match,
                           exact_match = exact_match,
                           skip_icd_update = skip_icd_update,
                           remove_point_in_diag_request = remove_point_in_diag_request,
                           ICDCM = ICDCM,
                           noLeadingICD = noLeadingICD,
                           icdprefix = icdprefix)
        #rule_dict = parse_pheno_rules(values_to_match)
        tmp_result_df = detect_advanced_cases(df=df1, rule_dict=rule_dict, input_date_in_name=input_date_in_name, exact_match=exact_match, diagcol=diagnostic_col, iidcol=iidcol)
        #tmp_result_df = detect_advanced_cases(df1, rule_dict)
        tmp_result_df['diagnosis'] = '[Advanced Rule]'
    else:
        tmp_result_df = map_cases(values_to_match=values_to_match, 
                                exact_match=exact_match, 
                                df1=df1, 
                                diagcol=diagnostic_col)
    return(tmp_result_df)

def map_cases(values_to_match, exact_match, df1, diagcol, cols=None):
        exact_set, prefixes = set(), []
        out = pd.DataFrame()
        if values_to_match is not None:
            for d in map(str, values_to_match):
                d = d.strip()
                if not exact_match or d.endswith("*"):
                    prefixes.append(d[:-1] if d.endswith("*") else d)
                else:
                    exact_set.add(d)

            # DIAG mask
            s = df1[diagcol].astype("string").str.strip()

            if values_to_match is None:
                diag_mask = pd.Series(True, index=df1.index)
            else:
                dmask = pd.Series(False, index=df1.index)
                if exact_set:
                    dmask |= s.isin(exact_set)
                if prefixes:
                    dmask |= s.str.startswith(tuple(prefixes), na=False)
                diag_mask = dmask
            if diag_mask.any():
                out = (df1.loc[diag_mask, cols if cols is not None else df1.columns])
            return out
        logger.info(f"[map_cases] ERROR: No values_to_match supplied: {values_to_match}")
        if not verbose:
            print(f"[map_cases] ERROR: No values_to_match supplied: {values_to_match}")


def build_phenotype_cases(
    df1,
    exact_match: bool,
    values_to_match,
    diagnostic_col,
    birthdatecol: str,
    iidcol: str,
    input_date_in_name: str,
    input_date_out_name: str,
    verbose: bool,
    Covariates: bool = False,
    Covar_Name: str = "",
    general_results: pd.DataFrame = pd.DataFrame(),
    BuildEntryExitDates: bool = False,
    skip_icd_update: bool = False, 
    remove_point_in_diag_request: bool = False, 
    ICDCM: bool = False, 
    noLeadingICD: bool = False, 
    icdprefix: str = "",
) -> pd.DataFrame:
    """
    Build phenotype cases or covariates from a diagnostic dataframe.

    Args:
        df1: DataFrame or [df_noatc, df_atc]
        exact_match: whether to require exact matches
        values_to_match: list of codes or [codes_noatc, codes_atc]
        diagnostic_col: column name or [diagcol_noatc, diagcol_atc]
        birthdatecol: column containing birthdates
        iidcol: individual ID column
        input_date_in_name: column with entry date
        input_date_out_name: column with exit date
        verbose: print debugging
        Covariates: build covariates instead of case definitions
        Covar_Name: name of covariate
        general_results: DataFrame to merge with (for covariates)
        BuildEntryExitDates: build entry/exit dates

    Returns:
        DataFrame with phenotype cases or covariates.
    """

    # --- Step 1: Build tmp_result_df ---
    if isinstance(df1, list):
        # Expect 2 dfs, 2 values_to_match, 2 diagnostic cols, 2 date cols
        if not (isinstance(values_to_match, list) and isinstance(diagnostic_col, list) and isinstance(input_date_in_name, list)):
            raise ValueError("[build_phenotype_cases] When df1 is a list, you must supply values_to_match, diagnostic_col, and input_date_in_name as lists.")
        df_noatc, df_atc = df1
        vals_noatc, vals_atc = values_to_match
        diag_noatc, diag_atc = diagnostic_col
        date_noatc, date_atc = input_date_in_name
        tmp_result_df = advanced_map_cases(values_to_match=vals_noatc, 
                                exact_match=exact_match, 
                                df1=df_noatc, 
                                diagnostic_col=diag_noatc,
                                input_date_in_name=date_noatc,
                                iidcol=iidcol, skip_icd_update = skip_icd_update, 
                                remove_point_in_diag_request = remove_point_in_diag_request, 
                                ICDCM = ICDCM, noLeadingICD = noLeadingICD, icdprefix = icdprefix)
        # tmp_result_df = map_cases(values_to_match=vals_noatc, 
        #                           exact_match=exact_match, df1=df_noatc, 
        #                           diagcol=diag_noatc)
        tmp_result_df_atc = advanced_map_cases(values_to_match=vals_atc, 
                                exact_match=exact_match, 
                                df1=df_atc, 
                                diagnostic_col=diag_atc,
                                input_date_in_name=date_atc,
                                iidcol=iidcol, skip_icd_update = skip_icd_update, 
                                remove_point_in_diag_request = remove_point_in_diag_request, 
                                ICDCM = ICDCM, noLeadingICD = noLeadingICD, icdprefix = icdprefix)
        # tmp_result_df_atc = map_cases(values_to_match=vals_atc, 
        #                               exact_match=exact_match, df1=df_atc, 
        #                               diagcol=diag_atc)
        

        # harmonize column names between ATC/noATC dfs
        #tmp_result_df_atc.rename(columns={diag_noatc: diag_atc, date_noatc: date_atc}, inplace=True)
        tmp_result_df_atc.rename(columns={diag_atc: diag_noatc, date_atc: date_noatc}, inplace=True)
        tmp_result_df = pd.concat([tmp_result_df, tmp_result_df_atc], ignore_index=True, sort=False)

        diagnostic_col = diag_noatc
        input_date_in_name = date_noatc
        if type(input_date_out_name) == list:
            input_date_out_name = input_date_out_name[0]
    else:
        # Single df
        tmp_result_df = advanced_map_cases(values_to_match=values_to_match, 
                                exact_match=exact_match, 
                                df1=df1, 
                                diagnostic_col=diagnostic_col,
                                input_date_in_name=input_date_in_name,
                                iidcol=iidcol, skip_icd_update = skip_icd_update, 
                                remove_point_in_diag_request = remove_point_in_diag_request, 
                                ICDCM = ICDCM, noLeadingICD = noLeadingICD, icdprefix = icdprefix)
        # tmp_result_df = map_cases(values_to_match=values_to_match, 
        #                           exact_match=exact_match, 
        #                           df1=df1, 
        #                           diagcol=diagnostic_col)

    if verbose:
        logger.debug(f"[build_phenotype_cases] columns: {list(tmp_result_df.columns)}")
        logger.debug(f"[build_phenotype_cases] Using {input_date_in_name}, {iidcol}, {diagnostic_col}")
    logger.info(f"[build_phenotype_cases] tmp_result_df.head(5): {tmp_result_df.head(5)}")
    logger.info(f"[build_phenotype_cases] tmp_result_df.columns: {tmp_result_df.columns}")
    logger.info(f"[build_phenotype_cases] Covariates: {Covariates}")
    # --- Step 2: Sanity checks ---
    if tmp_result_df.empty:
        if not Covariates:
            logger.info("[build_phenotype_cases] ERROR: No cases found")
        return pd.DataFrame()

    if not Covariates:
        logger.info(f"[build_phenotype_cases] Identified {tmp_result_df[iidcol].nunique()} IIDs with overlapping diagnostic code(s).")

    # --- Step 3: Convert date columns ---
    def _convert_series(s):
        return s.apply(lambda x: convert_if_not_datetime(x) if pd.notnull(x) else pd.NaT)

    for col in [input_date_in_name, input_date_out_name, birthdatecol]:
        if isinstance(col, list):
            for c in col:
                logger.info(f"[build_phenotype_cases] running _convert_series with col: {c}; on tmp_result_df.columns: {tmp_result_df.columns}")
                if c in tmp_result_df.columns:
                    logger.info(f"[build_phenotype_cases] starting _convert_series with col: {c}, on tmp_result_df[{c}]: {tmp_result_df[c]}")
                    tmp_result_df[c] = _convert_series(tmp_result_df[c])
        else:
            logger.info(f"[build_phenotype_cases] running _convert_series with col: {col}; on tmp_result_df.columns: {tmp_result_df.columns}")
            if col in tmp_result_df.columns:
                logger.info(f"[build_phenotype_cases] starting _convert_series with col: {col}, on tmp_result_df[{col}]: {tmp_result_df[col]}")
                tmp_result_df[col] = _convert_series(tmp_result_df[col])   
        # logger.info(f"[build_phenotype_cases] running _convert_series with col: {col}; on tmp_result_df.columns: {tmp_result_df.columns} ")
        # if col in tmp_result_df.columns:
        #     logger.info(f"[build_phenotype_cases] starting _convert_series with col: {col}, on tmp_result_df[col]: {tmp_result_df[col]}")
        #     tmp_result_df[col] = _convert_series(tmp_result_df[col])

    # --- Step 4: Merge cases or covariates ---
    merged = merge_IIDs(
        tmp_result_df,
        diagnostic_col,
        birthdatecol,
        input_date_in_name,
        input_date_out_name,
        iidcol,
        verbose,
        Cases=True,
        Covariates=Covariates,
        BuildEntryExitDates=BuildEntryExitDates,
    )
    logger.info(f"[build_phenotype_cases] merged.head(5): {merged.head(5)}\n merged.columns: {merged.columns:}")
    if not Covariates:
        logger.info(f"[build_phenotype_cases] Returning merged dataframe as it is for the MAIN PHENOTYPE and not a COVARIATE.")
        return merged
    else:
        logger.info(f"[build_phenotype_cases] Working on COVARIATE formatting.")
    # --- Step 5: Covariates-specific cleanup ---
    # Keep only iidcol + core columns
    keep_cols = {
        iidcol, "diagnosis", "diagnoses", "in_dates", "out_dates",
        "first_dx", "last_dx", "n_diags", "n_unique_in_days"
    }
    if "pattype" in merged.columns:
        keep_cols.add("pattype")
    if "admissiontype" in merged.columns:
        keep_cols.add("admissiontype")
    if "diagtype" in merged.columns:
        keep_cols.add("diagtype")
    if "source" in merged.columns:
        keep_cols.add("source")
    if "register" in merged.columns:
        keep_cols.add("register")
    logger.info(f"[build_phenotype_cases] keep_cols: {keep_cols}")
    covar = merged.drop(columns=[c for c in merged.columns if c not in keep_cols]).copy()
    logger.info(f"[build_phenotype_cases] covar.columns: {covar.columns}")

    if verbose:
        logger.debug(f"[build_phenotype_cases] Covariates DF for {Covar_Name} has columns: {list(covar.columns)}")
        logger.debug(f"[build_phenotype_cases] Duplicated IIDs? {covar[iidcol].duplicated().any()}")
        logger.debug(f"[build_phenotype_cases] Duplicated Columns? {covar.coumns.duplicated().any()}")

    covar = covar.drop_duplicates(subset=[iidcol])

    if not general_results.empty:
        if verbose:
            logger.debug("[build_phenotype_cases] Merging with general_results...")
        general_results = general_results.loc[:, ~general_results.columns.duplicated()]
        covar = covar.loc[:, ~covar.columns.duplicated()]
        covar = pd.merge(general_results, covar, on=iidcol)
        covar = covar.drop_duplicates(subset=[iidcol])

    if verbose:
        logger.debug(f"[build_phenotype_cases] Identified {covar.shape[0]} {Covar_Name} Covariate cases.")
    
    logger.info(f"[build_phenotype_cases] covar.columns: {covar.columns}")

    return covar

def columns_for_disorders(final_df, iidcol, disorder_dfs, base_cols=None):
    """
    disorder_dfs: iterable of DataFrames that each have a 'Disorder' column
    base_cols: columns you always want to keep (besides iidcol)
    """
    if base_cols is None:
        base_cols = ['diagnosis','diagnoses','first_dx','in_dates','birthdate','Age_FirstDx']

    # ordered unique list of disorder names in the order they appear
    disorders = []
    for df in disorder_dfs:
        if df is None or 'Disorder' not in df.columns:
            continue
        for d in df['Disorder']:
            if pd.notna(d) and str(d) not in disorders:
                disorders.append(str(d))

    # expand each disorder to the 3 related columns
    expanded = []
    for d in disorders:
        expanded.extend([d, f"{d}_In_Dates", f"{d}_Out_Dates"])

    # build final column list (preserve order, drop duplicates)
    wanted = [iidcol, *base_cols, *expanded]
    wanted = list(OrderedDict.fromkeys(wanted))  # remove dups, keep order

    # keep only columns that exist in final_df
    cols_present = [c for c in wanted if c in final_df.columns]
    return cols_present

def Sankey_build_processor(row, sankey_labels = ["MDD","Age","ID","SCZ","BPD","DEM","AUD","DUD","MCI","CTI","ClinPlausMDD"], main_diag="MDD", lifetime_excl = ["ID", "SCZ", "BPD"]):
    # Split the comma-separated dates in diag_date and convert them to datetime objects
    temp_exc_diag_date = row['temp_exc_diag_date']
    # Convert exc_diag string to a list
    temp_exc_diag = row['temp_exc_diag']
    previous_exc_diag = ""
    if row['Level2_AgeExclusion'] == "TRUE":
        row['Level3_Sankey'] = [main_diag,"Age"]
        row['Level3_Sankey_data'] = [0,1]
        row['Level3_Sankey_source'] = [0]
        row['Level3_Sankey_target'] = [1]
        row['Level3_Sankey_value'] = [1]
        # Return the updated row
        row['temp_exc_diag_date'] = []
        row['temp_exc_diag'] = []
    elif len(row['diagnoses_Level2_modifier']) == 0:
        row['Level3_Sankey'] = [main_diag]
        row['Level3_Sankey_data'] = [0]
        row['Level3_Sankey_source'] = [0]
        row['Level3_Sankey_target'] = [len(sankey_labels)-1]
        row['Level3_Sankey_value'] = [1]
        # Return the updated row
        row['temp_exc_diag_date'] = []
        row['temp_exc_diag'] = []
    else:
        while temp_exc_diag:
            if row['Level3_Sankey'] == [main_diag]:
                previous_exc_diag = 0
            # Get the earliest point of any of the exclusion diagnostic criteria
            currently_earliest_exc_diag_date = min(temp_exc_diag_date) if temp_exc_diag_date else pd.NaT
            current_exc_diag_date_position = [i for i, date in enumerate(temp_exc_diag_date) if date == currently_earliest_exc_diag_date]
            # # Get the latest point of any of the exclusion diagnostic criteria
            # currently_latest_exc_diag_date = max(temp_exc_diag_date) if temp_exc_diag_date else pd.NaT
            # current_exc_diag_date_latest_position = [i for i, date in enumerate(temp_exc_diag_date) if date == currently_latest_exc_diag_date]
            # Get those positions to keep
            current_exc_diag_date_position_to_keep = [i for i in range(len(temp_exc_diag_date)) if i not in current_exc_diag_date_position]
            # current_exc_diag_date_latest_position_to_keep = [i for i in range(len(temp_exc_diag_date)) if i not in current_exc_diag_date_latest_position]
            # Get the current exclusion diagnosis and the date
            currently_earliest_exc_diag = [temp_exc_diag[i] for i in current_exc_diag_date_position if temp_exc_diag[i] != ""]
            is_lifetime_exclusion = False
            if temp_exc_diag in lifetime_excl:
                is_lifetime_exclusion = True
            if (len(currently_earliest_exc_diag) > 0):
                # Identify which disorder this code refers to and add it to a list
                currently_earliest_exc_disorder = [row['disorder_Level2_modifier'][i] for i in current_exc_diag_date_position if temp_exc_diag[i] != ""]
                currently_earliest_exc_disorder_numeric = next((i for i, date in enumerate(sankey_labels) if date == currently_earliest_exc_disorder[0]), None)
                #lifetime 2, 3, 4; post 5, 9
                if currently_earliest_exc_disorder == 5 or currently_earliest_exc_disorder == 9:
                     is_lifetime_exclusion = True
                if currently_earliest_exc_disorder:
                    row['Level3_Sankey'] = row['Level3_Sankey'] + currently_earliest_exc_disorder
                    row['Level3_Sankey_data'].append(previous_exc_diag) 
                    row['Level3_Sankey_data'].append(currently_earliest_exc_disorder_numeric) 
                    row['Level3_Sankey_source'].append(previous_exc_diag) 
                    row['Level3_Sankey_target'].append(currently_earliest_exc_disorder_numeric) 
                    row['Level3_Sankey_value'].append(1) 
                    previous_exc_diag = currently_earliest_exc_disorder_numeric
            # Update temp_exc_diag and temp_exc_diag_date
            temp_exc_diag = [temp_exc_diag[i] for i in current_exc_diag_date_position_to_keep]
            temp_exc_diag_date = [temp_exc_diag_date[i] for i in current_exc_diag_date_position_to_keep]
            if is_lifetime_exclusion:
                temp_exc_diag = []
                temp_exc_diag_date = []
        # Return the updated row
        row['temp_exc_diag_date'] = temp_exc_diag_date
        row['temp_exc_diag'] = temp_exc_diag
    return row

def Build_sankey_data(data, fn, iidcol, verbose=False):
    data['Level3_Sankey'] = [[] for _ in range(len(data))]
    data['Level3_Sankey_data'] = [[] for _ in range(len(data))]
    data['Level3_Sankey_source'] = [[] for _ in range(len(data))]
    data['Level3_Sankey_target'] = [[] for _ in range(len(data))]
    data['Level3_Sankey_value'] = [[] for _ in range(len(data))]
    if verbose:
        logger.debug(f"data: {data.columns}")
    data.loc[data['diagnosis'] == "Case",'Level3_Sankey'] = data.loc[data['diagnosis'] == "Case",'Level3_Sankey'].apply(lambda x: x + ["MDD"])
    # Split the comma-separated dates in diag_date and convert them to datetime objects
    data['temp_exc_diag_date'] = data['date_Level2_modifier'].copy() #apply(lambda x: [pd.to_datetime(date) for date in x.split(',') if date.strip() != ''])
    # Convert exc_diag string to a list
    data['temp_exc_diag'] = data['diagnoses_Level2_modifier'].copy() #apply(lambda x: x.split(','))
    while data.apply(lambda row: len(row['temp_exc_diag']) > 0, axis=1).any():
        ### Build the lists of covar diagnostic codes in general and use this then in the phenotype generating step and here.
        data = data.apply(Sankey_build_processor, axis=1)
        ##### Assuming these are called AUD, DUD ...
    data.loc[data['Level3_CaseControl'] == "Case", 'Level3_Sankey'] = data.loc[data['Level3_CaseControl'] == "Case"].apply(lambda row: row['Level3_Sankey'] + ["ClinPlausMDD"], axis=1)
    data['Level3_Sankey'] = data['Level3_Sankey'].apply(remove_duplicates_preserve_order)
    if verbose:
        logger.debug(f"[Build_sankey_data] Sankey Target: {data.loc[(data['diagnosis'] == 'Case') & (data['Level3_Sankey_target'].apply(len) == 0),[iidcol,'temp_exc_diag']]}")
        logger.debug(f"[Build_sankey_data] data.loc[(data[\'diagnosis\'] == \"Case\") & (data[\'diagnoses_Level2_modifier\'].apply(len) != 0),]: {data.loc[(data['diagnosis'] == 'Case') & (data['diagnoses_Level2_modifier'].apply(len) != 0),]}")
        logger.debug(f"[Build_sankey_data] Level3_Sankey_source: {data.loc[(data['diagnosis'] == 'Case') & (data['diagnoses_Level2_modifier'].apply(len) != 0), 'Level3_Sankey_source']}")
    data.loc[(data['diagnosis'] == "Case") & (data['diagnoses_Level2_modifier'].apply(len) != 0), 'Level3_Sankey_source'] = data.loc[(data['diagnosis'] == "Case")  & (data['diagnoses_Level2_modifier'].apply(len) != 0)].apply(lambda row: row['Level3_Sankey_source'] + [row['Level3_Sankey_target'][-1] if row['Level3_Sankey_target'] else -1], axis=1)
    data.loc[(data['diagnosis'] == "Case") & (data['diagnoses_Level2_modifier'].apply(len) != 0), 'Level3_Sankey_target'] = data.loc[(data['diagnosis'] == "Case")  & (data['diagnoses_Level2_modifier'].apply(len) != 0)].apply(lambda row: row['Level3_Sankey_target'] + [10], axis=1)
    data.loc[(data['diagnosis'] == "Case") & (data['diagnoses_Level2_modifier'].apply(len) != 0), 'Level3_Sankey_value'] = data.loc[(data['diagnosis'] == "Case")  & (data['diagnoses_Level2_modifier'].apply(len) != 0)].apply(lambda row: row['Level3_Sankey_value'] + [1], axis=1)
    data['Level3_Sankey_source'] = data['Level3_Sankey_source'].apply(remove_duplicates_preserve_order)
    data['Level3_Sankey_target'] = data['Level3_Sankey_target'].apply(remove_duplicates_preserve_order)
    data['Level3_Sankey_value'] = data['Level3_Sankey_value'].apply(remove_duplicates_preserve_order)
    if verbose:
        logger.debug(f"[Build_sankey_data] {data.loc[data['diagnosis'] == 'Case',['Level3_Sankey']]}")
        logger.debug(f"[Build_sankey_data] {data.loc[data['diagnosis'] == 'Case',['Level3_Sankey_data']]}")
        logger.debug(f"[Build_sankey_data] {data.loc[data['diagnosis'] == 'Case',['Level3_Sankey_source','Level3_Sankey_target','Level3_Sankey_value']]}")
    # Initialize a dictionary to store the counts
    counts = {}
    for _, row in data.loc[data['diagnosis'] == "Case"].iterrows():
        for a, b in zip(row['Level3_Sankey_source'], row['Level3_Sankey_target']):
            key = (a, b)
            counts[key] = counts.get(key, 0) + 1
    # Convert the dictionary to a DataFrame
    result_df = pd.DataFrame(list(counts.keys()), columns=['Source', 'Target'])
    result_df['Value'] = result_df.apply(lambda row: counts.get((row['Source'], row['Target']), 0) if not result_df.empty else 0, axis=1)
    if verbose:
        logger.debug(result_df)
    try:    
        sankey_fn = os.path.splitext(os.path.basename(fn))[0]+".sankey.tsv"
        result_df.to_csv(sankey_fn, sep="\t", index=False)
    except:
        logger.info(f"[Build_sankey_data] Couldn't create sankey output file:{sankey_fn}")
    data['Level3_Sankey_source'] = data['Level3_Sankey_source'].apply(lambda row: [entry for entry in row if entry != ""])
    data['Level3_Sankey_target'] = data['Level3_Sankey_target'].apply(lambda row: [entry for entry in row if entry != ""])
    data['Level3_Sankey_value'] = data['Level3_Sankey_value'].apply(lambda row: [entry for entry in row if entry != ""])
    data['Level3_Sankey_data'] = data['Level3_Sankey_data'].apply(lambda row: [entry for entry in row if entry != ""])
    data = data.drop(['temp_exc_diag_date','temp_exc_diag'], axis=1)
    return(data)

def BuildEntryExitDate(df1, df3, iidcol, input_date_in_name, input_date_out_name, verbose=False):
    """
    Estimate entry and exit dates for individuals based on the earliest and latest available diagnosis dates.

    Parameters:
    - df1: DataFrame containing the raw diagnosis dates.
    - df3: DataFrame to which the entry/exit dates will be merged.
    - iidcol: Column name representing the unique individual identifier.
    - input_date_in_name: Preferred column name for the entry date.
    - input_date_out_name: Preferred column name for the exit date.
    - verbose: If True, prints debug information.
    
    Returns:
    - Merged DataFrame with new 'EntryDate' and 'ExitDate' columns.
    """
    if verbose:
        logger.debug("[BuildEntryExitDate] Estimating the Entry and Exit date for Individuals based on the first and last available diagnosis dates.")

    # Create a working copy of df1
    tmp_entry_exit_df = df1.copy()

    # Check if the preferred date column names exist; if not, use defaults if available
    available_columns = tmp_entry_exit_df.columns.tolist()
    if verbose:
        logger.debug(f"[BuildEntryExitDate] Available columns: {available_columns}")
    
    if input_date_in_name not in available_columns and "date_in" in available_columns:
        input_date_in_name = "date_in"
    if input_date_out_name not in available_columns and "date_out" in available_columns:
        input_date_out_name = "date_out"
    
    # Build a list of columns to keep: only iidcol and the two date columns.
    keep_cols = [iidcol, input_date_in_name]
    if input_date_out_name != input_date_in_name:
        keep_cols.append(input_date_out_name)

    if verbose:
        logger.debug(f"[BuildEntryExitDate] Keeping columns: {keep_cols}")
    
    tmp_entry_exit_df = tmp_entry_exit_df[keep_cols]
    
    if verbose:
        logger.debug("[BuildEntryExitDate] DataFrame after dropping unnecessary columns:\n", tmp_entry_exit_df.head())

    # Group by individual id and calculate entry and exit dates
    grouped = tmp_entry_exit_df.groupby(iidcol, group_keys=False)
    in_dates = grouped[input_date_in_name].min().reset_index(name='EntryDate')
    
    if input_date_out_name != input_date_in_name:
        out_dates = grouped[input_date_out_name].max().reset_index(name='ExitDate')
    else:
        out_dates = grouped[input_date_in_name].max().reset_index(name='ExitDate')

    if verbose:
        logger.debug(f"[BuildEntryExitDate] Entry dates:\n {in_dates.head()}")
        logger.debug(f"[BuildEntryExitDate] Exit dates:\n {out_dates.head()}")

    # Merge entry and exit dates
    entry_exit_date_df = in_dates.merge(out_dates, on=iidcol)
    
    if verbose:
        logger.debug(f"[BuildEntryExitDate] Combined Entry/Exit DataFrame:\n {entry_exit_date_df.head()}")

    # Merge with df3
    df3_new = df3.merge(entry_exit_date_df, on=iidcol)
    
    if verbose:
        logger.debug(f"[BuildEntryExitDate] Merged final DataFrame:\n {df3_new.head()}")
    
    return df3_new

def load_config(filename="get_pheno.ini"):
    config = configparser.ConfigParser()
    # First try the current working directory
    #config_path = os.path.join(os.getcwd(), filename)
    config_path = os.path.join(os.path.dirname(os.path.realpath(__file__)), filename)
    logger.info(f"[load_config] Trying to load load ini file from {config_path}")
    if not os.path.isfile(config_path):
        # If not found, try the directory where the script is located
        script_dir = os.path.dirname(os.path.abspath(__file__))
        config_path = os.path.join(script_dir, filename)
        logger.info(f"[load_config] Trying to load load ini file from {config_path}")
        if not os.path.isfile(config_path):
            # No configuration file found; you can either return an empty config or handle defaults
            return None

    config.read(config_path)
    return config

def merge_secondary_diagnoses(df1, df, diagnostic_col, lpr_recnummer, lpr2nd_recnummer, diagnostic2nd_col):
    """
    Merges secondary diagnosis data with the primary dataset.
    """
    df.rename(columns={lpr2nd_recnummer: lpr_recnummer, diagnostic2nd_col: diagnostic_col}, inplace=True)
    df1_temp = pd.merge(df[[diagnostic_col, lpr_recnummer]], df1.drop(columns=[diagnostic_col], errors='ignore'), on=lpr_recnummer, how='inner')
    df1_temp.drop_duplicates(inplace=True)
    
    # Replace diagnostic_col entry with c_tildiag as the c_diagtype is + which means, there already exists an entry with diagnostic_col and a different c_diagtype, so that we now can replace this entry with the associated diagnosis.
    if "c_tildiag" in df1_temp.columns and "c_diagtype" in df1_temp.columns:
        df1_temp.loc[df1_temp["c_diagtype"].str.contains("+", na=False), diagnostic_col] = df1_temp["c_tildiag"]
    
    return pd.concat([df1, df1_temp], ignore_index=True, sort=False)

def load_stam_file(
    stam_file: str,
    isep: str,
    birthdatecol: str,
    diagnostic_col: str,
    sexcol: str,
    stam_cols_to_read_as_date: Iterable[str]
):
    """
    Load 1 or more CSV/STATA files into a single DataFrame.
    - Let pandas infer non-date dtypes.
    - Parse only requested date columns (robustly).
    - Rename birthdate/diagnosis/sex to canonical names.
    Uses globals: dta_input, DayFirst, DateFormat, verbose.
    """
    paths: List[str] = [p.strip() for p in stam_file.split(",")] if "," in stam_file else [stam_file]
    frames: List[pd.DataFrame] = []

    # normalize requested date columns input
    if isinstance(stam_cols_to_read_as_date, str):
        stam_cols_to_read_as_date = [c.strip() for c in stam_cols_to_read_as_date.split(",")]
    requested_dates = set(stam_cols_to_read_as_date or [])

    for path in paths:
        if dta_input:
            df = pd.read_stata(path)
        else:
            # discover columns
            try:
                hdr = pd.read_csv(path, sep=isep, nrows=0)
            except TypeError:
                hdr = pd.read_csv(path, sep=isep, nrows=0, engine="python")
            if hdr.columns.empty:
                logger.info(f"[load_stam_file] ERROR: Could not load file header: {path}")
                sys.exit(1)

            # intersect requested dates with present columns
            date_cols = [c for c in requested_dates if c in hdr.columns]
            if verbose and date_cols:
                logger.debug(f"[load_stam_file] date columns in {path}: {date_cols}")
            if date_cols:
                logger.info(f"[load_stam_file] Origianl cols: {requested_dates}")
                logger.info(f"[load_stam_file] date columns in {path}: {date_cols}")
                hdr2 = pd.read_csv(path, sep=isep, nrows=2)
                logger.info(f"[load_stam_file] DayFirst = {DayFirst}; DateFormat = {DateFormat}\ndf3 before updating Dates: {hdr2}")
                del(hdr2)
        
            # read with type inference
            try:
                df = pd.read_csv(path, sep=isep)#, low_memory=False)
            except TypeError:
                df = pd.read_csv(path, sep=isep, engine="python")#, low_memory=False)

            # robust date parsing
            for col in date_cols:
                df[col] = _to_datetime_series(df[col], fmt=DateFormat, dayfirst=DayFirst)
            logger.info(df.head(5))

        frames.append(df)

    if not frames:
        return pd.DataFrame()

    df3 = pd.concat(frames, ignore_index=True, sort=False)

    # renames
    if birthdatecol in df3.columns and birthdatecol != "birthdate":
        df3 = df3.rename(columns={birthdatecol: "birthdate"})
        if verbose:
            logger.debug(f"[load_stam_file] Renamed {birthdatecol}  'birthdate'")

    if diagnostic_col in df3.columns and diagnostic_col != "diagnosis":
        df3 = df3.rename(columns={diagnostic_col: "diagnosis"})
        if verbose:
            logger.debug(f"[load_stam_file] Renamed {diagnostic_col}  'diagnosis'")

    if sexcol in df3.columns and sexcol != "sex":
        df3 = df3.rename(columns={sexcol: "sex"})

    # ensure 'birthdate' parsed even if it wasn't in date_cols
    if "birthdate" in df3.columns and not pd.api.types.is_datetime64_any_dtype(df3["birthdate"]):
        df3["birthdate"] = _to_datetime_series(df3["birthdate"], fmt=DateFormat, dayfirst=DayFirst)

    # optional: tidy object  string (skip datetimes)
    for c in df3.select_dtypes(include=["object"]).columns:
        if pd.api.types.is_datetime64_any_dtype(df3[c]):
            continue
        # keep mostly-numeric as is; others to pandas 'string'
        num = pd.to_numeric(df3[c], errors="coerce")
        if num.notna().mean() < 0.5:
            df3[c] = df3[c].astype("string")

    return df3

def process_lpr_data(lpr_file, lpr2nd_file, dta_input, fsep, lpr_cols_to_read_as_date, DateFormat, potential_lpr_cols_to_read_as_date, diagnostic_col, diagnostic2nd_col, lpr_recnummer, lpr2nd_recnummer):
    """
    Process the LPR data sequentially, ensuring an equal number of LPR and LPR2nd files are provided.
    """
    file_paths = lpr_file.split(',') if ',' in lpr_file else [lpr_file]
    if lpr2nd_file != "":
        secondary_paths = lpr2nd_file.split(',') if ',' in lpr2nd_file else [lpr2nd_file]
    else:
        secondary_paths = []
    
    if len(file_paths) != len(secondary_paths) and len(secondary_paths) > 0 :
        logger.debug(f"[process_lpr_data] Mismatch in number of LPR and LPR2nd files. Ensure both are provided in the same order.")
        raise ValueError("[process_lpr_data] Mismatch in number of LPR and LPR2nd files. Ensure both are provided in the same order.")
        
    
    df1 = None
    logger.info(f"[process_lpr_data] LPR files {file_paths}, and 2nd diagnosis files {secondary_paths}")
    if (len(secondary_paths) > 0):
        for lprfile, lpr2ndfile in zip(file_paths, secondary_paths):
            logger.info(f"[process_lpr_data] Loading {lprfile}, and {lpr2ndfile}")
            df_lpr = load_stam_file(stam_file= lprfile, isep=fsep, birthdatecol="", diagnostic_col="", sexcol = "", stam_cols_to_read_as_date = [lpr_cols_to_read_as_date + potential_lpr_cols_to_read_as_date])
            df_lpr2nd = load_stam_file(stam_file= lpr2ndfile, isep=fsep, birthdatecol="", diagnostic_col="", sexcol = "", stam_cols_to_read_as_date = [lpr_cols_to_read_as_date + potential_lpr_cols_to_read_as_date])
            df_merged = merge_secondary_diagnoses(df_lpr, df_lpr2nd, diagnostic_col, lpr_recnummer, lpr2nd_recnummer, diagnostic2nd_col)
            
            if df1 is None:
                df1 = df_merged
            else:
                df1 = pd.concat([df1, df_merged], ignore_index=True, sort=False)
    else:
        for lprfile in file_paths:
            logger.info(f"[process_lpr_data] Loading {lprfile}")
            df_lpr = load_stam_file(stam_file= lprfile, isep=fsep, birthdatecol="", diagnostic_col="", sexcol = "", stam_cols_to_read_as_date = set(lpr_cols_to_read_as_date + potential_lpr_cols_to_read_as_date))
                        
            if df1 is None:
                df1 = df_lpr
            else:
                df1 = pd.concat([df1, df_lpr], ignore_index=True, sort=False)
    return df1

def finalize_lpr_data(df1, diagnostic_col, birthdatecol, ctype_col, verbose):
    """
    Finalize the loaded LPR data, renaming columns where necessary.
    """
    if diagnostic_col in df1.columns and diagnostic_col != "diagnosis":
        df1.rename(columns={diagnostic_col: "diagnosis"}, inplace=True)
        if verbose:
            logger.debug("[finalize_lpr_data] Updated diagnostic_col name to 'diagnosis'.")
    
    if birthdatecol in df1.columns and birthdatecol != "birthdate":
        df1.rename(columns={birthdatecol: "birthdate"}, inplace=True)
        if verbose:
            logger.debug("[finalize_lpr_data] Updated birthdatecol name to 'birthdate'.")

    if "c_pattype" in df1.columns:
        df1.rename(columns={"c_pattype": "pattype"}, inplace=True)
        if verbose:
            logger.debug("[finalize_lpr_data] Updated c_pattype name to 'pattype'.")

    if "c_indm" in df1.columns:
        df1.rename(columns={"c_indm": "admissiontype"}, inplace=True)
        if verbose:
            logger.debug("[finalize_lpr_data] Updated c_indm name to 'admissiontype'.")

    if ctype_col in df1.columns:
        df1.rename(columns={ctype_col: "diagtype"}, inplace=True)
        if verbose:
            logger.debug(f"[finalize_lpr_data] Updated {ctype_col} name to 'diagtype'.")

    # if "c_diagtype" in df1.columns:
    #     df1.rename(columns={"c_diagtype": "diagtype"}, inplace=True)
    #     if verbose:
    #         logger.debug("[finalize_lpr_data] Updated c_diagtype name to 'diagtype'.")

    # if "type" in df1.columns:
    #     df1.rename(columns={"type": "diagtype"}, inplace=True)
    #     if verbose:
    #         logger.debug("[finalize_lpr_data] Updated type name to 'diagtype'.")

    if "source" in df1.columns:
        df1.rename(columns={"source": "register"}, inplace=True)
        if verbose:
            logger.debug("[finalize_lpr_data] Updated source name to 'register'.")

    if "diag_source" in df1.columns:
        df1.rename(columns={"diag_source": "source"}, inplace=True)
        if verbose:
            logger.debug("[finalize_lpr_data] Updated diag_source name to 'source'.")

    return df1

def process_entry(entry, remove_leading, eM, mode, icdprefix, remove_point, ICDCM):
    """
    Process a single ICD entry based on the settings.
    Modes: "NCRR_DST", "IBP_DST", "IBP_computerome", "iPSYCH", "CHB_DBDS", "Default" --> based on cluster_run
    The prefix variable is used when adjusting ICD10 entries. --> DK specific
    """
    remove_leading = str(remove_leading).strip().lower() in ("true", "1", "yes", "y")
    # Handle non-string numeric entries
    if not isinstance(entry, str):
        if isinstance(entry, (int, float)) and str(entry).isdigit():
            logger.info("[process_entry] WARNING: Integer given without information about underlying ICD definition. This will be interpreted as ICD8. Otherwise state ICD9:ZZZ.ZZ or ICD8:ZZZ.ZZ")
            if only_ICD10:
                return None
            return format_numeric(entry, mode)
        return str(entry).replace('.', '') if remove_point else entry
    # ATC entries are passed through (with minor adjustments in some modes)
    elif entry.startswith("ATC:"):
        if only_ICD10 or only_ICD9 or only_ICD10:
            return None
        global ATC_Requested 
        if ATC_Requested == "None":
            ATC_Requested = "Some"
        res = entry
        res = entry.replace("ATC:", "", 1) if remove_leading else entry
        if mode in DK_clusters and remove_leading:
            res = "ATC:" + res.upper().replace('.', '')
        return res.replace('.', '') if remove_point else res
    # ICD8 processing common to several modes
    elif entry.startswith("ICD8:"):
        if only_ICD10 or only_ICD9:
            return None
        if mode in DK_clusters:
            new_entry = entry.replace("ICD8:", "", 1)
            res = format_numeric(new_entry, mode)
            entry = "ICD8:" + res
        res = entry.replace('.', '') if remove_point else entry
        return res.replace("ICD8:", "", 1).upper() if remove_leading else res
    # Process ICD9-CM and ICD10-CM for ICDCM and skip in other modes
    elif entry.startswith("ICD9-CM:"):
        if only_ICD10 or only_ICD8:
            return None
        if mode in ("ICDCM","default"):
            res = entry.replace("ICD9-CM:", "", 1).upper()
            res = res if remove_leading else "ICD9-CM:" + res
            return res.replace('.', '') if remove_point else res
        return None  # skip in other modes
    elif entry.startswith("ICD10-CM:"):
        if only_ICD8 or only_ICD9:
            return None
        if mode in ("ICDCM","default") or ICDCM:
            res = entry.replace("ICD10-CM:", "", 1).upper()
            res = res if icdprefix != "" else icdprefix + res
            res = res if remove_leading else "ICD10-CM:" + res
            return res.replace('.', '') if remove_point else res
        return None  # skip in other modes
    # For ICD9 (non-CM)  only RegisterRun skips these
    elif entry.startswith("ICD9:"):
        if only_ICD10 or only_ICD8:
            return None
        if mode in DK_clusters:
            return None
        res = entry.replace("ICD9:", "", 1).upper()
        res = res if remove_leading else "ICD9:" + res
        return res.replace('.', '') if remove_point else res
    # Process ICD10 entries  behavior may differ if a custom icdprefix is desired.
    elif entry.startswith("ICD10:") or entry.startswith(f"{icdprefix}"):
        if only_ICD8 or only_ICD9:
            return None
        logger.info(f"[process_entry] Within ICD10 section; mode: {mode}; entry: {entry}; ICD prefix: {icdprefix}")
        # If the entry does not already have the custom icdprefix, add it.
        if icdprefix != "":
            if not entry.startswith(f"ICD10:") and not entry.startswith(f"{icdprefix}"):
                res = "ICD10:" + icdprefix + entry.replace("ICD10:", "", 1).upper()
                logger.info("[process_entry] Within ICD10 section; In if")
            elif entry.startswith(f"{icdprefix}"):
                res = entry.upper()
                res = res if remove_leading else "ICD10:" + res
                logger.info(f"[process_entry] Within ICD10 section; In first elif; ICD10 starts with {icdprefix}")
            elif entry.startswith(f"ICD10:{icdprefix}"):
                res = entry.upper()
                res = res.replace("ICD10:", "", 1) if remove_leading else res
                #res = res if remove_leading else "ICD10:" + icdprefix + res
                #res = icdprefix + entry.upper()
                logger.info("[process_entry] Within ICD10 section; In second elif")
            elif entry.startswith("ICD10:"):
                res = entry.replace("ICD10:", "", 1).upper()
                res = icdprefix + res
                res = res if remove_leading else "ICD10:" + res
                logger.info("[process_entry] Within ICD10 section; In third elif")
        else:
            # For ICDCM and default modes, just remove the icdprefix if requested.
            res = entry.replace("ICD10:", "", 1).upper()
            res = res if remove_leading else "ICD10:" + res
        res = res.replace('.', '') if remove_point else res
        logger.info(f"[process_entry] Within ICD10 section; mode: {mode}; entry: {entry}; res: {res}; ICD prefix: {icdprefix}")
        return res
    else: 
        res = entry
    if mode in DK_clusters and not entry.isdigit() and not res.upper().startswith("ATC"):
        entry_clean = str(entry).strip()
        # Two-letter ICD-10 like DF32, AA10...
        if re.match(r'^[A-Z]{2}\d', entry_clean, re.IGNORECASE):
            res = entry_clean.upper().replace('.', '')
            res = res if remove_leading else "ICD10:" + res
            logger.info(f"[process_entry] Updated res (two-letter ICD10) -> {res}; remove_leading: {remove_leading}")
        # One-letter ICD-10 like F, F3, F32... but NOT FX (letter followed by letter)
        elif re.match(r'^[A-Z](?:\d|$)', entry_clean, re.IGNORECASE):
            normalized = entry_clean.upper().replace('.', '')
            res = icdprefix + normalized
            res = res if remove_leading else "ICD10:" + res
            logger.info(f"[process_entry] Updated res (at least one-letter ICD10) -> {res}; remove_leading: {remove_leading}")
        else:
            logger.info(f"[process_entry] {entry_clean} has more than two letters before a numeric value. Leaving as is.")
            res = entry_clean.replace('.', '')
        logger.info(f"[process_entry] Within DK_cluster mode; mode: {mode}; entry: {entry_clean}; res: {res}; ICD prefix: {icdprefix}")
    elif isinstance(entry, str):
        entry_clean = str(entry).strip()
        res = entry_clean.replace('.', '') if remove_point else entry_clean
        res = res if icdprefix == "" else icdprefix + res
        res = res if remove_leading else "ICD10:" + res
        logger.info(f"[process_entry] Not in DK_cluster mode; mode: {mode}; entry: {entry_clean}; res: {res}; ICD prefix: {icdprefix}")
        print(f"[process_entry] Not in DK_cluster mode; mode: {mode}; entry: {entry_clean}; res: {res}; ICD prefix: {icdprefix}")
    for tag in ["ATC:", "ICD8:", "ICD9:", "ICD10:", "ICD9-CM:", "ICD10-CM:", "ICD9CM:", "ICD10CM:"]:
        if res.upper().startswith(tag.upper()):
            res = res.replace(tag, "", 1) if remove_leading else res
            break
    if remove_point: # or mode in DK_clusters:
        res = res.replace('.', '')
    #res = remove_leading_icd(res) if (remove_leading and isinstance(res, str)) else res
    logger.info(f"[process_entry] Within Standard mode and no ATC,ICD8/9/10/CM identified; mode: {mode}; entry: {entry}; res: {res}; ICD prefix: {icdprefix}")
    return res

def process_ophold(ophold, stam, tmp_result_df, ophold_out_file, birthdatecol, iidcol, verbose, min_code = 5000, max_code = 7000):
    #As exclusion criterion
    logger.info("[process_ophold] This will generate a new ophold combined file, outlining the date moved to Denmark.")
    if verbose:
        logger.debug("[process_ophold] This will be using the codes [5000,7000] (fkode) to determine if born in DK or not.\nInformation about the codes can be found here: https://www.dst.dk/da/Statistik/dokumentation/Times/cpr-oplysninger/foedreg-kode")
    stam = stam[(stam['fkode'] >= min_code) & (stam['fkode'] <= max_code)]
    stam['both_parents_DK'] = False
    stam.loc[(stam['fkode_m'] >= min_code) & (stam['fkode_m'] <= max_code) & (stam['fkode_f'] >= min_code) & (stam['fkode_f'] <= max_code), 'both_parents_DK'] = True
    stam = stam[[iidcol,'fkode',birthdatecol,"both_parents_DK"]]
    ophold = pd.merge(stam,ophold,how="left", on=iidcol)
    grouped = ophold.groupby([iidcol],group_keys=False) 
    del(ophold)
    # < 20 refers to alive and residing in DK https://www.dst.dk/da/Statistik/dokumentation/Times/cpr-oplysninger
    stat_lists = grouped['stat'].apply(list).reset_index(name='stat')
    statd_lists = grouped['statd'].apply(list).reset_index(name='statd')
    ophold_df = stat_lists.merge(statd_lists, on=iidcol)
    del(statd_lists)
    del(stat_lists)
    tflytd_lists = grouped['tflytd'].apply(list).reset_index(name='tflytd')
    ophold_df = ophold_df.merge(tflytd_lists, on=iidcol)
    del(tflytd_lists)
    fflytd_lists = grouped['fflytd'].apply(list).reset_index(name='fflytd')
    ophold_df = ophold_df.merge(fflytd_lists, on=iidcol)
    del(fflytd_lists)
    komkod_lists = grouped['komkod'].apply(list).reset_index(name='komkod')
    ophold_df = ophold_df.merge(komkod_lists, on=iidcol)
    del(komkod_lists)
    orig_lists = grouped['orig'].apply(list).reset_index(name='orig')
    ophold_df = ophold_df.merge(orig_lists, on=iidcol)
    del(orig_lists)
    opholdnr_lists = grouped['opholdnr'].apply(list).reset_index(name='opholdnr')
    ophold_df = ophold_df.merge(opholdnr_lists, on=iidcol)
    del(opholdnr_lists)
    moved_to_dk = grouped['fflytd'].min().reset_index(name='moved_to_dk')
    ophold_df = ophold_df.merge(moved_to_dk, on=iidcol)
    del(moved_to_dk)
    if (ophold_out_file != ""):
        ophold_df.to_csv(ophold_out_file, sep="\t", index=False, quoting=False)
        reformat_to_tsv(ophold_out_file)
    logger.info(f"[process_ophold] New Ophold file stored here: {ophold_out_file}")
    if(tmp_result_df != ""):
        dx_result_df = pd.merge(tmp_result_df, ophold_df, on=iidcol)
    else: 
        dx_result_df = ophold_df.copy()
    if verbose:
        logger.debug(f"[process_ophold] ophold_df contains duplicated iids: {dx_result_df[iidcol].duplicated().any()}")
    dx_result_df.drop_duplicates(subset=[iidcol], inplace=True)
    del tmp_result_df
    del ophold_df
    if verbose:
        logger.debug("[process_ophold] Mem after building opholds and deleting unneeded variables:")
        usage()
    return dx_result_df

def update_icd_coding(data, eM=False,
                      skip=False, remove_point_in_diag_request=False,
                      ICDCM=False, no_Fill=False, noLeadingICD=False,
                      icdprefix=""):
    """
    Updates the ICD coding for a list or DataFrame of entries.
    Parameters:
      - data: list or DataFrame containing the entries.
      - dst, dbdschb, ipsych, ICDCM, RegisterRun: booleans that determine the processing mode.
      - eM: flag passed to formatting functions.
      - remove_point_in_diag_request: if True, remove periods.
      - noLeadingICD: if True, remove any remaining ICD prefixes after processing.
      - prefix: custom prefix string to be used for ICD10 entries (replaces the default "D").
    """
    logger.info(f"[update_icd_coding] Starting update_icd_coding with the following codes to update: {data}")
    if not verbose:
        print(f"[update_icd_coding] Starting update_icd_coding with the following codes to update: {data}")
    # Convert data to a list of entries.
    if not isinstance(data, list):
        data = data.iloc[:, 0].tolist()
    # if only_ICD10:
    #     data = [str(entry) for entry in data 
    #                    if (isinstance(entry, str) and (entry.startswith("ICD10:") or entry.startswith("ICD10-CM:") or entry.startswith("ICD-10:")))]
    # if only_ICD9:
    #     data = [str(entry) for entry in data 
    #                    if (isinstance(entry, str) and (entry.startswith("ICD9:") or entry.startswith("ICD9-CM:") or entry.startswith("ICD-9:")))]
    # if only_ICD8:
    #     data = [str(entry) for entry in data 
    #                    if (isinstance(entry, str) and (entry.startswith("ICD8:") or entry.startswith("ICD-8:")))]
    if skip:
        return data
    output_list = []
    print_which_section = True
    # Determine mode priority.
    if ICDCM:
        if print_which_section:
            logger.info("[update_icd_coding] Processing only CM ICD codes in update_icd_coding")
            print_which_section = False
    if only_ICD10:
        if print_which_section and not ICDCM:
            logger.info("[update_icd_coding]Processing only ICD10 codes in update_icd_coding")
            print_which_section = False
    elif only_ICD9 and not ICDCM:
        if print_which_section:
            logger.info("[update_icd_coding]Processing only ICD9 codes in update_icd_coding")
            print_which_section = False
    elif only_ICD8 and not ICDCM:
        if print_which_section:
            logger.info("[update_icd_coding]Processing only ICD8 codes in update_icd_coding")
            print_which_section = False
    if verbose:
        logger.debug(f"[update_icd_coding] noLeadingICD is set to: {noLeadingICD}")
    output_list = []
    for entry in data:     
        p = process_entry(
            entry=entry,
            remove_leading=noLeadingICD,
            eM=eM,
            mode=cluster_run,
            icdprefix=icdprefix,
            remove_point=remove_point_in_diag_request,
            ICDCM=ICDCM,
        )
        logger.info(f"[update_icd_coding] Processing entry [process_entry]: entry={entry}; "
            f"remove_leading={noLeadingICD}; eM={eM}; mode={cluster_run}; "
            f"icdprefix={icdprefix}; remove_point={remove_point_in_diag_request}; ICDCM={ICDCM}. Result: {p}")
        if p is not None:
            output_list.append(p)
    if verbose:
        logger.debug(f"[update_icd_coding] output_list before mode check (duplicated) and after remove_leading_icd and process_entry: ",output_list)
    # For DBDS or IPSYCH modes, remove any entries starting with ICD10-CM or ICD9-CM - this is already done in process_entry.
    if cluster_run in DK_clusters:
        output_list = [entry for entry in output_list 
                       if not (isinstance(entry, str) and (entry.startswith("ICD10-CM:") or entry.startswith("ICD9-CM:")))]
    logger.info(f"[update_icd_coding] Final updated ICD codes: {output_list}")
    if not verbose:
        print(f"[update_icd_coding] Final updated ICD codes: {output_list}")
    return output_list

def dict_update_icd_coding(curr_codes: pd.DataFrame,
                           exact_match: bool,
                           skip_icd_update: bool,
                           remove_point_in_diag_request: bool,
                           ICDCM: bool,
                           noLeadingICD: bool,
                           icdprefix: str) -> pd.DataFrame:
    def is_advanced(val):
        if not isinstance(val, str):
            return False
        return any(k in val for k in ['main=', 'sub=', 'rule_out=', '-']) and '-CM' not in val
    # Expect columns: 'Disorder', 'Disorder Codes' (string with commas OR list)
    for _, row in curr_codes.iterrows():
        disorder = row['Disorder']
        raw = row['Disorder Codes']
        updated_list = []
        if any(is_advanced(v) for v in raw):
            logger.info("[dict_update_icd_coding] Advanced phenotype logic detected. Skipping ICD mapping and using raw input.")
            #advanced_dict = parse_pheno_rules(raw)
            if isinstance(raw,list):
                for sublist in raw:
                    logger.info(f"[dict_update_icd_coding] diags:{raw}; sublist:{sublist}; is_advanced(sublist): {is_advanced(sublist)}")
                    if is_advanced(sublist):
                        advanced_dict = parse_pheno_rules(sublist,exact_match=exact_match,
                                                          skip_icd_update=skip_icd_update,remove_point_in_diag_request=remove_point_in_diag_request,
                                                          ICDCM=ICDCM,noLeadingICD=noLeadingICD,icdprefix=icdprefix)
                        logger.info(f"[dict_update_icd_coding] Within sublist; advanced_dict: {advanced_dict}")
                    else:
                        if sublist is None or (isinstance(sublist, float) and pd.isna(sublist)):
                            raw_list = []
                        elif isinstance(sublist, list):
                            raw_list = [str(x).strip() for x in sublist if str(x).strip() != ""]
                        else:
                            # strings like "ICD10:F32,ICD10:F33" or a single code
                            s = str(sublist).strip()
                            raw_list = [x.strip() for x in s.split(',')] if (',' in s) else ([s] if s else [])
                        updated = update_icd_coding(
                            data=raw_list,
                            eM=exact_match,
                            skip=skip_icd_update,
                            remove_point_in_diag_request=remove_point_in_diag_request,
                            ICDCM=ICDCM,
                            noLeadingICD=noLeadingICD,
                            icdprefix=icdprefix,
                        )
                        updated_list.extend(updated)
            else:
                advanced_dict = parse_pheno_rules(raw,exact_match=exact_match,skip_icd_update=skip_icd_update,
                                                  remove_point_in_diag_request=remove_point_in_diag_request,
                                                  ICDCM=ICDCM,noLeadingICD=noLeadingICD,icdprefix=icdprefix)
                logger.info(f"[dict_update_icd_coding] diags:{raw}; advanced_dict: {advanced_dict}")
            updated = ""
            # main matches
            if advanced_dict.get('main'):
                main_codes = set(advanced_dict['main'])
                # main_codes = update_icd_coding(
                #     data=main_codes,
                #     eM=exact_match,
                #     skip=skip_icd_update,
                #     remove_point_in_diag_request=remove_point_in_diag_request,
                #     ICDCM=ICDCM,
                #     noLeadingICD=noLeadingICD,
                #     icdprefix=icdprefix,
                # )
                if updated != "":
                    updated = updated + ";"
                if advanced_dict.get('sub') or advanced_dict.get('rule_out'):
                    updated = updated + "main=" + ','.join(main_codes)
                else:
                    updated = main_codes
                updated_list.extend(main_codes)
            # sub matches
            if advanced_dict.get('sub'):
                sub_codes = set(advanced_dict['sub'])
                # sub_codes = update_icd_coding(
                #     data=sub_codes,
                #     eM=exact_match,
                #     skip=skip_icd_update,
                #     remove_point_in_diag_request=remove_point_in_diag_request,
                #     ICDCM=ICDCM,
                #     noLeadingICD=noLeadingICD,
                #     icdprefix=icdprefix,
                # )
                if updated != "":
                    updated = updated + ";"
                updated = updated + "sub=" + ','.join(sub_codes)
                updated_list.extend(sub_codes)
            # rule_out logic
            if advanced_dict.get('rule_out'):
                ruleout_codes = set(advanced_dict['rule_out'])
                # ruleout_codes = update_icd_coding(
                #     data=ruleout_codes,
                #     eM=exact_match,
                #     skip=skip_icd_update,
                #     remove_point_in_diag_request=remove_point_in_diag_request,
                #     ICDCM=ICDCM,
                #     noLeadingICD=noLeadingICD,
                #     icdprefix=icdprefix,
                # )
                if updated != "":
                    updated = updated + ";"
                    updated_list
                updated = updated + "rule_out=" + ','.join(ruleout_codes)
                updated_list.extend(ruleout_codes)
        else:
            # normalize raw -> list[str]
            if raw is None or (isinstance(raw, float) and pd.isna(raw)):
                raw_list = []
            elif isinstance(raw, list):
                raw_list = [str(x).strip() for x in raw if str(x).strip() != ""]
            else:
                # strings like "ICD10:F32,ICD10:F33" or a single code
                s = str(raw).strip()
                raw_list = [x.strip() for x in s.split(',')] if (',' in s) else ([s] if s else [])
            updated = update_icd_coding(
                data=raw_list,
                eM=exact_match,
                skip=skip_icd_update,
                remove_point_in_diag_request=remove_point_in_diag_request,
                ICDCM=ICDCM,
                noLeadingICD=noLeadingICD,
                icdprefix=icdprefix,
            )
            updated_list.extend(updated)
        curr_codes.loc[curr_codes['Disorder'] == disorder, 'Disorder Codes'] = [updated]
    return curr_codes,updated_list

def load_phenotypes(pheno_request, pheno_requestcol, pheno_name="NoName",
                    icdprefix="", noLeadingICD=True, ICDCM=False,
                    skip_icd_update=False, exact_match=False,
                    remove_point_in_diag_request=True):
    # Load the file with the phenotype code requests
    with open(pheno_request, 'r') as file:
        in_pheno_codes_lines = file.readlines()

    # Peek at first line to decide format
    header_tokens = in_pheno_codes_lines[0].strip().split('\t')
    detected_header = False
    skip = False

    if len(header_tokens) == 2:
        # ---- TWO-COLUMN FORMAT: Disorder | Disorder Codes (comma-separated) ----
        multi_inclusions = True

        # Header detection based on second column name
        second_col = header_tokens[1].strip()
        if second_col.lower() in ("diagnosis", "diagnoses"):
            detected_header = True
        else:
            logger.info("[load_phenotypes] No header identified (expected 'Diagnosis' or 'Diagnoses' in the second column).")
        logger.info(f"[load_phenotypes] Detected 2-column inclusion file. Header detected: {detected_header}. Second column: {second_col!r}")

        # Strip empties; optionally skip the header line
        lines = [ln.strip() for ln in in_pheno_codes_lines if ln.strip()]
        if detected_header:
            lines = lines[1:]

        # Parse rows into (Disorder, Disorder Codes) tuples
        processed = []
        for ln in lines:
            if '\t' not in ln:
                continue
            left, right = ln.split('\t', 1)
            processed.append((left.strip(), right.strip()))
        in_pheno_codes = pd.DataFrame(processed, columns=['Disorder', 'Disorder Codes'])

        # Drop fully-empty rows; normalize whitespace
        in_pheno_codes = (
            in_pheno_codes
            .dropna(how='all')
            .map(lambda x: x.strip() if isinstance(x, str) else x)
            .replace('', pd.NA)
            .dropna(how='all')
        )
        # in_pheno_codes = (
        #     in_pheno_codes
        #     .dropna(how='all')
        #     .applymap(lambda x: x.strip() if isinstance(x, str) else x)
        #     .replace('', pd.NA)
        #     .dropna(how='all')
        # )

        # For ATC detection below
        column_values = in_pheno_codes['Disorder Codes']

    elif len(header_tokens) == 1:
        # ---- ONE-COLUMN FORMAT: codes only (one per row). Disorder == pheno_name ----
        multi_inclusions = False
        logger.info(f"[load_phenotypes] One-column inclusion file detected. Disorder name will be set to '{pheno_name}'.")

        # Normalize lines, drop empties
        lines = [ln.strip() for ln in in_pheno_codes_lines if ln.strip()]

        # Detect and skip header named 'diagnosis' (case-insensitive)
        first_token = lines[0].split('\t')[0].strip() if lines else ""
        if first_token.lower() == "diagnosis":
            detected_header = True
            lines = lines[1:]

        # Enforce one value per row (no commas/tabs in the code itself)
        bad = [ln for ln in lines if ('\t' in ln) or (',' in ln)]
        if bad:
            logger.error(f"""[load_phenotypes] One-column format requires exactly one code per row (no commas or tabs). \n
                Invalid rows: {bad[:5]}{' ...' if len(bad) > 5 else ''}""")
            raise ValueError(
                "One-column format requires exactly one code per row (no commas or tabs). "
                f"Invalid rows: {bad[:5]}{' ...' if len(bad) > 5 else ''}"
            )

        # Build the same target structure as the 2-col branch:
        # DataFrame with a single row: Disorder = pheno_name, Disorder Codes = list of all codes
        codes = [ln.strip() for ln in lines]
        in_pheno_codes = pd.DataFrame(
            {'Disorder': [pheno_name], 'Disorder Codes': [codes]}
        )

        # For ATC detection below
        column_values = pd.Series(codes)

    else:
        logger.error(
            f"[load_phenotypes] ERROR: Could not identify the format of your input for {pheno_name}. "
            "Expected either:\n"
            "   two columns: <Disorder>\\t<comma-separated codes>, or\n"
            "   one column: one code per row (optional header 'diagnosis').\n"
            "Skipping this phenotype input."
        )
        skip = True

    if not skip:
        # Determine presence of ATC codes
        global ATC_Requested 
        if all(str(v).startswith('ATC') for v in column_values):
            ATC_Requested = "All"
        elif any(str(v).startswith('ATC') for v in column_values):
            ATC_Requested = "Some"
        elif ATC_Requested not in ["Some", "All"]:
            ATC_Requested = "None"

        logger.info(f"[load_phenotypes] in_pheno_codes before running dict_update_icd_coding: {in_pheno_codes}")
        # Update/normalize ICD coding; keeps DataFrame shape and sets 'Disorder Codes' to lists
        pheno_codes, normalized_phenos = dict_update_icd_coding(
            curr_codes=in_pheno_codes,
            exact_match=exact_match,
            skip_icd_update=skip_icd_update,
            remove_point_in_diag_request=remove_point_in_diag_request,
            ICDCM=ICDCM,
            noLeadingICD=noLeadingICD,
            icdprefix=icdprefix
        )
        return pheno_codes, multi_inclusions, normalized_phenos

    # If skipped due to format error, return safe defaults
    return pd.DataFrame(columns=['Disorder', 'Disorder Codes']), False

############ TEST To Remove if not working well

def Exclusion_interpreter(
    data: pd.DataFrame,
    *,
    min_Age: int = 0,
    max_Age: int = 0,
    lifetime_exclusions: pd.DataFrame = pd.DataFrame(),
    oneYearPrior_exclusions: pd.DataFrame = pd.DataFrame(),
    post_exclusions: pd.DataFrame = pd.DataFrame(),
    date_format: str = "%Y-%m-%d",
    verbose: bool = False,
) -> pd.DataFrame:
    """Apply age- and rule-based exclusions to derive Level2 fields.

    Notes:
    - Works with lists or comma-separated strings in input columns.
    - Stores list outputs in object-dtype columns (no CSV strings).
    - Uses label-safe assignments and vectorized date math.
    """
    out = data.copy()
    cols_to_drop = ['temp_Level2_dates','diagnosis','diagnoses','first_dx','in_dates','c_diagtype','Age_FirstDx','birthdate','temp_birthdate']

    # Initialize required columns with object dtype where we store lists per row
    list_cols = [
        'diagnoses_Level2_modifier', 'disorder_Level2_modifier', 'date_Level2_modifier',
        'Level2_diagnoses', 'Level2_dates', 'Level2_ExclusionReason', 'Level2_FirstDx'
    ]
    for col in list_cols:
        if col not in out.columns:
            out[col] = pd.Series([[] for _ in range(len(out))], dtype=object)
    # base Level2 copies
    out['Level2_diagnoses'] = out['diagnoses'].apply(_as_list).astype(object)
    out['Level2_dates'] = out['in_dates'].apply(_as_list).astype(object)
    if 'Level2_AgeExclusion' not in out.columns:
        out['Level2_AgeExclusion'] = "False"

    # Age-based exclusion
    if min_Age or max_Age:
        if verbose:
            logger.debug(f"[Exclusion_interpreter] Updating Case-Diagnoses based on Age, min={min_Age}, max={max_Age}")
        # compute age at first dx from original in_dates (robust)
        if 'out_dates' in out.columns:
            first_dx = min(pd.to_datetime(out['in_dates'].apply(lambda x: min(_to_dt_list(x)) if _to_dt_list(x) else pd.NaT), errors='coerce'),
                           pd.to_datetime(out['out_dates'].apply(lambda x: min(_to_dt_list(x)) if _to_dt_list(x) else pd.NaT), errors='coerce'))
        else:
            first_dx = pd.to_datetime(out['in_dates'].apply(lambda x: min(_to_dt_list(x)) if _to_dt_list(x) else pd.NaT), errors='coerce')
        if 'out_dates' in out.columns:
            last_dx = min(pd.to_datetime(out['in_dates'].apply(lambda x: max(_to_dt_list(x)) if _to_dt_list(x) else pd.NaT), errors='coerce'),
                           pd.to_datetime(out['out_dates'].apply(lambda x: max(_to_dt_list(x)) if _to_dt_list(x) else pd.NaT), errors='coerce'))
        else:
            last_dx = pd.to_datetime(out['in_dates'].apply(lambda x: max(_to_dt_list(x)) if _to_dt_list(x) else pd.NaT), errors='coerce')
        lbirth = pd.to_datetime(out.get('birthdate', pd.NaT), errors='coerce')
        age_years = ((first_dx - birth).dt.days // 365).astype('Int64')
        out['Age_FirstDx'] = age_years
        mask_age_excl = (
            (age_years < min_Age) |
            ((age_years > max_Age) & (age_years > 0))
        )
        out.loc[mask_age_excl.fillna(False), 'Level2_AgeExclusion'] = "True"
        out.loc[out['Level2_AgeExclusion'].eq("True"), 'Level2_ExclusionReason'] = (
            out.loc[out['Level2_AgeExclusion'].eq("True"), 'Level2_ExclusionReason'].apply(lambda x: _as_list(x) + [1])
        )

    # Apply exclusion tables
    def _apply_table(df: pd.DataFrame, table: pd.DataFrame, etype: str, cols_to_drop: list):
        if table.empty or 'Disorder' not in table.columns:
            return df
        exclNumber = 1
        for disorder in table['Disorder']:
            if verbose:
                logger.debug(f"Applying {etype} exclusion for {disorder}")
            has_rows = (df['diagnoses'].astype(str) != "") & (df[disorder].astype(str) != "")
            if has_rows.any():
                df = update_DxDates_multi_exclusion(
                    df,
                    etype,
                    disorder,
                    f"{disorder}_In_Dates",
                    f"{disorder}_Inflicted_changes",
                    exclNumber,
                    "Level2_diagnoses",
                    "Level2_dates",
                    "diagnoses_Level2_modifier",
                    "date_Level2_modifier",
                    'disorder_Level2_modifier',
                    date_format=date_format,
                )
            else:
                if verbose:
                    logger.debug(f"No Case overlap with {disorder}. Skipping")
            exclNumber += 1
            cols_to_drop = sorted(
                    set(cols_to_drop + [disorder, disorder+"_Out_Dates", disorder+"_In_Dates"])
                )
        return df, cols_to_drop

    out, cols_to_drop= _apply_table(out, oneYearPrior_exclusions, "1yprior", cols_to_drop)
    out, cols_to_drop = _apply_table(out, post_exclusions, "post", cols_to_drop)
    out, cols_to_drop = _apply_table(out, lifetime_exclusions, "lifetime", cols_to_drop)

    # For AgeExclusion==True, clear Level2 fields and tag modifier
    mask_age_true = out['Level2_AgeExclusion'].eq("True")
    if mask_age_true.any():
        out.loc[mask_age_true, 'Level2_dates'] = [[] for _ in range(mask_age_true.sum())]
        out.loc[mask_age_true, 'Level2_diagnoses'] = [[] for _ in range(mask_age_true.sum())]
        out.loc[mask_age_true, 'disorder_Level2_modifier'] = out.loc[mask_age_true, 'disorder_Level2_modifier'].apply(
            lambda lst: _as_list(lst) + ["Age"]
        )

    # Compute Level2_FirstDx from Level2_dates
    level2_first = out['Level2_dates'].apply(lambda x: _to_dt_list(x))
    out['Level2_FirstDx'] = level2_first.apply(lambda xs: [min(xs)] if xs else [])

    # Recalculate age at first diagnosis using Level2
    birth = pd.to_datetime(out.get('birthdate', pd.NaT), errors='coerce')
    first_dx0 = out['Level2_FirstDx'].apply(lambda xs: xs[0] if isinstance(xs, list) and xs else pd.NaT)
    first_dx0 = pd.to_datetime(first_dx0, errors='coerce')
    out['Level3_Age_FirstDx'] = ((first_dx0 - birth).dt.days // 365).fillna(0).astype(int)

    # Case/Control label
    out['Level3_CaseControl'] = "Control"
    out.loc[out['diagnosis'] == "Case", 'Level3_CaseControl'] = "Case_Excluded"
    out.loc[out['Level2_FirstDx'].apply(lambda xs: isinstance(xs, list) and len(xs) > 0), 'Level3_CaseControl'] = "Case"

    # Drop temps if present
    out.drop(columns=[c for c in cols_to_drop if c in out.columns], inplace=True, errors='ignore')

    return out

#input example: data, exclusion_type, "DUD", "DUD_Dates", "DUD_Inflicted_changes", 7, "Level2_diagnoses", "Level2_dates", "diagnoses_Level2_modifier", "date_Level2_modifier", 'disorder_Level2_modifier')
def update_DxDates_multi_exclusion(
    all_data: pd.DataFrame,
    exclusion_type: str,
    exc_diag: str,
    exc_diag_date: str,
    exc_diag_inflicted_changes: str,
    diag_excode: Any,
    level2codes: str,
    level2dates: str,
    level2datemodifiercodes: str,
    level2datemodifierdates: str,
    level2datemodifierDXs: str,
    *,
    date_format: str = "%Y-%m-%d",
    verbose: bool = False,
) -> pd.DataFrame:
    """Optimized, date-format-aware version.

    - Accepts list or comma-separated strings in inputs; writes lists (object dtype).
    - Works with any parsable date format; output dates formatted via `date_format`.
    - Label-safe indexing; no reliance on RangeIndex.
    - Avoids `.apply` over entire DataFrame where possible.
    """

    def _as_list(x: Any) -> List[Any]:
        # Normalize value to list and drop empty strings.
        if isinstance(x, list):
            seq = x
        elif pd.isna(x) or x == "":
            seq = []
        elif isinstance(x, (tuple, set)):
            seq = list(x)
        else:
            # if original storage was comma-separated strings
            if isinstance(x, str) and "," in x:
                seq = [s for s in (i.strip() for i in x.split(",")) if s != ""]
            else:
                seq = [x]
        return [item for item in seq if str(item).strip() != ""]


    def _to_dt_list(x: Any) -> List[pd.Timestamp]:
        # Convert values to list[Timestamp], robust to scalars, lists, comma-strings.
        arr = _as_list(x)
        if not arr:
            return []
        # Let pandas parse heterogeneous date formats; coerce invalids to NaT and drop
        ts = pd.to_datetime(arr, errors="coerce")
        return [t for t in ts if not pd.isna(t)]


    def _keep_indices_1yprior(level2_dates: List[pd.Timestamp], exc_dates: List[pd.Timestamp]) -> Tuple[List[int], List[int], List[int], List[int]]:
        # Return (mod_pos, keep_pos, modifier_pos_exc, mod_pos_original_rule_same_as_level2).
        if not level2_dates or not exc_dates:
            return [], list(range(len(level2_dates))), [], []

        exc_arr = np.array(exc_dates, dtype="datetime64[ns]")
        mod_pos, keep_pos, mod_exc = [], [], set()

        for i, d in enumerate(level2_dates):
            diffs = (np.array(d, dtype="datetime64[ns]") - exc_arr).astype("timedelta64[D]").astype(int)
            ok = (diffs <= 365) & (diffs >= 0)
            if ok.any():
                mod_pos.append(i)
                mod_exc.update(np.where(ok)[0].tolist())
            else:
                keep_pos.append(i)

        return mod_pos, keep_pos, sorted(mod_exc), mod_pos


    def _keep_indices_post(level2_dates: List[pd.Timestamp], exc_dates: List[pd.Timestamp]) -> Tuple[List[int], List[int], List[int], Optional[pd.Timestamp]]:
        if not exc_dates:
            return [], list(range(len(level2_dates))), [], None
        earliest = min(exc_dates)
        mod = [i for i, d in enumerate(level2_dates) if d >= earliest]
        keep = [i for i, d in enumerate(level2_dates) if d < earliest]
        mod_exc = [i for i, d in enumerate(exc_dates) if d == earliest]
        return mod, keep, mod_exc, earliest
    diag_inflicted_codes = f"{exc_diag_inflicted_changes}_MDD_codes"
    diag_inflicted_dates = f"{exc_diag_inflicted_changes}_MDD_dates"
    diag_inflicted_N = f"{exc_diag_inflicted_changes}_MDD_dXnumber"
    diag_lost_due_to_exc = f"[update_DxDates_multi_exclusion] MDD_diagnoses_in_percent_lost_due_to_{exc_diag}"

    out = all_data.copy()

    # Ensure list/object columns exist
    list_cols = [
        level2codes,
        level2dates,
        level2datemodifiercodes,
        level2datemodifierdates,
        level2datemodifierDXs,
        "in_dates",
        "diagnoses",
        "Level2_ExclusionReason",
        diag_inflicted_codes,
        diag_inflicted_dates,
    ]
    for col in list_cols:
        if col not in out.columns:
            out[col] = pd.Series([[] for _ in range(len(out))], dtype=object)
        elif out[col].dtype != object:
            out[col] = out[col].astype(object)

    # Ensure numeric columns
    for col, default in {
        diag_inflicted_N: 0,
        exc_diag_inflicted_changes: 0,
        diag_lost_due_to_exc: 0.0,
    }.items():
        if col not in out.columns:
            out[col] = default

    # Rows to process
    mask = (out[exc_diag].astype(str) != "") & (out["diagnoses"].astype(str) != "")
    if not mask.any():
        if verbose:
            logger.debug(f"[update_DxDates_multi_exclusion] No rows to process for {exc_diag}.")
        return out

    idx_labels = out.index[mask].tolist()

    # Per-row processing
    for idx in idx_labels:
        row = out.loc[idx]

        exc_dx_list = _as_list(row[exc_diag])
        exc_date_list = _to_dt_list(row[exc_diag_date])
        lvl2_codes = _as_list(row[level2codes])
        lvl2_dates = _to_dt_list(row[level2dates])
        in_diags = _as_list(row["diagnoses"])
        in_dates = _to_dt_list(row["in_dates"])

        # choose rule
        exclusion_type_l = str(exclusion_type).lower()
        if exclusion_type_l == "1yprior":
            mod_pos, keep_pos, mod_pos_exc, mod_pos_orig = _keep_indices_1yprior(lvl2_dates, exc_date_list)
        elif exclusion_type_l == "post":
            mod_pos, keep_pos, mod_pos_exc, earliest = _keep_indices_post(lvl2_dates, exc_date_list)
            mod_pos_orig = [i for i, d in enumerate(in_dates) if earliest is not None and d >= earliest]
        elif exclusion_type_l == "lifetime":
            mod_pos = list(range(len(lvl2_dates)))
            keep_pos = []
            mod_pos_exc = list(range(len(exc_date_list)))
            mod_pos_orig = list(range(len(in_dates)))
        else:
            if verbose:
                logger.debug(f"[update_DxDates_multi_exclusion] WARNING: unknown exclusion_type={exclusion_type}")
            continue

        # Update Level2 (keep positions)
        out.at[idx, level2codes] = [lvl2_codes[j] for j in keep_pos if j < len(lvl2_codes)]
        out.at[idx, level2dates] = [lvl2_dates[j].strftime(date_format) for j in keep_pos if j < len(lvl2_dates)]

        # Update modifier dates/codes/DX labels (append)
        exist_mod_dates = _as_list(row[level2datemodifierdates])
        exist_mod_codes = _as_list(row[level2datemodifiercodes])
        exist_mod_dxs = _as_list(row[level2datemodifierDXs])

        mod_dates_add = [exc_date_list[j].strftime(date_format) for j in mod_pos_exc if j < len(exc_date_list)]
        mod_codes_add = [exc_dx_list[j] for j in mod_pos_exc if j < len(exc_dx_list)]
        mod_dxs_add = [exc_diag] * len(mod_pos_exc)

        out.at[idx, level2datemodifierdates] = [d for d in (exist_mod_dates + mod_dates_add) if str(d).strip() != ""]
        out.at[idx, level2datemodifiercodes] = [c for c in (exist_mod_codes + mod_codes_add) if str(c).strip() != ""]
        out.at[idx, level2datemodifierDXs] = [x for x in (exist_mod_dxs + mod_dxs_add) if str(x).strip() != ""]

        # Inflicted changes on original diagnoses
        if row.get("diagnosis", "") == "Case" and mod_pos_orig:
            out.at[idx, exc_diag_inflicted_changes] = len(mod_pos_orig)
        else:
            out.at[idx, exc_diag_inflicted_changes] = 0

        out.at[idx, diag_inflicted_dates] = [in_dates[j].strftime(date_format) for j in mod_pos_orig if j < len(in_dates)]
        out.at[idx, diag_inflicted_codes] = [in_diags[j] for j in mod_pos_orig if j < len(in_diags)]
        out.at[idx, diag_inflicted_N] = len(in_dates) if (str(row.get(exc_diag, "")).strip() != "" and row.get("diagnosis", "") == "Case") else 0
        out.at[idx, diag_lost_due_to_exc] = (len(mod_pos_orig) / max(len(in_dates), 1)) * 100.0 if in_dates else 0.0

        if verbose and (idx_labels.index(idx) % 500 == 0):
            logger.debug(f"[update_DxDates_multi_exclusion] Processed {idx_labels.index(idx)+1}/{len(idx_labels)} rows for {exc_diag}.")

    # Append exclusion reason
    sel = out.index.isin(idx_labels) & (out.get("diagnosis", pd.Series(index=out.index)) == "Case")
    if sel.any():
        out.loc[sel, "Level2_ExclusionReason"] = out.loc[sel, "Level2_ExclusionReason"].apply(lambda x: (_as_list(x) + [diag_excode]))

    return out

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='Extracts a Phenotype from input files based on IIDs and diagnostic codes.\nThe best way to start, is to generate a test dataset: \'python get_phenotype.py -g \"\" -o ./ --BuildTestSet\' and then run \'python get_phenotype.py -g \"\" -o testrun.tsv --eM --ExDepExc --testRun\' ')
    parser.add_argument('--ini', required=False, default = '', help='Load an ini file that contains your data sources. Default: "%(default)s"')
    parser.add_argument('-g', required=True, help='File with all Diagnostic codes to export')
    parser.add_argument('-o', required=True, help='Outfile name; dont forget to add the location, otherwise it will be saved in the local dir.') 
    parser.add_argument('-f', required=False, default='', help='Diagnoses file. Should have at least \"IID\", \"date_in\", \"date_out\", and \"diagnosis\" as columns. Names can be defined using --iidcol, --din, --don, and --fcol. No default. Will be automatically determined if not entered for GenomeDK and CHB/DBDS (DEGEN protocol)') 
    parser.add_argument('--f2', required=False, default='', help='Secondary diagnosis file. This is meant to be used if you have files with secondary diagnosis information as i.e. in Denmark. Here we would usually use the \'recnum\' to merge these entries with \'-f\'. These files also contain less information as the \'-f\' files. Should have at least \"recnum\", and \"diagnosis\" as columns. Names can be defined using --recnum2 and --f2col. The recnum column in the \'-f\' files can be specified using --recnum. Default: "%(default)s"') 
    parser.add_argument('--atc', required=False, default='', help='PRescription information file. Will be handeld simillar to -f but needs additional information to be specified: --atccol. Default: "%(default)s"') 
    parser.add_argument('-i', required=False, default='', help='This file should adds based on a mapping using the supplied \"IID\" column information about Gender/Sex (needed), Brithdate (needed) ... e.g., information from stamdata file. No default. Will be automatically determined if not entered for GenomeDK and CHB/DBDS (DEGEN protocol)')
    parser.add_argument('-j', required=False, default='', help='This file should have additional \"IID\" information e.g. date of entry to cohort. This is not needed and can also be skipped. No default. Will be automatically determined if not entered for GenomeDK and CHB/DBDS (DEGEN protocol)') 
    parser.add_argument('--ge', required=False, default='', help='General exclusion list. This referrs to a list of IIDs that should be excluded from the study. Within CHB/DBDS it will default to /data/preprocessed/genetics/chb_degen_freeze_20210503/degen_exclusion_latest') 
    parser.add_argument('--qced',required=False, default='', help='List with all IIDs that pass initial QC. Default: "%(default)s" (or \'/data/preprocessed/genetics/chb_degen_freeze_20210503/DEGEN_GSA_FINAL.fam\' in CHB/DBDS)')
    parser.add_argument('--name', required=False, default='MainPheno', help='Define your main Phenotype name. Defaults to "%(default)s"')
    parser.add_argument('--fcol', required=False, default="c_adiag", help='Columname of -f and -i file to be mapped. Defaulting to "%(default)s" (or diagnosis in CHB/DBDS)')
    parser.add_argument('--gcol', required=False, default="c_adiag", help='Columname of -g file to be mapped against. Defaulting to "%(default)s" (or diagnosis in CHB/DBDS)') 
    parser.add_argument('--iidcol', required=False, default="pnr", help='Columname of IDs in -f and -i file. Defaulting to "%(default)s" (or cpr_enc in CHB/DBDS)')
    parser.add_argument('--bdcol', required=False, default="birthdate", help='Columname of Birthdate in files. Defaults to "%(default)s"')
    parser.add_argument('--sexcol', required=False, default="sex", help='Columname of Sex/Gender in files. Defaults to "%(default)s"')
    parser.add_argument('--atccol', required=False, default="", help='Columname of ATC codes in files. Defaults to "%(default)s"')
    parser.add_argument('--atcdatecol', required=False, default="", help='Columname of ATC prescription date column in the --atc file(s). Defaults to "%(default)s"')
    parser.add_argument('--fsep', required=False, default=",", help='Separator of -f i.e. tab; default "%(default)s" (or "\\t\" in CHB/DBDS)')
    parser.add_argument('--isep', required=False, default=",", help='Separator of -i i.e. tab; default "%(default)s" (or "\\t\" in CHB/DBDS)')
    parser.add_argument('--jsep', required=False, default=",", help='Separator of -j i.e. tab; default "%(default)s" (or "\\t\" in CHB/DBDS)')
    parser.add_argument('--gsep', required=False, default=",", help='Separator of -g i.e. tab; default "%(default)s" (or "\\t\" in CHB/DBDS)')
    parser.add_argument('--ophsep', required=False, default=",", help='Separator of Ophold file - currently only available on CHB/DBDS,DST i.e. "\\t\"; default "%(default)s" (or "\\t\" in CHB/DBDS)')
    parser.add_argument('--din',required=False, default='d_inddto', help='Columname of first diagnosis date. e.g. \'d_inddto\' or \'date_in\'. Default: "%(default)s" (or \'date_in\' in CHB/DBDS)')
    parser.add_argument('--don',required=False, default='d_uddto', help='Columname of first diagnosis date. e.g. \'d_uddto\' or \'date_out\'. Default: "%(default)s" (or \'date_out\' in CHB/DBDS)')
    parser.add_argument('--recnum',required=False, default='', help='Columname of the recnum field in -f files. Default: "%(default)s" (or \'recnum\' on NCRR)')
    parser.add_argument('--recnum2',required=False, default='', help='Columname of the recnum field in -f files. Default: "%(default)s" (or \'recnum\' on NCRR)')
    parser.add_argument('--f2col',required=False, default='c_adiag', help='Columname of first diagnosis date. e.g. \'d_uddto\' or \'date_out\'. Default: "%(default)s" (or \'date_out\' in CHB/DBDS)')
    parser.add_argument('--ExDepExc', action='store_true' , help='List of diagnostic codes to exclude (e.g. Cases holding also these codes will be excluded). This is currently precoded and will be changed to allowing to specify a file with codes.') 
    parser.add_argument('--eM', action='store_true', help='Exact Match. If the ICD or diagnosis code should max exactly and not e.g. searching for ICD:F33 and receiving ICD:F331, ICD:F33, ICD:F339 and so on.')
    parser.add_argument('--noLeadingICD', action='store_true', help='Set this, if your diagnostic codes in \"-g\" do have a leading ICD*:, but your \"-f\" does not. This will only take affect together with --ExDepExc.')
    parser.add_argument('--ICDCM', action='store_true', help='Set this, if your Cohort is based on ICD10/9-CM codes and not base WHO. This will only take affect together with --ExDepExc.')
    parser.add_argument('--ICD8', action='store_true', help='Set this, if you only want to export ICD8 codes. This will only take affect together with --ExDepExc.')
    parser.add_argument('--ICD9', action='store_true', help='Set this, if you only want to export ICD9/9-CM codes. This will only take affect together with --ExDepExc.')
    parser.add_argument('--ICD10', action='store_true', help='Set this, if you only want to export ICD10/10-CM codes. This will only take affect together with --ExDepExc.')
    parser.add_argument('--icdprefix',required=False, default='', help='If your input ICD codes are i.e. ICD10:F32 or F32 and you need them to be i.e. ICD10:DF32 or DF32, use this argument with \'D\'. Default: "%(default)s"')
    parser.add_argument('--iidstatus', default='', help='Information about the status of the IID, i.e. if they withdrew their consent (), moved outside the country (), or died (). Default: "%(default)s"'),
    parser.add_argument('--iidstatusdate', default='', help='Date columname for the --iidstatus information. Default: "%(default)s"'),
    parser.add_argument('--selectIIDs', default='', help='Input file that lists IIDs to use in the analysis. This file should have one IID per row and NO HEADER. Default: "%(default)s"'),
    parser.add_argument('--DiagTypeExclusions', default='', help='List of diagnostic types to exclude. e.g. \"H,M\". Potential c_types may be +,A,B,C,G,M,H. \nA (main diagnosis), B (secondary diagnosis), G (grundmorbus), H (referral), + (secondary diagnosis), C (complication), and M (temporary) are all possible diagnosis types.\nFrom 1995 to 1998, registering a referral diagnosis (H) was optional; thereafter, it became mandatory for certain referral methods. Grundmorbus (G) was optional from 1995 to 2001, used exclusively for psychiatric patients in 20022003, and then discontinued entirely. Complication (C) and temporary diagnosis (M) were both discontinued at the end of 2013. \nDefault: "%(default)s"'),
    parser.add_argument('--DiagTypeInclusions', default='', help='List of diagnostic types to include. e.g. \"A,B\". Potential c_types may be +,A,B,C,G,M,H. \nA (main diagnosis), B (secondary diagnosis), G (grundmorbus), H (referral), + (secondary diagnosis), C (complication), and M (temporary) are all possible diagnosis types.\nFrom 1995 to 1998, registering a referral diagnosis (H) was optional; thereafter, it became mandatory for certain referral methods. Grundmorbus (G) was optional from 1995 to 2001, used exclusively for psychiatric patients in 20022003, and then discontinued entirely. Complication (C) and temporary diagnosis (M) were both discontinued at the end of 2013. \nDefault: "%(default)s"'),
    parser.add_argument('--DiagTypecol', default='c_types', help='Columname of diagnostic types to include/exclude; see --DiagTypeInclusions and --DiagTypeExclusions. \nDefault: "%(default)s"'),
    parser.add_argument('--LifetimeExclusion', default='', help='Define Lifetime exclusions (if a case has any of the listed codes, it will be excluded). This should be a file containing either one row with all codes listed (comma separated) or per row a exclusion name, followed by a list of diagnostic codes (similar to the input). e.g. "BPD\tICD10:F30,ICD10:F30.0,ICD10:F30.1,ICD10:F30.2,ICD10:F30.8,ICD10:F30.9,ICD10:F31,ICD10:F31.0,ICD10:F31.1,ICD10:F31.2,ICD10:F31.3,ICD10:F31.4,ICD10:F31.5,ICD10:F31.6,ICD10:F31.7,ICD10:F31.8,ICD10:F31.9"\nIf you are using CHB/DBDS data, please remember, that the ICD10 codes have to start with ICD10:D***, e.g. ICD10:DF33.0. Default: "%(default)s"'),
    parser.add_argument('--PostExclusion', default='', help='Define Post exclusions (if a case has any of the listed codes, all main diagnoses after the first occuring date of the listed codes will be excluded). This should be a file containing either one row with all codes listed (comma separated) or  per row a exclusion name, followed by a list of diagnostic codes (similar to the input). Default: "%(default)s"'),
    parser.add_argument('--OneyPriorExclusion', default='', help='Define One Year Prio exclusions (all entries for a case that happen within one year prior to any date of the listed codes, these entries will be excluded). This should be a file containing either one row with all codes listed (comma separated) or  per row a exclusion name, followed by a list of diagnostic codes (similar to the input). Default: "%(default)s"'),
    parser.add_argument('--fDates', default=[], help='Which columns in your -f file are Dates? This automatically uses the columns you supplied under --bdcol, --don and --din. You only need to specify this, if you have additonal Dates that should be reformatted. Specify as comma separated list without spaces. Default: "%(default)s"'),
    parser.add_argument('--iDates', default=[], help='Which columns in your -i file are Dates? This automatically uses the columns you supplied under --bdcol, --don and --din. You only need to specify this, if you have additonal Dates that should be reformatted. Specify as comma separated list without spaces. Default: "%(default)s"'),
    parser.add_argument('--atcDates', default=[], help='Which columns in your --atc file are Dates? This automatically uses the columns you supplied under --bdcol, --don and --din. You only need to specify this, if you have additonal Dates that should be reformatted. Specify as comma separated list without spaces. Default: "%(default)s"'),
    parser.add_argument('--DateFormat',required=False, default="%d/%m/%Y", help='Format of Dates in your data. Default "%(default)s" --> 31/01/1980')
    parser.add_argument('--MinMaxAge', default='0,0', help='Filter by Min and Max Age at first diagnosis. This should be given as comma separated numerics in the form of x,y; i.e. 18,50 or 17.99,50.01, 0,0 (no exclusion) and is interpreted as inclusion as case if first diagnosis age >x and <y. Default: "%(default)s"'),
    parser.add_argument('--Fyob', default='', help='Filter by Year of Birth. Everyone before this date will be excluded. Date needs to be given in the following format: \"YYYY-MM-DD\". Default: "%(default)s"'),
    parser.add_argument('--Fgender', default='', help='Filter by gender. Only Individuals with the selected Gender (\"F\" or \"M\") will be included in the output. Default: "%(default)s"'),
    parser.add_argument('--eCc', action='store_true', help='Exclude Individuals that are identified as Controls and are only part of CHB and not DBDS. This flag only works when ran on CHB/DBDS cluster.'),
    parser.add_argument('--removePointInDiagCode', action='store_true', help='If your supplied diagnostic codes contain a point but your -f file does not')
    parser.add_argument('--skipICDUpdate', action='store_true', help='If your supplied diagnostic codes are already in line with your datastructure, you can use this toggle to skip the updating (highly recommended if your input is in a correct format!)')
    parser.add_argument('--MatchFI', action='store_true', help='If you want to keep only the IIDs overlapping between -g and -f use this flag.')
    parser.add_argument('--BuildEntryExitDates',action='store_true', help='If you want to extract additional information about the controls. This basically adds first and last date of observation (any diagnostic code) and writes it to first in date, last out date, in_dates, and out_dates. NB: This can be used if you don\'t have an Entry and Exit date known. Be aware, this will increase time needed for calculation.')
    parser.add_argument('--Ophold', default="", help='If you want to load and process the ophold file (on IBP Cluster only). Default "%(default)s"')
    parser.add_argument('--BuildOphold', action='store_true', help='If you want to update the ophold file used (on IBP Cluster only).')
    parser.add_argument('--RegisterRun', action='store_true', help='We will try to determine this automatically based on known Servers. If you are using this method on an unknown Server and your diagnostic codes are in the follwing format DYXXxx; where Y stands for the letter of the group and XX for the main and xx for the subcode.')
    parser.add_argument('--lpp', action='store_true', help='Set this if you want to load phenotypes (one phenotype per file) and run our exclusions on them. Keep in mind, that -g will only allow one file to be supplied. All others will need to be supplied through the appropriate exclusion flags.: "%(default)s" (or \'date_out\' in CHB/DBDS)')
    parser.add_argument('--write_pickle', action='store_true', help='If you want to write the results to a pickle format in addition to the standard output.')
    parser.add_argument('--write_fastGWA_format', action='store_true', help='If you want to write the phenotype into fastGWA format.')
    parser.add_argument('--write_Plink2_format', action='store_true', help='If you want to write the phenotype into PLINK2 format.')
    parser.add_argument('--BuildTestSet', action='store_true', help='Build the test set that can be used to see an example of the input data or to test if your setup runs smoothly'),
    parser.add_argument('--testRun', action='store_true', help='Run only on smaller test data input (only available for the test dataset)'),
    parser.add_argument('--nthreads', default=8, help='DEPRECATED! - How many threads should be used. Default: "%(default)s"'),
    parser.add_argument('--lowmem', action='store_true', help='Experimental! - This will devide the LPR file into groups of each 100.000 individuals, run the phenotype on them, save it to file and then run the nex 100k until the end. This will increase the runtime.'),
    parser.add_argument('--batchsize', required=False, default=100000, help='Experimental! - This will set the batches (when --lowmem is set) to the desired value. Default: "%(default)s"'),
    parser.add_argument('--PSYK', action='store_true', help='Experimental! - To run only based on the PSYK diagnoses.'),
    parser.add_argument('--LPR', action='store_true', help='Experimental! - To run only based on the LPR diagnoses.'),
    parser.add_argument('--BuildIndex', action='store_true', help='Build an initial index file. This applies to -f, and --atc files. Existing Index files will be overwritten.'),
    parser.add_argument('--IndexDtypes', required=False, default='{"iidcol":"int","c_pattype":"float","c_adiag":"string","c_diagtype": "string","register":"string","source":"string","d_inddto":"datetime64[ns]","d_uddto":"datetime64[ns]"}', help='Set the dtype dict for your file that should be indexed. Default: "%(default)s"')
    parser.add_argument('--verbose', action='store_true', help='Verbose output')

    args = parser.parse_args()

    argstring = ""
    default_argstring = ""
    #for arg, value in vars(args).items():
    #    default_value = parser.get_default(arg)  # Get the default value
    for action in parser._actions:
        if action.dest == 'help':  # Skip the help argument
            continue
        value = getattr(args, action.dest)  # Get the current value
        default_value = action.default  # Get the default value    
        if value != default_value:
            arg_name = action.option_strings[0]  # Use the first option string (e.g., --arg1)
            argstring = argstring+(f"{arg_name}: {value} (default: {default_value})\n")
    default_args = []
    for action in parser._actions:
        if action.dest == 'help':  # Skip the help argument
            continue
        value = getattr(args, action.dest)  # Get the current value
        default_value = action.default  # Get the default value    
        
        if value == default_value:
            default_args.append(action.option_strings[0])
            default_argstring = default_argstring+(f"{action.option_strings[0]}: {value} (default: {default_value})\n")

    main(args.f,args.g,args.i,args.j,args.ExDepExc,args.ge,args.fcol,args.gcol,args.iidcol,args.bdcol,
         args.sexcol,args.fsep,args.isep,args.jsep,args.gsep,args.o,args.eM,args.din,args.don,args.qced,args.DiagTypeExclusions,
         args.DiagTypeInclusions,args.DiagTypecol,args.LifetimeExclusion,args.PostExclusion,args.OneyPriorExclusion,args.eCc,
         args.Fyob,args.Fgender,args.verbose,args.BuildTestSet,args.testRun,args.MatchFI,args.skipICDUpdate,
         args.DateFormat,args.iidstatus,args.iidstatusdate,args.selectIIDs,args.removePointInDiagCode,args.nthreads,args.name,args.BuildEntryExitDates,
         args.BuildOphold,args.write_pickle, args.write_fastGWA_format, args.write_Plink2_format,args.fDates,args.iDates,
         args.MinMaxAge,args.ICDCM,args.lpp, args.RegisterRun, args.lowmem, args.batchsize, args.noLeadingICD, args.f2, 
         args.recnum, args.recnum2, args.f2col, args.atc, args.atccol, args.atcdatecol, args.atcDates, args.LPR, args.PSYK, 
         args.ophsep, args.Ophold, args.ini, args.ICD8, args.ICD9, args.ICD10, args.BuildIndex, args.IndexDtypes, args.icdprefix, argstring, default_args, default_argstring)

# If wantig to start it locally in python and run through it step by step
'''
lpr_file = ""  # Diagnosis file
pheno_request = "/dpibp/home/mislun/scripts/MDD.request"  # File with all Diagnostic codes to export
stam_file = ""  # File adding information about Gender/Sex, Birthdate, etc.
addition_information_file = ""  # File with additional IID information
use_predefined_exdep_exclusions = ""  # List of diagnostic codes to exclude
general_exclusions = ""  # General exclusion list
diagnostic_col = "c_adiag"  # Column name of -f and -i file to be mapped
pheno_requestcol = "c_adiag"  # Column name of -g file to be mapped against
iidcol = "pnr"  # Column name of IDs in -f and -i file
birthdatecol = "birthdate"  # Column name of Birthdate in files
sexcol = "kqn"  # Column name of Sex/Gender in files
main_pheno_name = "MDD"
fsep = ","  # Separator of -f file
gsep = ","  # Separator of -g file
outfile = "/dpibp/home/mislun/scripts/MDD.result.tsv"  # Outfile name
exact_match = False  # Exact Match flag
input_date_in_name = "d_inddto"  # Column name of first diagnosis date
input_date_out_name = "d_uddto"  # Column name of last diagnosis date
qced_iids = ""  # List with all IIDs that pass initial QC
ctype_excl = ""  # List of diagnostic types to exclude
ctype_incl = ""  # List of diagnostic types to include
lifetime_exclusions_file = ""  # Lifetime exclusions file
post_exclusions_file = ""  # Post exclusions file
oneYearPrior_exclusions_file = ""  # One Year Prior exclusions file
exclCHBcontrols = False  # Exclude CHB controls flag
Filter_YoB = ""  # Filter by Year of Birth
Filter_Gender = ""  # Filter by Gender
verbose = False  # Verbose output flag
Build_Test_Set = False  # Build Test Set flag
test_run = False  # Test Run flag
MatchFI = False  # Match FI flag
skip_icd_update = False  # Skip ICD Update flag
DateFormat_in = "%d/%m/%Y"  # Format of Dates in your data
iidstatus_col = ""  # Column name of IID status
remove_point_in_diag_request = False  # Remove point in diag request flag
num_threads = 8  # Number of threads to be used
'''