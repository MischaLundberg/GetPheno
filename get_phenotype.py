#!/usr/bin/env python

import os 
import re
import argparse 
import uuid
import sys
import psutil
import gc
import pandas as pd
from pandas.errors import ParserWarning
import warnings
import numpy as np
from datetime import datetime, timedelta
import socket
import random
from concurrent.futures import ThreadPoolExecutor
import pickle
import subprocess
from packaging.version import Version, parse

python_min_version = "3.8.1"
pd_min_version = "1.3.4"
DateFormat = "%Y-%m-%d"
ATC_Requested = "NotSet"

#################################################################
#===============================================================#
#################################################################

BP_MDD_Codes = ["ICD10:F32", "ICD10:F32.0", "ICD10:F32.1", "ICD10:F32.2", "ICD10:F32.3", "ICD10:F32.8", "ICD10:F32.9", "ICD10:F33", "ICD10:F33.0", 
"ICD10:F33.1", "ICD10:F33.2", "ICD10:F33.3", "ICD10:F33.4", "ICD10:F33.8", "ICD10:F33.9", "ICD10-CM:F32", "ICD10-CM:F32.0", "ICD10-CM:F32.1", 
"ICD10-CM:F32.2", "ICD10-CM:F32.3", "ICD10-CM:F32.8", "ICD10-CM:F32.9", "ICD10-CM:F33", "ICD10-CM:F33.0", "ICD10-CM:F33.1", "ICD10-CM:F33.2", 
"ICD10-CM:F33.3", "ICD10-CM:F33.4", "ICD10-CM:F33.8", "ICD10-CM:F33.9", "ICD9:296.1", "ICD9:296.10", "ICD9:296.11", "ICD9:296.12", "ICD9:296.13", 
"ICD9:296.14", "ICD9:296.15", "ICD9:296.16", "ICD9:296.2", "ICD9:296.20", "ICD9:296.21", "ICD9:296.22", "ICD9:296.23", 
"ICD9:296.24", "ICD9:296.25", "ICD9:296.26", "ICD9:298.0", "ICD9:300.4", "ICD9-CM:296.1", "ICD9-CM:296.10", "ICD9-CM:296.11", "ICD9-CM:296.12", "ICD9-CM:296.13", 
"ICD9-CM:296.14", "ICD9-CM:296.15", "ICD9-CM:296.16", "ICD9-CM:296.2", "ICD9-CM:296.20", "ICD9-CM:296.21", "ICD9-CM:296.22", "ICD9-CM:296.23", "ICD9-CM:296.24", 
"ICD9-CM:296.25", "ICD9-CM:296.26", "ICD9-CM:298.0", "ICD9-CM:300.4", "ICD8:296.2", "ICD8:298.0", "ICD8:300.4"]

# Intellectual Disability / Mental retardation (ID)
#ID_Codes = ["F70", "F70.0", "F70.1", "F70.8", "F70.9", "F71", "F71.0", "F71.1", "F71.8", "F71.9", "F72", "F72.0", "F72.1", "F72.8", "F72.9", "F73", "F73.0", "F73.1", "F73.8", "F73.9", "F74", "F74.0", "F74.1", "F74.8", "F74.9", "F75", "F75.0", "F75.1", "F75.8", "F75.9", "F76", "F76.0", "F76.1", "F76.8", "F76.9", "F77", "F77.0", "F77.1", "F77.8", "F77.9", "F78", "F78.0", "F78.1", "F78.8", "F78.9", "F79", "F79.0", "F79.1", "F79.8", "F79.9", 310, 310.0, 310.1, 310.2, 310.3, 310.4, 310.5, 310.7, 310.8, 310.9, 311, 311.0, 311.1, 311.2, 311.3, 311.4, 311.5, 311.7, 311.8, 311.9, 312, 312.0, 312.1, 312.2, 312.3, 312.4, 312.5, 312.7, 312.8, 312.9, 313, 313.0, 313.1, 313.2, 313.3, 313.4, 313.5, 313.7, 313.8, 313.9, 314, 314.0, 314.1, 314.2, 314.3, 314.4, 314.5, 314.7, 314.8, 314.9, 315, 315.0, 315.1, 315.2, 315.3, 315.4, 315.5, 315.7, 315.8, 315.9, 310.00, 310.10, 310.20, 310.30, 310.40, 310.50, 310.70, 310.80, 310.90, 311.00, 311.10, 311.20, 311.30, 311.40, 311.50, 311.70, 311.80, 311.90, 312.00, 312.10, 312.20, 312.30, 312.40, 312.50, 312.70, 312.80, 312.90, 313.00, 313.10, 313.20, 313.30, 313.40, 313.50, 313.70, 313.80, 313.90, 314.00, 314.10, 314.20, 314.30, 314.40, 314.50, 314.70, 314.80, 314.90, 315.00, 315.10, 315.20, 315.30, 315.40, 315.50, 315.70, 315.80, 315.90]
ID_Codes = ["ICD10:F70", "ICD10:F70.0", "ICD10:F70.1", "ICD10:F70.8", "ICD10:F70.9", "ICD10:F71", "ICD10:F71.0", "ICD10:F71.1", 
    "ICD10:F71.8", "ICD10:F71.9", "ICD10:F72", "ICD10:F72.0", "ICD10:F72.1", "ICD10:F72.8", "ICD10:F72.9", "ICD10:F73", "ICD10:F73.0", 
    "ICD10:F73.1", "ICD10:F73.8", "ICD10:F73.9", "ICD10:F74", "ICD10:F74.0", "ICD10:F74.1", "ICD10:F74.8", "ICD10:F74.9", "ICD10:F75", 
    "ICD10:F75.0", "ICD10:F75.1", "ICD10:F75.8", "ICD10:F75.9", "ICD10:F76", "ICD10:F76.0", "ICD10:F76.1", "ICD10:F76.8", "ICD10:F76.9", 
    "ICD10:F77", "ICD10:F77.0", "ICD10:F77.1", "ICD10:F77.8", "ICD10:F77.9", "ICD10:F78", "ICD10:F78.0", "ICD10:F78.1", "ICD10:F78.8", 
    "ICD10:F78.9", "ICD10:F79", "ICD10:F79.0", "ICD10:F79.1", "ICD10:F79.8", "ICD10:F79.9", "ICD10-CM:F70", "ICD10-CM:F70.0", "ICD10-CM:F70.1", 
    "ICD10-CM:F70.8", "ICD10-CM:F70.9", "ICD10-CM:F71", "ICD10-CM:F71.0", "ICD10-CM:F71.1", "ICD10-CM:F71.8", "ICD10-CM:F71.9", "ICD10-CM:F72", 
    "ICD10-CM:F72.0", "ICD10-CM:F72.1", "ICD10-CM:F72.8", "ICD10-CM:F72.9", "ICD10-CM:F73", "ICD10-CM:F73.0", "ICD10-CM:F73.1", "ICD10-CM:F73.8",
    "ICD10-CM:F73.9", "ICD10-CM:F74", "ICD10-CM:F74.0", "ICD10-CM:F74.1", "ICD10-CM:F74.8", "ICD10-CM:F74.9", "ICD10-CM:F75", "ICD10-CM:F75.0", 
    "ICD10-CM:F75.1", "ICD10-CM:F75.8", "ICD10-CM:F75.9", "ICD10-CM:F76", "ICD10-CM:F76.0", "ICD10-CM:F76.1", "ICD10-CM:F76.8", "ICD10-CM:F76.9", 
    "ICD10-CM:F77", "ICD10-CM:F77.0", "ICD10-CM:F77.1", "ICD10-CM:F77.8", "ICD10-CM:F77.9", "ICD10-CM:F78", "ICD10-CM:F78.0", "ICD10-CM:F78.1", 
    "ICD10-CM:F78.8", "ICD10-CM:F78.9", "ICD10-CM:F79", "ICD10-CM:F79.0", "ICD10-CM:F79.1", "ICD10-CM:F79.8", "ICD10-CM:F79.9", "ICD9:317", 
    "ICD9:318.0", "ICD9:318.1", "ICD9:318.2", "ICD9:319", "ICD9-CM:317", "ICD9-CM:318.0", "ICD9-CM:318.1", "ICD9-CM:318.2", "ICD9-CM:319", 
    "ICD8:310", "ICD8:310.0", "ICD8:310.1", "ICD8:310.2", "ICD8:310.3", "ICD8:310.4", "ICD8:310.5", "ICD8:310.7", "ICD8:310.8", "ICD8:310.9", 
    "ICD8:311", "ICD8:311.0", "ICD8:311.1", "ICD8:311.2", "ICD8:311.3", "ICD8:311.4", "ICD8:311.5", "ICD8:311.7", "ICD8:311.8", "ICD8:311.9", 
    "ICD8:312", "ICD8:312.0", "ICD8:312.1", "ICD8:312.2", "ICD8:312.3", "ICD8:312.4", "ICD8:312.5", "ICD8:312.7", "ICD8:312.8", "ICD8:312.9", 
    "ICD8:313", "ICD8:313.0", "ICD8:313.1", "ICD8:313.2", "ICD8:313.3", "ICD8:313.4", "ICD8:313.5", "ICD8:313.7", "ICD8:313.8", "ICD8:313.9", 
    "ICD8:314", "ICD8:314.0", "ICD8:314.1", "ICD8:314.2", "ICD8:314.3", "ICD8:314.4", "ICD8:314.5", "ICD8:314.7", "ICD8:314.8", "ICD8:314.9", 
    "ICD8:315", "ICD8:315.0", "ICD8:315.1", "ICD8:315.2", "ICD8:315.3", "ICD8:315.4", "ICD8:315.5", "ICD8:315.7", "ICD8:315.8", "ICD8:315.9",
    ]

# Schizophrenia (SCZ)
#SCZ_Codes = ["F20", "F20.0", "F20.1", "F20.2", "F20.3", "F20.5", "F20.6", "F20.8", "F20.9", 295, 295.0, 295.1, 295.2, 295.3, 295.5, 295.6, 295.8, 295.9, 295.00, 295.10, 295.20, 295.30, 295.50, 295.60, 295.80, 295.90]
SCZ_Codes =  ["ICD10:F20", "ICD10:F20.0", "ICD10:F20.1", "ICD10:F20.2", "ICD10:F20.3", "ICD10:F20.5", "ICD10:F20.6", "ICD10:F20.8", "ICD10:F20.9",
    "ICD10-CM:F20", "ICD10-CM:F20.0", "ICD10-CM:F20.1", "ICD10-CM:F20.2", "ICD10-CM:F20.3", "ICD10-CM:F20.5", "ICD10-CM:F20.6", "ICD10-CM:F20.8", 
    "ICD10-CM:F20.9", "ICD9:295", "ICD9:295.0", "ICD9:295.00", "ICD9:295.01", "ICD9:295.02", "ICD9:295.03", "ICD9:295.04", "ICD9:295.05", 
    "ICD9:295.1", "ICD9:295.10", "ICD9:295.11", "ICD9:295.12", "ICD9:295.13", "ICD9:295.14", "ICD9:295.15", "ICD9:295.2", "ICD9:295.20", 
    "ICD9:295.21", "ICD9:295.22", "ICD9:295.23", "ICD9:295.24", "ICD9:295.25", "ICD9:295.3", "ICD9:295.30", "ICD9:295.31", "ICD9:295.32", 
    "ICD9:295.33", "ICD9:295.34", "ICD9:295.35", "ICD9:295.40", "ICD9:295.41", "ICD9:295.42", "ICD9:295.43", "ICD9:295.44", "ICD9:295.45", 
    "ICD9:295.5", "ICD9:295.50", "ICD9:295.51", "ICD9:295.52", "ICD9:295.53", "ICD9:295.54", "ICD9:295.55", "ICD9:295.6", "ICD9:295.60", "ICD9:295.61", 
    "ICD9:295.62", "ICD9:295.63", "ICD9:295.64", "ICD9:295.65", "ICD9:295.7", "ICD9:295.70", "ICD9:295.71", "ICD9:295.72", "ICD9:295.73", "ICD9:295.74", 
    "ICD9:295.75", "ICD9:295.8", "ICD9:295.80", "ICD9:295.81", "ICD9:295.82", "ICD9:295.83", "ICD9:295.84", "ICD9:295.85", "ICD9:295.9", "ICD9:295.90", 
    "ICD9:295.91", "ICD9:295.92", "ICD9:295.93", "ICD9:295.94", "ICD9:295.95", "ICD9-CM:295", "ICD9-CM:295.0", "ICD9-CM:295.00", "ICD9-CM:295.01", 
    "ICD9-CM:295.02", "ICD9-CM:295.03", "ICD9-CM:295.04", "ICD9-CM:295.05", "ICD9-CM:295.1", "ICD9-CM:295.10", "ICD9-CM:295.11", "ICD9-CM:295.12", 
    "ICD9-CM:295.13", "ICD9-CM:295.14", "ICD9-CM:295.15", "ICD9-CM:295.2", "ICD9-CM:295.20", "ICD9-CM:295.21", "ICD9-CM:295.22", "ICD9-CM:295.23", 
    "ICD9-CM:295.24", "ICD9-CM:295.25", "ICD9-CM:295.3", "ICD9-CM:295.30", "ICD9-CM:295.31", "ICD9-CM:295.32", "ICD9-CM:295.33", "ICD9-CM:295.34", 
    "ICD9-CM:295.35", "ICD9-CM:295.40", "ICD9-CM:295.41", "ICD9-CM:295.42", "ICD9-CM:295.43", "ICD9-CM:295.44", "ICD9-CM:295.45", "ICD9-CM:295.5", 
    "ICD9-CM:295.50", "ICD9-CM:295.51", "ICD9-CM:295.52", "ICD9-CM:295.53", "ICD9-CM:295.54", "ICD9-CM:295.55", "ICD9-CM:295.6", "ICD9-CM:295.60", 
    "ICD9-CM:295.61", "ICD9-CM:295.62", "ICD9-CM:295.63", "ICD9-CM:295.64", "ICD9-CM:295.65", "ICD9-CM:295.7", "ICD9-CM:295.70", "ICD9-CM:295.71", 
    "ICD9-CM:295.72", "ICD9-CM:295.73", "ICD9-CM:295.74", "ICD9-CM:295.75", "ICD9-CM:295.8", "ICD9-CM:295.80", "ICD9-CM:295.81", "ICD9-CM:295.82", 
    "ICD9-CM:295.83", "ICD9-CM:295.84", "ICD9-CM:295.85", "ICD9-CM:295.9", "ICD9-CM:295.90", "ICD9-CM:295.91", "ICD9-CM:295.92", "ICD9-CM:295.93", 
    "ICD9-CM:295.94", "ICD9-CM:295.95", "ICD9-CM:V11.0", "ICD8:295", "ICD8:295.0", "ICD8:295.09", "ICD8:295.1", "ICD8:295.19", "ICD8:295.2", 
    "ICD8:295.29", "ICD8:295.3", "ICD8:295.39", "ICD8:295.4", "ICD8:295.49", "ICD8:295.5", "ICD8:295.59", "ICD8:295.6", "ICD8:295.69", 
    "ICD8:295.8", "ICD8:295.89", "ICD8:295.9", "ICD8:295.99"]

# Bipolar Disorder (BPD)
#BPD_Codes = ["F30", "F30.0", "F30.1", "F30.2", "F30.8", "F30.9", "F31", "F31.0", "F31.1", "F31.2", "F31.3", "F31.4", "F31.5", "F31.6", "F31.7", "F31.8", "F31.9", 296.1, 296.3, 296.8, 296.9, 296.10, 296.30, 296.80, 296.90, 298.1, 298.10]
BPD_Codes = ["ICD10:F30", "ICD10:F30.0", "ICD10:F30.1", "ICD10:F30.2", "ICD10:F30.8", "ICD10:F30.9", "ICD10:F31", "ICD10:F31.0", "ICD10:F31.1", 
    "ICD10:F31.2", "ICD10:F31.3", "ICD10:F31.4", "ICD10:F31.5", "ICD10:F31.6", "ICD10:F31.7", "ICD10:F31.8", "ICD10:F31.9", "ICD10-CM:F30", 
    "ICD10-CM:F30.0", "ICD10-CM:F30.1", "ICD10-CM:F30.2", "ICD10-CM:F30.8", "ICD10-CM:F30.9", "ICD10-CM:F31", "ICD10-CM:F31.0", "ICD10-CM:F31.1", 
    "ICD10-CM:F31.2", "ICD10-CM:F31.3", "ICD10-CM:F31.4", "ICD10-CM:F31.5", "ICD10-CM:F31.6", "ICD10-CM:F31.7", "ICD10-CM:F31.8", "ICD10-CM:F31.9",
    "ICD9:296.0", "ICD9:296.00", "ICD9:296.01", "ICD9:296.02", "ICD9:296.03", "ICD9:296.04", "ICD9:296.05", "ICD9:296.06", "ICD9:296.1", "ICD9:296.10", 
    "ICD9:296.11", "ICD9:296.12", "ICD9:296.13", "ICD9:296.14", "ICD9:296.15", "ICD9:296.16", "ICD9:296.4", "ICD9:296.40", "ICD9:296.41", "ICD9:296.42", 
    "ICD9:296.43", "ICD9:296.44", "ICD9:296.45", "ICD9:296.46", "ICD9:296.5", "ICD9:296.50", "ICD9:296.51", "ICD9:296.52", "ICD9:296.53", "ICD9:296.54", 
    "ICD9:296.55", "ICD9:296.56", "ICD9:296.6", "ICD9:296.60", "ICD9:296.61", "ICD9:296.62", "ICD9:296.63", "ICD9:296.64", "ICD9:296.65", "ICD9:296.66", 
    "ICD9:296.7", "ICD9:296.70", "ICD9:296.8", "ICD9:296.80", "ICD9:296.81", "ICD9:296.82", "ICD9:296.89", "ICD9-CM:296.0", "ICD9-CM:296.00", 
    "ICD9-CM:296.01", "ICD9-CM:296.02", "ICD9-CM:296.03", "ICD9-CM:296.04", "ICD9-CM:296.05", "ICD9-CM:296.06", "ICD9-CM:296.1", "ICD9-CM:296.10", 
    "ICD9-CM:296.11", "ICD9-CM:296.12", "ICD9-CM:296.13", "ICD9-CM:296.14", "ICD9-CM:296.15", "ICD9-CM:296.16", "ICD9-CM:296.4", "ICD9-CM:296.40", 
    "ICD9-CM:296.41", "ICD9-CM:296.42", "ICD9-CM:296.43", "ICD9-CM:296.44", "ICD9-CM:296.45", "ICD9-CM:296.46", "ICD9-CM:296.5", "ICD9-CM:296.50", 
    "ICD9-CM:296.51", "ICD9-CM:296.52", "ICD9-CM:296.53", "ICD9-CM:296.54", "ICD9-CM:296.55", "ICD9-CM:296.56", "ICD9-CM:296.6", "ICD9-CM:296.60", 
    "ICD9-CM:296.61", "ICD9-CM:296.62", "ICD9-CM:296.63", "ICD9-CM:296.64", "ICD9-CM:296.65", "ICD9-CM:296.66", "ICD9-CM:296.7", "ICD9-CM:296.70", 
    "ICD9-CM:296.8", "ICD9-CM:296.80", "ICD9-CM:296.81", "ICD9-CM:296.82", "ICD9-CM:296.89", "ICD8:296.1", "ICD8:296.19", "ICD8:296.2", "ICD8:296.29", 
    "ICD8:296.3", "ICD8:296.39", "ICD8:296.8", "ICD8:296.89", "ICD8:296.9", "ICD8:296.99"]

# Dementia (DEM)
#DEM_Codes = ["F00", "F00.0", "F00.1", "F00.2", "F00.9", "F01.1", "F01.2", "F01.3", "F01.8", "F01.9", "F02.0", "F02.1", "F02.2", "F02.3", "F02.4", "F02.8", "F03", 290, 290.0, 290.1, 293.0, 293.2, 293.4, 293.9, 294.8, 290.00, 290.10, 293.00, 293.20, 293.40, 293.90, 294.80]
DEM_Codes = ["ICD10:F00", "ICD10:F00.0", "ICD10:F00.1", "ICD10:F00.2", "ICD10:F00.9", "ICD10:F01.1", "ICD10:F01.2", "ICD10:F01.3", "ICD10:F01.8", 
    "ICD10:F01.9", "ICD10:F02.0", "ICD10:F02.1", "ICD10:F02.2", "ICD10:F02.3", "ICD10:F02.4", "ICD10:F02.8", "ICD10:F03", "ICD10-CM:F00", "ICD10-CM:F00.0", 
    "ICD10-CM:F00.1", "ICD10-CM:F00.2", "ICD10-CM:F00.9", "ICD10-CM:F01.1", "ICD10-CM:F01.2", "ICD10-CM:F01.3", "ICD10-CM:F01.8", "ICD10-CM:F01.9", 
    "ICD10-CM:F02.0", "ICD10-CM:F02.1", "ICD10-CM:F02.2", "ICD10-CM:F02.3", "ICD10-CM:F02.4", "ICD10-CM:F02.8", "ICD10-CM:F03", "ICD9:290.0", 
    "ICD9:290.1", "ICD9:290.2", "ICD9:290.4", "ICD9:290.8", "ICD9:290.9", "ICD9:294.1", "ICD9-CM:290.0", "ICD9-CM:290.1", "ICD9-CM:290.2", "ICD9-CM:290.4", 
    "ICD9-CM:290.8", "ICD9-CM:290.9", "ICD9-CM:294.1", "ICD8:290", "ICD8:290.0", "ICD8:290.1", "ICD8:293.0", "ICD8:293.2", "ICD8:293.4", "ICD8:293.9", "ICD8:294.8"]

# Alcohol use Disorder (AUD)
#AUD_Codes = ["F10.2", 303.2, 303.20]
AUD_Codes = ["ICD10:F10.2", "ICD10-CM:F10.2", "ICD9:303", "ICD9:303.0", "ICD9:303.1", "ICD9:303.2", "ICD9:303.9",
    "ICD9-CM:303", "ICD9-CM:303.0", "ICD9-CM:303.1", "ICD9-CM:303.2", "ICD9-CM:303.9", "ICD8:303.2"]

# Drug use Disorder (DUD)
#DUD_Codes = ["F11.2", "F12.2", "F13.2", "F14.2", "F15.2", "F16.2", "F18.2", "F19.2", 304.0, 304.09, 304.19, 304.5, 304.59, 304.2, 304.29, 304.39, 304.4, 304.49, 304.69, 304.7, 304.79, 304.89, 304.99, 304.8, 304.00, 304.09, 304.19, 304.50, 304.59, 304.20, 304.29, 304.39, 304.40, 304.49, 304.69, 304.70, 304.79, 304.89, 304.99, 304.80]
DUD_Codes = ["ICD10:F11.2", "ICD10:F12.2", "ICD10:F13.2", "ICD10:F14.2", "ICD10:F15.2", "ICD10:F16.2", "ICD10:F18.2", "ICD10:F19.2", "ICD10-CM:F11.2", 
    "ICD10-CM:F12.2", "ICD10-CM:F13.2", "ICD10-CM:F14.2", "ICD10-CM:F15.2", "ICD10-CM:F16.2", "ICD10-CM:F18.2", "ICD10-CM:F19.2", "ICD9:304.0", 
    "ICD9:304.3", "ICD9:304.1", "ICD9:304.2", "ICD9:304.4", "ICD9:304.5", "ICD9:304.6", "ICD9:304.7", "ICD9-CM:304.0", "ICD9-CM:304.3", "ICD9-CM:304.1", 
    "ICD9-CM:304.2", "ICD9-CM:304.4", "ICD9-CM:304.5", "ICD9-CM:304.6", "ICD9-CM:304.7", "ICD8:304.0", "ICD8:304.09", "ICD8:304.19", "ICD8:304.5", 
    "ICD8:304.59", "ICD8:304.2", "ICD8:304.29", "ICD8:304.39", "ICD8:304.4", "ICD8:304.49", "ICD8:304.69", "ICD8:304.7", "ICD8:304.79", "ICD8:304.89", 
    "ICD8:304.99", "ICD8:304.8"]

# Mild cognitive Impairment (MCI)
#MCI_Codes = ["G31.84", "F06.7"]
MCI_Codes = ["ICD10:G31.84", "ICD10:F06.7", "ICD10-CM:G31.84", "ICD10-CM:F06.7", "ICD9:331.83", "ICD9:310.1",
    "ICD9-CM:331.83", "ICD9-CM:310.1"]

# Concurrent terminal illness (CTI)
#CTI_Codes = ["C79.9", "C77.9", "G12.2", "I50.9", "J44.9", "J44.0", "J44.1", "K74.6", 198.99, 196.99, 348.09, 348.19, 348.20,248.29, 248.99, 427.09, 571.92, 571.99]
CTI_Codes = ["ICD10:C25", "ICD10:C25.0", "ICD10:C25.1", "ICD10:C25.2", "ICD10:C25.3", "ICD10:C25.4", "ICD10:C25.7", "ICD10:C25.8", 
    "ICD10:C25.9", "ICD10:C22", "ICD10:C22.0", "ICD10:C22.1", "ICD10:C22.2", "ICD10:C22.3", "ICD10:C22.4", "ICD10:C22.7", "ICD10:C22.8", 
    "ICD10:C22.9", "ICD10:C34", "ICD10:C34.0", "ICD10:C34.1", "ICD10:C34.2", "ICD10:C34.3", "ICD10:C34.8", "ICD10:C34.9", "ICD10:C71", 
    "ICD10:C71.0", "ICD10:C71.1", "ICD10:C71.2", "ICD10:C71.3", "ICD10:C71.4", "ICD10:C71.5", "ICD10:C71.6", "ICD10:C71.7", "ICD10:C71.8", 
    "ICD10:C71.9", "ICD10:C15", "ICD10:C15.0", "ICD10:C15.1", "ICD10:C15.2", "ICD10:C15.3", "ICD10:C15.4", "ICD10:C15.5", "ICD10:C15.8", 
    "ICD10:C15.9", "ICD10:C16", "ICD10:C16.0", "ICD10:C16.1", "ICD10:C16.2", "ICD10:C16.3", "ICD10:C16.4", "ICD10:C16.5", "ICD10:C16.6", 
    "ICD10:C16.7", "ICD10:C16.8", "ICD10:C16.9", "ICD10:C50", "ICD10:C50.0", "ICD10:C50.1", "ICD10:C50.2", "ICD10:C50.3", "ICD10:C50.4", 
    "ICD10:C50.5", "ICD10:C50.6", "ICD10:C50.8", "ICD10:C50.9", "ICD10:C61", "ICD10:C61.9", "ICD10:C67", "ICD10:C67.0", "ICD10:C67.1", 
    "ICD10:C67.2", "ICD10:C67.3", "ICD10:C67.4", "ICD10:C67.5", "ICD10:C67.6", "ICD10:C67.7", "ICD10:C67.8", "ICD10:C67.9", "ICD10:C64", 
    "ICD10:C64.0", "ICD10:C64.1", "ICD10:C64.2", "ICD10:C64.8", "ICD10:C64.9", "ICD10:C56", "ICD10:C56.9", "ICD10:C79.9", "ICD10:C77.9", 
    "ICD10:G30", "ICD10:G30.0", "ICD10:G30.1", "ICD10:G30.8", "ICD10:G30.9", "ICD10:G12.21", "ICD10:G12.2A", "ICD10:N18.5", "ICD10:I50", 
    "ICD10:I50.0", "ICD10:I50.1", "ICD10:I50.2", "ICD10:I50.3", "ICD10:I50.4", "ICD10:I50.8", "ICD10:I50.9", "ICD10:J44", "ICD10:J44.0", 
    "ICD10:J44.1", "ICD10:J44.8", "ICD10:J44.9", "ICD10:C18", "ICD10:C18.0", "ICD10:C18.1", "ICD10:C18.2", "ICD10:C18.3", "ICD10:C18.4", 
    "ICD10:C18.5", "ICD10:C18.6", "ICD10:C18.7", "ICD10:C18.8", "ICD10:C18.9", "ICD10:G20", "ICD10:G20.9", "ICD10:G20.9A", "ICD10:G35", 
    "ICD10:G35.9", "ICD10:G35.9A", "ICD10:G35.9B", "ICD10:G35.9C", "ICD10:K72.9", "ICD10:K72.90", "ICD10:K72.9A", "ICD10:K72.10", 
    "ICD10:K72.11", "ICD10:K74.6", "ICD10:K74.60", "ICD10:K74.6A", "ICD10:K74.6B", "ICD10:K74.6C", "ICD10:K74.6D", "ICD10:K74.6E", 
    "ICD10:K74.6F", "ICD10:K74.6G", "ICD10:K74.6H", "ICD10:K70.3", "ICD10:K70.30", "ICD10:K70.3A", "ICD10:K74.69", "ICD10-CM:C25.0", 
    "ICD10-CM:C25.1", "ICD10-CM:C25.2", "ICD10-CM:C25.3", "ICD10-CM:C25.4", "ICD10-CM:C25.7", "ICD10-CM:C25.8", "ICD10-CM:C25.9", "ICD10-CM:C22.0", 
    "ICD10-CM:C22.1", "ICD10-CM:C22.2", "ICD10-CM:C22.3", "ICD10-CM:C22.4", "ICD10-CM:C22.7", "ICD10-CM:C22.8", "ICD10-CM:C22.9", "ICD10-CM:C34", 
    "ICD10-CM:C34.0", "ICD10-CM:C34.1", "ICD10-CM:C34.2", "ICD10-CM:C34.3", "ICD10-CM:C34.8", "ICD10-CM:C34.9", "ICD10-CM:C71", "ICD10-CM:C71.0", 
    "ICD10-CM:C71.1", "ICD10-CM:C71.2", "ICD10-CM:C71.3", "ICD10-CM:C71.4", "ICD10-CM:C71.5", "ICD10-CM:C71.6", "ICD10-CM:C71.7", 
    "ICD10-CM:C71.8", "ICD10-CM:C71.9", "ICD10-CM:C15", "ICD10-CM:C15.0", "ICD10-CM:C15.1", "ICD10-CM:C15.2", "ICD10-CM:C15.3", "ICD10-CM:C15.4", 
    "ICD10-CM:C15.5", "ICD10-CM:C15.8", "ICD10-CM:C15.9", "ICD10-CM:C16", "ICD10-CM:C16.0", "ICD10-CM:C16.1", "ICD10-CM:C16.2", "ICD10-CM:C16.3", 
    "ICD10-CM:C16.4", "ICD10-CM:C16.5", "ICD10-CM:C16.6", "ICD10-CM:C16.7", "ICD10-CM:C16.8", "ICD10-CM:C16.9", "ICD10-CM:C50", "ICD10-CM:C50.0", 
    "ICD10-CM:C50.1", "ICD10-CM:C50.2", "ICD10-CM:C50.3", "ICD10-CM:C50.4", "ICD10-CM:C50.5", "ICD10-CM:C50.6", "ICD10-CM:C50.8", 
    "ICD10-CM:C50.9", "ICD10-CM:C61", "ICD10-CM:C61.9", "ICD10-CM:C67", "ICD10-CM:C67.0", "ICD10-CM:C67.1", "ICD10-CM:C67.2", "ICD10-CM:C67.3", "ICD10-CM:C67.4", 
    "ICD10-CM:C67.5", "ICD10-CM:C67.6", "ICD10-CM:C67.7", "ICD10-CM:C67.8", "ICD10-CM:C67.9,C64", "ICD10-CM:C64.0", "ICD10-CM:C64.1", 
    "ICD10-CM:C64.2", "ICD10-CM:C64.8", "ICD10-CM:C64.9", "ICD10-CM:C56", "ICD10-CM:C56.9", "ICD10-CM:C79.9", "ICD10-CM:C77.9,G30", "ICD10-CM:G30.0", "ICD10-CM:G30.1", 
    "ICD10-CM:G30.8", "ICD10-CM:G30.9", "ICD10-CM:G12.21", "ICD10-CM:G12.2A", "ICD10-CM:N18.6", "ICD10-CM:I50", "ICD10-CM:I50.0", "ICD10-CM:I50.1", "ICD10-CM:I50.2", "ICD10-CM:I50.3", 
    "ICD10-CM:I50.4", "ICD10-CM:I50.8", "ICD10-CM:I50.9", "ICD10-CM:J44", "ICD10-CM:J44.0", "ICD10-CM:J44.1", "ICD10-CM:J44.8", "ICD10-CM:J44.9", "ICD10-CM:C18", 
    "ICD10-CM:C18.0", "ICD10-CM:C18.1", "ICD10-CM:C18.2", "ICD10-CM:C18.3", "ICD10-CM:C18.4", "ICD10-CM:C18.5", "ICD10-CM:C18.6", "ICD10-CM:C18.7", 
    "ICD10-CM:C18.8", "ICD10-CM:C18.9", "ICD10-CM:G20", "ICD10-CM:G20.A", "ICD10-CM:G20.A1", "ICD10-CM:G20.A2", "ICD10-CM:G20.B", "ICD10-CM:G20.B1", 
    "ICD10-CM:G20.B2", "ICD10-CM:G20.C", "ICD10-CM:G35", "ICD10-CM:K72.9", "ICD10-CM:K72.90", "ICD10-CM:K72.91,K72.1", "ICD10-CM:K72.10", "ICD10-CM:K72.11", 
    "ICD10-CM:K72.90", "ICD10-CM:K74.60", "ICD10-CM:K74.69", "ICD10-CM:K70.30", "ICD10-CM:K70.31", "ICD10-CM:K74.60", "ICD10-CM:K74.69",
    "ICD9:157", "ICD9:157.0", "ICD9:157.1", "ICD9:157.2", "ICD9:157.3", "ICD9:157.4", "ICD9:157.8", "ICD9:157.9", "ICD9:155", "ICD9:155.0", 
    "ICD9:155.1", "ICD9:155.2", "ICD9:162", "ICD9:162.0", "ICD9:162.2", "ICD9:162.3", "ICD9:162.4", "ICD9:162.5", "ICD9:162.8", 
    "ICD9:162.9", "ICD9:191", "ICD9:191.0", "ICD9:191.1", "ICD9:191.2", "ICD9:191.3", "ICD9:191.4", "ICD9:191.5", "ICD9:191.6", 
    "ICD9:191.7", "ICD9:191.8", "ICD9:191.9", "ICD9:150", "ICD9:150.0", "ICD9:150.1", "ICD9:150.2", "ICD9:150.3", "ICD9:150.4", 
    "ICD9:150.5", "ICD9:150.8", "ICD9:150.9", "ICD9:151", "ICD9:151.0", "ICD9:151.1", "ICD9:151.2", "ICD9:151.3", "ICD9:151.4", 
    "ICD9:151.5", "ICD9:151.6", "ICD9:151.7", "ICD9:151.8", "ICD9:151.9", "ICD9:174", "ICD9:174.0", "ICD9:174.1", "ICD9:174.2", 
    "ICD9:174.3", "ICD9:174.4", "ICD9:174.5", "ICD9:174.6", "ICD9:174.8", "ICD9:174.9", "ICD9:185", "ICD9:188", "ICD9:188.0", 
    "ICD9:188.1", "ICD9:188.2", "ICD9:188.3", "ICD9:188.4", "ICD9:188.5", "ICD9:188.6", "ICD9:188.7", "ICD9:188.8", "ICD9:188.9", 
    "ICD9:189", "ICD9:189.0", "ICD9:189.1", "ICD9:189.2", "ICD9:189.3", "ICD9:189.4", "ICD9:189.8", "ICD9:189.9", "ICD9:183", 
    "ICD9:183.0", "ICD9:183.2", "ICD9:183.3", "ICD9:183.4", "ICD9:183.5", "ICD9:183.8", "ICD9:183.9", "ICD9:199", "ICD9:331", 
    "ICD9:331.0", "ICD9:331.1", "ICD9:331.2", "ICD9:331.9", "ICD9:335.2", "ICD9:335.20", "ICD9:335.21", "ICD9:335.22", "ICD9:335.23", 
    "ICD9:335.24", "ICD9:335.29", "ICD9:586", "ICD9:428", "ICD9:428.0", "ICD9:428.1", "ICD9:428.2", "ICD9:428.3", "ICD9:428.4", 
    "ICD9:428.9", "ICD9:496", "ICD9:153", "ICD9:153.0", "ICD9:153.1", "ICD9:153.2", "ICD9:153.3", "ICD9:153.4", "ICD9:153.5", 
    "ICD9:153.6", "ICD9:153.7", "ICD9:153.8", "ICD9:153.9", "ICD9:332", "ICD9:332.0", "ICD9:332.1", "ICD9:340", "ICD9:572.8", 
    "ICD9:571.9", "ICD9:571.2", "ICD9:571.5", "ICD9:571.9", "ICD9-CM:157", "ICD9-CM:157.0", "ICD9-CM:157.1", "ICD9-CM:157.2", "ICD9-CM:157.3", 
    "ICD9-CM:157.4", "ICD9-CM:157.8", "ICD9-CM:157.9", "ICD9-CM:155", "ICD9-CM:155.0", "ICD9-CM:155.1", "ICD9-CM:155.2", "ICD9-CM:162", 
    "ICD9-CM:162.0", "ICD9-CM:162.2", "ICD9-CM:162.3", "ICD9-CM:162.4", "ICD9-CM:162.5", "ICD9-CM:162.8", "ICD9-CM:162.9", "ICD9-CM:191", 
    "ICD9-CM:191.0", "ICD9-CM:191.1", "ICD9-CM:191.2", "ICD9-CM:191.3", "ICD9-CM:191.4", "ICD9-CM:191.5", "ICD9-CM:191.6", "ICD9-CM:191.7", 
    "ICD9-CM:191.8", "ICD9-CM:191.9", "ICD9-CM:150", "ICD9-CM:150.0", "ICD9-CM:150.1", "ICD9-CM:150.2", "ICD9-CM:150.3", "ICD9-CM:150.4", 
    "ICD9-CM:150.5", "ICD9-CM:150.8", "ICD9-CM:150.9", "ICD9-CM:151", "ICD9-CM:151.0", "ICD9-CM:151.1", "ICD9-CM:151.2", "ICD9-CM:151.3", 
    "ICD9-CM:151.4", "ICD9-CM:151.5", "ICD9-CM:151.6", "ICD9-CM:151.7", "ICD9-CM:151.8", "ICD9-CM:151.9", "ICD9-CM:174", "ICD9-CM:174.0", 
    "ICD9-CM:174.1", "ICD9-CM:174.2", "ICD9-CM:174.3", "ICD9-CM:174.4", "ICD9-CM:174.5", "ICD9-CM:174.6", "ICD9-CM:174.8", "ICD9-CM:174.9", 
    "ICD9-CM:185", "ICD9-CM:188", "ICD9-CM:188.0", "ICD9-CM:188.1", "ICD9-CM:188.2", "ICD9-CM:188.3", "ICD9-CM:188.4", "ICD9-CM:188.5", 
    "ICD9-CM:188.6", "ICD9-CM:188.7", "ICD9-CM:188.8", "ICD9-CM:188.9", "ICD9-CM:189", "ICD9-CM:189.0", "ICD9-CM:189.1", "ICD9-CM:189.2", 
    "ICD9-CM:189.3", "ICD9-CM:189.4", "ICD9-CM:189.8", "ICD9-CM:189.9", "ICD9-CM:183", "ICD9-CM:183.0", "ICD9-CM:183.2", "ICD9-CM:183.3", 
    "ICD9-CM:183.4", "ICD9-CM:183.5", "ICD9-CM:183.8", "ICD9-CM:183.9", "ICD9-CM:199", "ICD9-CM:331", "ICD9-CM:331.0", "ICD9-CM:331.1", 
    "ICD9-CM:331.2", "ICD9-CM:331.9", "ICD9-CM:335.2", "ICD9-CM:335.20", "ICD9-CM:335.21", "ICD9-CM:335.22", "ICD9-CM:335.23", "ICD9-CM:335.24", 
    "ICD9-CM:335.29", "ICD9-CM:586", "ICD9-CM:428", "ICD9-CM:428.0", "ICD9-CM:428.1", "ICD9-CM:428.2", "ICD9-CM:428.3", "ICD9-CM:428.4", 
    "ICD9-CM:428.9", "ICD9-CM:496", "ICD9-CM:153", "ICD9-CM:153.0", "ICD9-CM:153.1", "ICD9-CM:153.2", "ICD9-CM:153.3", "ICD9-CM:153.4", 
    "ICD9-CM:153.5", "ICD9-CM:153.6", "ICD9-CM:153.7", "ICD9-CM:153.8", "ICD9-CM:153.9", "ICD9-CM:332", "ICD9-CM:332.0", "ICD9-CM:332.1", 
    "ICD9-CM:340", "ICD9-CM:572.8", "ICD9-CM:572.8", "ICD9-CM:572.8", "ICD9-CM:572.8", "ICD9-CM:571.9", "ICD9-CM:571.2", "ICD9-CM:571.5", 
    "ICD9-CM:571.9", "ICD8:157", "ICD8:157.09", "ICD8:157.80", "ICD8:157.81", "ICD8:157.89", 
    "ICD8:157.99", "ICD8:155", "ICD8:155.09", "ICD8:155.19", "ICD8:155.89", "ICD8:162", "ICD8:162.09", "ICD8:162.10", "ICD8:162.11", 
    "ICD8:162.12", "ICD8:162.13", "ICD8:162.14", "ICD8:162.15", "ICD8:162.16", "ICD8:162.18", "ICD8:162.19", "ICD8:191", "ICD8:191.00", 
    "ICD8:191.01", "ICD8:191.02", "ICD8:191.03", "ICD8:191.04", "ICD8:191.05", "ICD8:191.06", "ICD8:191.07", "ICD8:191.08", 
    "ICD8:191.09", "ICD8:150", "ICD8:150.00", "ICD8:150.01", "ICD8:150.02", "ICD8:150.08", "ICD8:150.09", "ICD8:151", "ICD8:151.09", 
    "ICD8:151.19", "ICD8:151.80", "ICD8:151.81", "ICD8:151.82", "ICD8:151.89", "ICD8:151.99", "ICD8:170", "ICD8:170.09", "ICD8:170.19", 
    "ICD8:170.29", "ICD8:170.39", "ICD8:170.49", "ICD8:170.59", "ICD8:170.69", "ICD8:170.79", "ICD8:170.89", "ICD8:170.99", "ICD8:185",
    "ICD8:185.09", "ICD8:188", "ICD8:188.00", "ICD8:188.01", "ICD8:188.02", "ICD8:188.08", "ICD8:188.09", "ICD8:189", "ICD8:189.09",
    "ICD8:189.19", "ICD8:189.29", "ICD8:189.90", "ICD8:189.91", "ICD8:189.92", "ICD8:189.99", "ICD8:183", "ICD8:183.00", "ICD8:183.01", 
    "ICD8:183.02", "ICD8:183.03", "ICD8:183.08", "ICD8:183.09", "ICD8:183.19", "ICD8:183.99", "ICD8:198.99", "ICD8:196.99", "ICD8:290.1", 
    "ICD8:290.10", "ICD8:348", "ICD8:348.09", "ICD8:348.19", "ICD8:348.20", "ICD8:348.29", "ICD8:348.99", "ICD8:792.9", "ICD8:792.99", 
    "ICD8:427", "ICD8:427.09", "ICD8:427.10", "ICD8:427.11", "ICD8:427.19", "ICD8:427.20", "ICD8:427.21", "ICD8:427.22", "ICD8:427.23", 
    "ICD8:427.24", "ICD8:427.25", "ICD8:427.26", "ICD8:427.27", "ICD8:427.28", "ICD8:427.29", "ICD8:427.90", "ICD8:427.91", "ICD8:427.92", 
    "ICD8:427.93", "ICD8:427.94", "ICD8:427.95", "ICD8:427.96", "ICD8:427.97", "ICD8:427.99", "ICD8:491", "ICD8:491.00", "ICD8:491.01", 
    "ICD8:491.02", "ICD8:491.03", "ICD8:491.04", "ICD8:491.08", "ICD8:491.09", "ICD8:153", "ICD8:153.00", "ICD8:153.01", "ICD8:153.02", 
    "ICD8:153.09", "ICD8:153.19", "ICD8:153.29", "ICD8:153.39", "ICD8:153.80", "ICD8:153.89", "ICD8:153.99", "ICD8:342", "ICD8:342.99", 
    "ICD8:345", "ICD8:345.09", "ICD8:345.10", "ICD8:345.11", "ICD8:345.18", "ICD8:345.19", "ICD8:345.29", "ICD8:345.30", "ICD8:345.31", 
    "ICD8:345.38", "ICD8:345.39", "ICD8:345.99", "ICD8:782.8", "ICD8:782.89", "ICD8:782.8", "ICD8:782.89", "ICD8:782.8", "ICD8:782.89", 
    "ICD8:782.8", "ICD8:782.89", "ICD8:586", "ICD8:586.00", "ICD8:581.1", "ICD8:581.10", "ICD8:586.1", "ICD8:586.10", "ICD8:586.9", 
    "ICD8:586.90", "ICD8:348.09", "ICD8:348.19", "ICD8:348.20", "ICD8:248.29", "ICD8:248.99", "ICD8:427.09", "ICD8:571.92", "ICD8:571.99"]

# Generlaized Anxiety Disorder (GAD)
GAD_Codes = ["ICD10:F41.1", "ICD10-CM:F41.1"]

# Panic Disorder (PD)
PD_Codes = ["ICD10:F41.0", "ICD10-CM:F41.0"]

# Phobias
#Phobias_Codes = ["F40" , "F40.0", "F40.1", "F40.2", "F40.8", "F40.9", 300.2, 300.0, 300.20, 300.00]
Phobias_Codes = ["ICD10:F40", "ICD10:F40.0", "ICD10:F40.1", "ICD10:F40.2", "ICD10:F40.8", "ICD10:F40.9",
    "ICD10-CM:F40", "ICD10-CM:F40.0", "ICD10-CM:F40.1", "ICD10-CM:F40.2", "ICD10-CM:F40.8", "ICD10-CM:F40.9",
    "ICD9:300.2", "ICD9:300.0", "ICD9-CM:300.2", "ICD9-CM:300.0", "ICD8:300.2", "ICD8:300.0"]
# Anxiety Disorder (ANX)
ANX_Codes = ["F40", "F40.0", "F40.1", "F40.2", "F40.8", "F40.9", "F41", "F41.0", "F41.1", "F41.1", "F41.3", "F41.8", "F41.9", 300.0, 300.2, 300.00, 300.20]

# ADHD 
#ADHD_Codes = ["F90", "F90.0", "F90.1", "F90.8", "F90.9", 380.01]#DONT DO EXACT MAPPING!
ADHD_Codes = ["ICD10:F90", "ICD10:F90.0", "ICD10:F90.1", "ICD10:F90.2", "ICD10:F90.8", "ICD10:F90.9",
    "ICD10-CM:F90", "ICD10-CM:F90.0", "ICD10-CM:F90.1", "ICD10-CM:F90.2", "ICD10-CM:F90.8", "ICD10-CM:F90.9",
    "ICD9:314", "ICD9:314.0", "ICD9:314.00", "ICD9:314.01", "ICD9:314.1", "ICD9-CM:314", "ICD9-CM:314.0", "ICD9-CM:314.00", 
    "ICD9-CM:314.01", "ICD9-CM:314.1", "ICD8:308.01", "ICD8:308.02"]
# Autism (ASD)
#ASD_Codes = ["F84.0", "F84.1", "F84.5", "F84.9", 299.00, 299.01, 299.02, 299.03]
ASD_Codes = ["ICD10:F84.0", "ICD10:F84.1", "ICD10:F84.5", "ICD10:F84.9", 
    "ICD10-CM:F84.0", "ICD10-CM:F84.1", "ICD10-CM:F84.5", "ICD10-CM:F84.9",
    "ICD9:299.0", "ICD9:299.00", "ICD9:299.01", "ICD9:299.1", "ICD9:299.10", "ICD9:299.11", "ICD9:299.8", "ICD9:299.80", 
    "ICD9:299.81", "ICD9:299.9", "ICD9:299.90", "ICD9:299.91", "ICD9-CM:299.0", "ICD9-CM:299.00", "ICD9-CM:299.01", 
    "ICD9-CM:299.1", "ICD9-CM:299.10", "ICD9-CM:299.11", "ICD9-CM:299.8", "ICD9-CM:299.80", "ICD9-CM:299.81", "ICD9-CM:299.9", 
    "ICD9-CM:299.90", "ICD9-CM:299.91", "ICD8:299.00", "ICD8:299.01", "ICD8:299.02", "ICD8:299.03"]

# Bulimia (BUL)
BUL_Codes = ["F50.2", "F50.3"]

# Anorexia (ANO)
ANO_Codes = ["F50.0", 306.50]

# Posttraumatic Stress Disorder (PTSD)
PTSD_Codes = ["ICD10:F43.1", "ICD10-CM:F43.1", "ICD9:309.8", "ICD9-CM:309.8"]

# Low Back Pain (LBP)
LBP_Codes = ["M54.5", 728.79, 717.09]

# Obsessive-compulsive disorder (OCD)
#OCD_Codes = ["F42", "F42.0", "F42.1", "F42.2", "F42.8", "F42.9", 300.3, 300.30, 300.39]
OCD_Codes = ["ICD10:F42", "ICD10:F42.0", "ICD10:F42.1", "ICD10:F42.2", "ICD10:F42.8", "ICD10:F42.9",
    "ICD10-CM:F42", "ICD10-CM:F42.0", "ICD10-CM:F42.1", "ICD10-CM:F42.2", "ICD10-CM:F42.8", "ICD10-CM:F42.9",
    "ICD9:300.3", "ICD9-CM:300.3", "ICD8:300.3", "ICD8:300.39"]

# Chronic Pain (CP)
CP_Codes = ["ICD10:M79.60", "ICD10:M54.5", "ICD10:M54.2", "ICD10:G89.0", "ICD10:G89.4", "ICD10:G89.8", "ICD10:G89.29", "ICD10:G89.21", 
    "ICD10:M06.9", "ICD10:M15", "ICD10:M15.0", "ICD10:M15.1", "ICD10:M15.2", "ICD10:M15.3", "ICD10:M15.4", "ICD10:M15.8", "ICD10:M15.9", 
    "ICD10:M16", "ICD10:M16.0", "ICD10:M16.1", "ICD10:M16.2", "ICD10:M16.3", "ICD10:M16.4", "ICD10:M16.5", "ICD10:M16.6", "ICD10:M16.7", 
    "ICD10:M16.9", "ICD10:M17", "ICD10:M17.0", "ICD10:M17.1", "ICD10:M17.2", "ICD10:M17.3", "ICD10:M17.4", "ICD10:M17.5", "ICD10:M17.9", 
    "ICD10:M18", "ICD10:M18.0", "ICD10:M18.1", "ICD10:M18.2", "ICD10:M18.3", "ICD10:M18.4", "ICD10:M18.5", "ICD10:M18.9", "ICD10:M19", 
    "ICD10:M19.0", "ICD10:M19.1", "ICD10:M19.2", "ICD10:M19.8", "ICD10:M19.9", "ICD10:G90.50", "ICD10:G90.511", "ICD10:G53.0", "ICD10:D57.81", 
    "ICD10:G35", "ICD10:M45", "ICD10:M79.7", "ICD10:M05", "ICD10:M05.0", "ICD10:M05.1", "ICD10:M05.2", "ICD10:M05.3", "ICD10:M05.8", 
    "ICD10:M05.9", "ICD10:M06", "ICD10:M06.0", "ICD10:M06.1", "ICD10:M06.2", "ICD10:M06.3", "ICD10:M06.4", "ICD10:M06.8", "ICD10:M06.9",
    "ICD10-CM:M79.60", "ICD10-CM:M54.5", "ICD10-CM:M54.2", "ICD10-CM:G89.0", "ICD10-CM:G89.4", "ICD10-CM:G89.8", "ICD10-CM:G89.29", 
    "ICD10-CM:G89.21", "ICD10-CM:M06.9", "ICD10-CM:M15", "ICD10-CM:M15.0", "ICD10-CM:M15.1", "ICD10-CM:M15.2", "ICD10-CM:M15.3", 
    "ICD10-CM:M15.4", "ICD10-CM:M15.8", "ICD10-CM:M15.9", "ICD10-CM:M16", "ICD10-CM:M16.0", "ICD10-CM:M16.1", "ICD10-CM:M16.2", 
    "ICD10-CM:M16.3", "ICD10-CM:M16.4", "ICD10-CM:M16.5", "ICD10-CM:M16.6", "ICD10-CM:M16.7", "ICD10-CM:M16.9", "ICD10-CM:M17", 
    "ICD10-CM:M17.0", "ICD10-CM:M17.1", "ICD10-CM:M17.2", "ICD10-CM:M17.3", "ICD10-CM:M17.4", "ICD10-CM:M17.5", "ICD10-CM:M17.9", 
    "ICD10-CM:M18", "ICD10-CM:M18.0", "ICD10-CM:M18.1", "ICD10-CM:M18.2", "ICD10-CM:M18.3", "ICD10-CM:M18.4", "ICD10-CM:M18.5", "ICD10-CM:M18.9", 
    "ICD10-CM:M19", "ICD10-CM:M19.0", "ICD10-CM:M19.1", "ICD10-CM:M19.2", "ICD10-CM:M19.8", "ICD10-CM:M19.9", "ICD10-CM:G90.50", 
    "ICD10-CM:G90.511", "ICD10-CM:G53.0", "ICD10-CM:D57.81", "ICD10-CM:G35", "ICD10-CM:M45", "ICD10-CM:M79.7", "ICD10-CM:M05", "ICD10-CM:M05.0", 
    "ICD10-CM:M05.1", "ICD10-CM:M05.2", "ICD10-CM:M05.3", "ICD10-CM:M05.8", "ICD10-CM:M05.9", "ICD10-CM:M06", "ICD10-CM:M06.0", 
    "ICD10-CM:M06.1", "ICD10-CM:M06.2", "ICD10-CM:M06.3", "ICD10-CM:M06.4", "ICD10-CM:M06.8", "ICD10-CM:M06.9", "ICD9:338.2", "ICD9:724.2", 
    "ICD9:723.1", "ICD9:338.3", "ICD9:338.4", "ICD9:338.8", "ICD9:714.0", "ICD9:714.1", "ICD9:714.2", "ICD9:714.3", "ICD9:714.4", 
    "ICD9:714.5", "ICD9:714.6", "ICD9:714.7", "ICD9:714.8", "ICD9:714.9", "ICD9:715.0", "ICD9:715.1", "ICD9:715.2", "ICD9:715.3", 
    "ICD9:715.4", "ICD9:715.5", "ICD9:715.6", "ICD9:715.7", "ICD9:715.8", "ICD9:715.9", "ICD9:337.0", "ICD9:337.1", "ICD9:337.2", 
    "ICD9:337.3", "ICD9:337.4", "ICD9:337.5", "ICD9:337.6", "ICD9:337.7", "ICD9:337.8", "ICD9:337.9", "ICD9:337.22", "ICD9:053.12", 
    "ICD9:282.60", "ICD9:340", "ICD9:714.0", "ICD9-CM:338.2", "ICD9-CM:724.2", "ICD9-CM:723.1", "ICD9-CM:338.3", "ICD9-CM:338.4", 
    "ICD9-CM:338.8", "ICD9-CM:714.0", "ICD9-CM:714.1", "ICD9-CM:714.2", "ICD9-CM:714.3", "ICD9-CM:714.4", "ICD9-CM:714.5", "ICD9-CM:714.6", 
    "ICD9-CM:714.7", "ICD9-CM:714.8", "ICD9-CM:714.9", "ICD9-CM:715.0", "ICD9-CM:715.1", "ICD9-CM:715.2", "ICD9-CM:715.3", "ICD9-CM:715.4", 
    "ICD9-CM:715.5", "ICD9-CM:715.6", "ICD9-CM:715.7", "ICD9-CM:715.8", "ICD9-CM:715.9", "ICD9-CM:337.0", "ICD9-CM:337.1", "ICD9-CM:337.2", 
    "ICD9-CM:337.3", "ICD9-CM:337.4", "ICD9-CM:337.5", "ICD9-CM:337.6", "ICD9-CM:337.7", "ICD9-CM:337.8", "ICD9-CM:337.9", "ICD9-CM:337.22", 
    "ICD9-CM:053.12", "ICD9-CM:282.60", "ICD9-CM:340", "ICD9-CM:714.0"]    
#CP_Codes = [338.2, 724.2, 723.1, 338.3, 338.4, 338.8, 714.0, 714.1, 714.2, 714.3, 714.4, 714.5, 
#    714.6, 714.7, 714.8, 714.9, 715.0, 715.1, 715.2, 715.3, 715.4, 715.5, 715.6, 715.7, 
#    715.8, 715.9, 337.0, 337.1, 337.2, 337.3, 337.4, 337.5, 337.6, 337.7, 337.8, 337.9, 
#    337.22, 053.12, 282.60, 340, 714.0,
#    338.20, 724.20, 723.10, 338.30, 338.40, 338.80, 714.00, 714.10, 714.20, 714.30, 714.40, 714.50, 
#    714.60, 714.70, 714.80, 714.90, 715.00, 715.10, 715.20, 715.30, 715.40, 715.50, 715.60, 715.70, 
#    715.80, 715.90, 337.00, 337.10, 337.20, 337.30, 337.40, 337.50, 337.60, 337.70, 337.80, 337.90, 340.00, 714.00,
#    "M79.60", "M54.5", "M54.2", "G89.0", "G89.4", "G89.8", "G89.29", "G89.21", "M06.9", 
#    "M15", "M15.0", "M15.1", "M15.2", "M15.3", "M15.4", "M15.8", "M15.9", "M16", "M16.0", 
#    "M16.1", "M16.2", "M16.3", "M16.4", "M16.5", "M16.6", "M16.7", "M16.9", "M17", "M17.0",
#    "M17.1", "M17.2", "M17.3", "M17.4", "M17.5", "M17.9", "M18", "M18.0", "M18.1", "M18.2", 
#    "M18.3", "M18.4", "M18.5", "M18.9", "M19", "M19.0", "M19.1", "M19.2", "M19.8", "M19.9", 
#    "G90.50", "G90.511", "G53.0", "D57.81", "G35", "M45", "M79.7", "M05", "M05.0", "M05.1", 
#    "M05.2", "M05.3", "M05.8", "M05.9", "M06", "M06.0", "M06.1", "M06.2", "M06.3", "M06.4", 
#    "M06.8", "M06.9", "R52.2", "R52.1"]

# Ken's Pain (22.12.2023 via E-Mail)
Ken_Pain_Codes = ["R52", "M50", "M51", "M53", "M54"]

Sleep_Disorder_Codes = ["ICD10:F51.0", "ICD10:F51.00", "ICD10:F51.05", "ICD10:F51.02", "ICD10:F51.09", 
    "ICD10:F51.01", "ICD10:F51.03", "ICD10:F51.04", "ICD10:G47.00", "ICD10:G47.01", "ICD10:G47.09", 
    "ICD10:F51.1", "ICD10:F51.10", "ICD10:F51.11", "ICD10:F51.12", "ICD10:F51.19", "ICD10:G47.10", 
    "ICD10:G47.11", "ICD10:G47.12", "ICD10:G47.13", "ICD10:G47.14", "ICD10:G47.19", "ICD10:F51.2", 
    "ICD10:F51.20", "ICD10:F51.21", "ICD10:F51.22", "ICD10:F51.23", "ICD10:G47.20", "ICD10:G47.21", 
    "ICD10:G47.22", "ICD10:G47.23", "ICD10:G47.24", "ICD10:G47.26", "ICD10:F51.3", "ICD10:F51.4", 
    "ICD10:F51.5", "ICD10:G47.5", "ICD10:G47.50", "ICD10:G47.51", "ICD10:G47.52", "ICD10:G47.53", 
    "ICD10:G47.54", "ICD10:G47.55", "ICD10:G47.56", "ICD10:G47.57", "ICD10:G47.59", "ICD10:G47.5A", 
    "ICD10:G47.5B", "ICD10:G47.5W", "ICD10:G47.59", "ICD10:G47.6", "ICD10:G47.60", "ICD10:G47.61", 
    "ICD10:G47.62", "ICD10:G47.63", "ICD10:G47.64", "ICD10:G47.65", "ICD10:G47.66", "ICD10:G47.67", 
    "ICD10:G47.69", "ICD10:G47.41", "ICD10:G47.411", "ICD10:G47.42", "ICD10:G47.419", 
    "ICD10:G47.3", "ICD10:G47.32", "ICD10:G47.33", "ICD10:G47.34", "ICD10:G47.36", "ICD10:G47.31", 
    "ICD10:G47.37", "ICD10:G47.39", "ICD10:G47.30", "ICD10-CM:F51.0", "ICD10-CM:F51.05", "ICD10-CM:F51.02", 
    "ICD10-CM:F51.09", "ICD10-CM:F51.01", "ICD10-CM:F51.03", "ICD10-CM:F51.04", "ICD10-CM:G47.00", "ICD10-CM:G47.01", 
    "ICD10-CM:G47.09", "ICD10-CM:F51.1", "ICD10-CM:F51.11", "ICD10-CM:F51.12", "ICD10-CM:F51.19", "ICD10-CM:G47.10", 
    "ICD10-CM:G47.11", "ICD10-CM:G47.12", "ICD10-CM:G47.13", "ICD10-CM:G47.14", "ICD10-CM:G47.19", "ICD10-CM:G47.20", 
    "ICD10-CM:G47.21", "ICD10-CM:G47.22", "ICD10-CM:G47.23", "ICD10-CM:G47.24", "ICD10-CM:G47.26", "ICD10-CM:F51.3", 
    "ICD10-CM:F51.4", "ICD10-CM:F51.5", "ICD10-CM:G47.5", "ICD10-CM:G47.50", "ICD10-CM:G47.51", "ICD10-CM:G47.52", 
    "ICD10-CM:G47.53", "ICD10-CM:G47.54", "ICD10-CM:G47.55", "ICD10-CM:G47.56", "ICD10-CM:G47.57", "ICD10-CM:G47.59", 
    "ICD10-CM:G47.5A", "ICD10-CM:G47.5B", "ICD10-CM:G47.5W", "ICD10-CM:G47.59", "ICD10-CM:G47.6", "ICD10-CM:G47.60", 
    "ICD10-CM:G47.61", "ICD10-CM:G47.62", "ICD10-CM:G47.63", "ICD10-CM:G47.64", "ICD10-CM:G47.65", "ICD10-CM:G47.66", 
    "ICD10-CM:G47.67", "ICD10-CM:G47.69", "ICD10-CM:G47.41", "ICD10-CM:G47.411", "ICD10-CM:G47.42", "ICD10-CM:G47.419", 
    "ICD10-CM:G47.3", "ICD10-CM:G47.32", "ICD10-CM:G47.33", "ICD10-CM:G47.34", "ICD10-CM:G47.36", "ICD10-CM:G47.31", 
    "ICD10-CM:G47.37", "ICD10-CM:G47.39", "ICD10-CM:G47.30", "ICD9:307.42", "ICD9:307.44", "ICD9:780.54", "ICD9:307.45", 
    "ICD9:780.55", "ICD9:307.46", "ICD9:307.47", "ICD9:307.48", "ICD9:780.51", "ICD9:347.00", "ICD9:347.10", "ICD9:780.53", 
    "ICD9:780.53", "ICD9:780.53", "ICD9:770.81", "ICD9:327.23", "ICD9:780.51", "ICD9:327.21", "ICD9:327.29", "ICD9:327.20",
    "ICD9-CM:307.42", "ICD9-CM:307.44", "ICD9-CM:780.54", "ICD9-CM:307.45", "ICD9-CM:780.55", "ICD9-CM:307.46", "ICD9-CM:307.47", 
    "ICD9-CM:307.48", "ICD9-CM:780.51", "ICD9-CM:347.00", "ICD9-CM:347.10", "ICD9-CM:780.53", "ICD9-CM:780.53", "ICD9-CM:780.53", 
    "ICD9-CM:770.81", "ICD9-CM:780.57", "ICD9-CM:780.51", "ICD9-CM:327.21", "ICD9-CM:327.29", "ICD9-CM:327.20",
    "ICD8:306.49", "ICD8:306.49", "ICD8:347.00", "ICD8:347.01", "ICD8:347.09", "ICD8:347.10"]

#LOAD_Codes = ["ICD10:G30.1"]
#EOAD_Codes = ["ICD10:G30.0"]
AD_Codes = ["ICD10:G30.1", "ICD10:F00.1", "ICD10:F00.2", "ICD10:G30.8", "ICD10:G30.9", "ICD10:F00.9",
    "ICD10-CM:G30.1", "ICD10-CM:F00.1", "ICD10-CM:G30.8", "ICD10-CM:G30.9", "ICD10-CM:F00.9",
    "ICD9:331.0", "ICD9-CM:331.0", "ICD8:290.1", "ICD8:290.10"]
#WideAD_Codes = ["ICD10:F00", "ICD10:F00.0", "ICD10:F00.1", "ICD10:F00.2", "ICD10:F00.9", "ICD10:F01",
#    "ICD10:F01.1", "ICD10:F01.2", "ICD10:F01.3", "ICD10:F01.8", "ICD10:F01.9", "ICD10:F02", "ICD10:F02.0", 
#    "ICD10:F02.1", "ICD10:F02.2", "ICD10:F02.3", "ICD10:F02.4", "ICD10:F02.8","ICD10:F03", "ICD10:F05.1",
#    "ICD10:F05.9", "ICD10:F06.0", "ICD10:F06.7", "ICD10:F10.7", "ICD10:G30", "ICD10:G30.8", "ICD10:G30.9", 
#    "ICD10:G30.0", "ICD10:G30.1", "ICD10:G31.0", "ICD10:G31.1", "ICD10:G31.8"]

Pain_Codes = ["ICD10:R52", "ICD10:R52.0", "ICD10:R52.1", "ICD10:R52.2", "ICD10:R52.9", "ICD10:M50", "ICD10:M50.0", "ICD10:M50.1", 
    "ICD10:M50.2", "ICD10:M50.3", "ICD10:M50.8", "ICD10:M50.9", "ICD10:M51", "ICD10:M51.0", "ICD10:M51.1", "ICD10:M51.2", "ICD10:M51.3", 
    "ICD10:M51.4", "ICD10:M51.8", "ICD10:M51.9", "ICD10:M53", "ICD10:M53.0", "ICD10:M53.1", "ICD10:M53.2", "ICD10:M53.3", "ICD10:M53.8", 
    "ICD10:M53.9", "ICD10:M54", "ICD10:M54.0", "ICD10:M54.1", "ICD10:M54.2", "ICD10:M54.3", "ICD10:M54.4", "ICD10:M54.5", "ICD10:M54.6", 
    "ICD10:M54.8", "ICD10:M54.9", "ICD10-CM:R52", "ICD10-CM:R52.0", "ICD10-CM:R52.1", "ICD10-CM:R52.2", "ICD10-CM:R52.9", "ICD10-CM:M50", 
    "ICD10-CM:M50.0", "ICD10-CM:M50.1", "ICD10-CM:M50.2", "ICD10-CM:M50.3", "ICD10-CM:M50.8", "ICD10-CM:M50.9", "ICD10-CM:M51", 
    "ICD10-CM:M51.0", "ICD10-CM:M51.1", "ICD10-CM:M51.2", "ICD10-CM:M51.3", "ICD10-CM:M51.4", "ICD10-CM:M51.8", "ICD10-CM:M51.9", 
    "ICD10-CM:M53", "ICD10-CM:M53.0", "ICD10-CM:M53.1", "ICD10-CM:M53.2", "ICD10-CM:M53.3", "ICD10-CM:M53.8", "ICD10-CM:M53.9", 
    "ICD10-CM:M54", "ICD10-CM:M54.0", "ICD10-CM:M54.1", "ICD10-CM:M54.2", "ICD10-CM:M54.3", "ICD10-CM:M54.4", "ICD10-CM:M54.5", 
    "ICD10-CM:M54.6", "ICD10-CM:M54.8", "ICD10-CM:M54.9", "ICD9:338.19", "ICD9:780.96", "ICD9-CM:338.19", "ICD9-CM:780.96", "ICD8:834.10", 
    "ICD8:821.71"]

#Don't use exact Matching!
Chronic_Codes = ["ICD10:K50", "ICD10:K51", "ICD10:K52", "ICD10:B20", "ICD10:B22", "ICD10:B23", "ICD10:B24", 
    "ICD10:E10", "ICD10:E11", "ICD10:J45", "ICD10:G80", "ICD10:I10", "ICD10:I11", "ICD10:I12", "ICD10:I13", "ICD10:I14", "ICD10:I15", 
    "ICD10:I20", "ICD10:I21", "ICD10:I22", "ICD10:I23", "ICD10:I24", "ICD10:I25", "ICD10:N18", "ICD10:J44", "ICD10:N80", "ICD10:N80.0", "ICD10:N80.1", 
    "ICD10:N80.2", "ICD10:N80.3", "ICD10:N80.4", "ICD10:N80.4A", "ICD10:N80.4B", "ICD10:N80.5", "ICD10:N80.5A", "ICD10:N80.5B", "ICD10:N80.5C", 
    "ICD10:N80.6", "ICD10:N80.8", "ICD10:N80.8A", "ICD10:N80.8B", "ICD10:N80.8C", "ICD10:N80.8D", "ICD10:N80.9", "ICD10:M05", "ICD10:M06",
    "ICD10-CM:K50", "ICD10-CM:K51", "ICD10-CM:K52", "ICD10-CM:B20", "ICD10-CM:E10", "ICD10-CM:E11", 
    "ICD10-CM:J45", "ICD10-CM:G80", "ICD10-CM:I10", "ICD10-CM:I11", "ICD10-CM:I12", "ICD10-CM:I13", "ICD10-CM:I14", "ICD10-CM:I15", 
    "ICD10-CM:I20", "ICD10-CM:I21", "ICD10-CM:I22", "ICD10-CM:I23", "ICD10-CM:I24", "ICD10-CM:I25", "ICD10-CM:N18", "ICD10-CM:J44", 
    "ICD10-CM:N80", "ICD10-CM:N80.0", "ICD10-CM:N80.1", "ICD10-CM:N80.2", "ICD10-CM:N80.3", 
    "ICD10-CM:N80.4", "ICD10-CM:N80.5", "ICD10-CM:N80.6", "ICD10-CM:N80.8", "ICD10-CM:N80.9", "ICD10-CM:M05", "ICD10-CM:M06K50", 
    "ICD10-CM:K51", "ICD10-CM:K52", "ICD10-CM:K50", "ICD10-CM:K51", "ICD10-CM:B20", "ICD10-CM:E10", "ICD10-CM:E11", "ICD10-CM:J45", 
    "ICD10-CM:G80", "ICD10-CM:I10", "ICD10-CM:I11", "ICD10-CM:I12", "ICD10-CM:I13", "ICD10-CM:I14", "ICD10-CM:I15", "ICD10-CM:I20", 
    "ICD10-CM:I21", "ICD10-CM:I22", "ICD10-CM:I23", "ICD10-CM:I24", "ICD10-CM:I25", "ICD10-CM:N18", "ICD10-CM:J44", "ICD10-CM:C00", 
    "ICD10-CM:C01", "ICD10-CM:C02", "ICD10-CM:C03", "ICD10-CM:C04", "ICD10-CM:C05", "ICD10-CM:C06", "ICD10-CM:C07", "ICD10-CM:C08", 
    "ICD10-CM:C09", "ICD10-CM:C10", "ICD10-CM:C11", "ICD10-CM:C12", "ICD10-CM:C13", "ICD10-CM:C14", "ICD10-CM:C15", "ICD10-CM:C16", 
    "ICD10-CM:C17", "ICD10-CM:C18", "ICD10-CM:C19", "ICD10-CM:C20", "ICD10-CM:C21", "ICD10-CM:C22", "ICD10-CM:C23", "ICD10-CM:C24", 
    "ICD10-CM:C25", "ICD10-CM:C26", "ICD10-CM:C30", "ICD10-CM:C31", "ICD10-CM:C32", "ICD10-CM:C33", "ICD10-CM:C34", "ICD10-CM:C37", 
    "ICD10-CM:C38", "ICD10-CM:C39", "ICD10-CM:C40", "ICD10-CM:C41", "ICD10-CM:C43", "ICD10-CM:C44", "ICD10-CM:C4A", "ICD10-CM:C45", 
    "ICD10-CM:C46", "ICD10-CM:C47", "ICD10-CM:C48", "ICD10-CM:C49", "ICD10-CM:C50", "ICD10-CM:C51", "ICD10-CM:C52", "ICD10-CM:C53", 
    "ICD10-CM:C54", "ICD10-CM:C55", "ICD10-CM:C56", "ICD10-CM:C57", "ICD10-CM:C58", "ICD10-CM:C60", "ICD10-CM:C61", "ICD10-CM:C62", 
    "ICD10-CM:C63", "ICD10-CM:C64", "ICD10-CM:C65", "ICD10-CM:C66", "ICD10-CM:C67", "ICD10-CM:C68", "ICD10-CM:C69", "ICD10-CM:C70", 
    "ICD10-CM:C71", "ICD10-CM:C72", "ICD10-CM:C73", "ICD10-CM:C74", "ICD10-CM:C75", "ICD10-CM:C76", "ICD10-CM:C77", "ICD10-CM:C78", 
    "ICD10-CM:C79", "ICD10-CM:C7A", "ICD10-CM:C7B", "ICD10-CM:C80", "ICD10-CM:C81", "ICD10-CM:C82", "ICD10-CM:C83", "ICD10-CM:C84", 
    "ICD10-CM:C85", "ICD10-CM:C86", "ICD10-CM:C88", "ICD10-CM:C90", "ICD10-CM:C91", "ICD10-CM:C92", "ICD10-CM:C93", "ICD10-CM:C94", 
    "ICD10-CM:C99", "ICD10-CM:C96", "ICD10-CM:D00", "ICD10-CM:D01", "ICD10-CM:D02", "ICD10-CM:D03", "ICD10-CM:D04", "ICD10-CM:D05", 
    "ICD10-CM:D06", "ICD10-CM:D07", "ICD10-CM:D09", "ICD10-CM:D10", "ICD10-CM:D11", "ICD10-CM:D12", "ICD10-CM:D13", "ICD10-CM:D14", 
    "ICD10-CM:D15", "ICD10-CM:D16", "ICD10-CM:D17", "ICD10-CM:D18", "ICD10-CM:D19", "ICD10-CM:D20", "ICD10-CM:D21", "ICD10-CM:D22", 
    "ICD10-CM:D23", "ICD10-CM:D24", "ICD10-CM:D25", "ICD10-CM:D26", "ICD10-CM:D27", "ICD10-CM:D28", "ICD10-CM:D29", "ICD10-CM:D30", 
    "ICD10-CM:D31", "ICD10-CM:D32", "ICD10-CM:D33", "ICD10-CM:D34", "ICD10-CM:D35", "ICD10-CM:D36", "ICD10-CM:D37", "ICD10-CM:D38", 
    "ICD10-CM:D39", "ICD10-CM:D40", "ICD10-CM:D41", "ICD10-CM:D42", "ICD10-CM:D43", "ICD10-CM:D44", "ICD10-CM:D45", "ICD10-CM:D46", 
    "ICD10-CM:D47", "ICD10-CM:D48", "ICD10-CM:N80", "ICD10-CM:N80.0", "ICD10-CM:N80.1", "ICD10-CM:N80.2", "ICD10-CM:N80.3", 
    "ICD10-CM:N80.4", "ICD10-CM:N80.5", "ICD10-CM:N80.6", "ICD10-CM:N80.8", "ICD10-CM:N80.9", "ICD10-CM:M05", "ICD10-CM:M06",
    "ICD9:555", "ICD9:556", "ICD9:042", "ICD9:043", "ICD9:044", "ICD9:250.01", "ICD9:250.11", "ICD9:250.21", "ICD9:250.31", 
    "ICD9:250.41", "ICD9:250.51", "ICD9:250.61", "ICD9:250.71", "ICD9:250.81", "ICD9:250.91", "ICD9:250.00", "ICD9:250.10", 
    "ICD9:250.20", "ICD9:250.30", "ICD9:250.40", "ICD9:250.50", "ICD9:250.60", "ICD9:250.70", "ICD9:250.80", "ICD9:250.90", 
    "ICD9:493", "ICD9:343", "ICD9:401", "ICD9:402", "ICD9:403", "ICD9:404", "ICD9:405", "ICD9:410", "ICD9:411", "ICD9:412", 
    "ICD9:413", "ICD9:414", "ICD9:585", "ICD9:490", "ICD9:491", "ICD9:492", "ICD9:493", "ICD9:494", "ICD9:495", "ICD9:496",  
    "ICD9:617", "ICD9:617.0", "ICD9:617.1", "ICD9:617.2", "ICD9:617.3", "ICD9:617.4", "ICD9:617.5", "ICD9:617.6", "ICD9:617.8", 
    "ICD9:617.9 ", "ICD9:714", "ICD9-CM:555", "ICD9-CM:556", "ICD9-CM:042", "ICD9-CM:250.01", "ICD9-CM:250.11", "ICD9-CM:250.21", 
    "ICD9-CM:250.31", "ICD9-CM:250.41", "ICD9-CM:250.51", "ICD9-CM:250.61", "ICD9-CM:250.71", "ICD9-CM:250.81", "ICD9-CM:250.91", 
    "ICD9-CM:250.00", "ICD9-CM:250.10", "ICD9-CM:250.20", "ICD9-CM:250.30", "ICD9-CM:250.40", "ICD9-CM:250.50", "ICD9-CM:250.60", 
    "ICD9-CM:250.70", "ICD9-CM:250.80", "ICD9-CM:250.90", "ICD9-CM:493", "ICD9-CM:343", "ICD9-CM:401", "ICD9-CM:402", "ICD9-CM:403", 
    "ICD9-CM:404", "ICD9-CM:405", "ICD9-CM:410", "ICD9-CM:411", "ICD9-CM:412", "ICD9-CM:413", "ICD9-CM:414", "ICD9-CM:585", 
    "ICD9-CM:490", "ICD9-CM:491", "ICD9-CM:492", "ICD9-CM:493", "ICD9-CM:494", "ICD9-CM:495", "ICD9-CM:496", 
    "ICD9-CM:617", "ICD9-CM:617.0", "ICD9-CM:617.1", "ICD9-CM:617.2", "ICD9-CM:617.3", 
    "ICD9-CM:617.4", "ICD9-CM:617.5", "ICD9-CM:617.6", "ICD9-CM:617.8", "ICD9-CM:617.9 ", "ICD9-CM:714",
    "ICD8:563", "ICD8:563.1", "ICD8:563.2", "ICD8:079.83", "ICD8:250.01", "ICD8:250.02", "ICD8:250.03", "ICD8:250.11", 
    "ICD8:250.12", "ICD8:250.13", "ICD8:250.21", "ICD8:250.22", "ICD8:250.23", "ICD8:250.31", "ICD8:250.32", "ICD8:250.33", 
    "ICD8:250.41", "ICD8:250.42", "ICD8:250.43", "ICD8:250.51", "ICD8:250.52", "ICD8:250.53", "ICD8:250.61", "ICD8:250.62", 
    "ICD8:250.63", "ICD8:250.71", "ICD8:250.72", "ICD8:250.73", "ICD8:250.81", "ICD8:250.82", "ICD8:250.83", "ICD8:250.91", 
    "ICD8:250.92", "ICD8:250.93", "ICD8:-", "ICD8:493", "ICD8:-", "ICD8:400.09", "ICD8:400,19. 400.29", "ICD8:400.39", 
    "ICD8:400.99", "ICD8:401.99", "ICD8:402.99", "ICD8:403.99", "ICD8:404.99", "ICD8:410.09", "ICD8:410.99", "ICD8:411.09", 
    "ICD8:411.99", "ICD8:412.09", "ICD8:412.99", "ICD8:413.09", "ICD8:413.99", "ICD8:414.09", "ICD8:414.99", "ICD8:584", 
    "ICD8:491", "ICD8:492", "ICD8:493", "ICD8:625.30", "ICD8:625.31", "ICD8:625.32", "ICD8:625.33", "ICD8:625.34", "ICD8:625.35", 
    "ICD8:625.36", "ICD8:625.37", "ICD8:625.38", "ICD8:625.39", "ICD8:712.09", "ICD8:712.19", "ICD8:712.39", "ICD8:712.59"]

Other_Mental_Codes = ["ICD10:F00", "ICD10:F01", "ICD10:F02", "ICD10:F03", "ICD10:F04", "ICD10:F05", "ICD10:F06", "ICD10:F07", "ICD10:F09", 
    "ICD10:F10", "ICD10:F1", "ICD10:F2", "ICD10:F30", "ICD10:F31", "ICD10:F34", "ICD10:F38", "ICD10:F39", "ICD10:F4", "ICD10:F5", 
    "ICD10:F6", "ICD10:F7", "ICD10:F8", "ICD10:F9", "ICD10-CM:F00", "ICD10-CM:F01", "ICD10-CM:F02", "ICD10-CM:F03", "ICD10-CM:F04", 
    "ICD10-CM:F05", "ICD10-CM:F06", "ICD10-CM:F07", "ICD10-CM:F09", "ICD10-CM:F10", "ICD10-CM:F1", "ICD10-CM:F2", "ICD10-CM:F30", 
    "ICD10-CM:F31", "ICD10-CM:F34", "ICD10-CM:F38", "ICD10-CM:F39", "ICD10-CM:F4", "ICD10-CM:F5", "ICD10-CM:F6", "ICD10-CM:F7", 
    "ICD10-CM:F8", "ICD10-CM:F9", "ICD9:046.1", "ICD9:279.1", "ICD9:290", "ICD9:290.0", "ICD9:290.1", "ICD9:290.2", "ICD9:290.3", 
    "ICD9:290.4", "ICD9:290.5", "ICD9:290.6", "ICD9:290.7", "ICD9:290.8", "ICD9:290.9", "ICD9:291", "ICD9:291.0", "ICD9:291.1", 
    "ICD9:291.2", "ICD9:291.3", "ICD9:291.4", "ICD9:291.5", "ICD9:291.6", "ICD9:291.7", "ICD9:291.8", "ICD9:291.9", "ICD9:292", 
    "ICD9:292.0", "ICD9:292.1", "ICD9:292.2", "ICD9:292.3", "ICD9:292.4", "ICD9:292.5", "ICD9:292.6", "ICD9:292.7", "ICD9:292.8", 
    "ICD9:292.9", "ICD9:293", "ICD9:293.0", "ICD9:293.1", "ICD9:293.2", "ICD9:293.3", "ICD9:293.4", "ICD9:293.5", "ICD9:293.6", 
    "ICD9:293.7", "ICD9:293.8", "ICD9:293.9", "ICD9:294.0", "ICD9:294.8", "ICD9:294.9", "ICD9:296.0", "ICD9:296.2", "ICD9:296.3", 
    "ICD9:296.4", "ICD9:296.7", "ICD9:296.9", "ICD9:297", "ICD9:297.0", "ICD9:297.1", "ICD9:297.2", "ICD9:297.3", "ICD9:297.4", "ICD9:297.5", 
    "ICD9:297.6", "ICD9:297.7", "ICD9:297.8", "ICD9:297.9", "ICD9:298.1", "ICD9:298.2", "ICD9:298.3", "ICD9:298.4", "ICD9:298.5", "ICD9:298.6", 
    "ICD9:298.7", "ICD9:298.8", "ICD9:298.9", "ICD9:299.0", "ICD9:299.1", "ICD9:299.8", "ICD9:299.9", "ICD9:300.0", "ICD9:300.1", "ICD9:300.2", 
    "ICD9:300.3", "ICD9:301", "ICD9:301.0", "ICD9:301.1", "ICD9:301.2", "ICD9:301.3", "ICD9:301.4", "ICD9:301.5", "ICD9:301.6", "ICD9:301.7", 
    "ICD9:301.8", "ICD9:301.9", "ICD9:302.1", "ICD9:302.2", "ICD9:302.3", "ICD9:302.4", "ICD9:302.5", "ICD9:302.6", "ICD9:302.7", "ICD9:302.8", 
    "ICD9:302.9", "ICD9:303", "ICD9:303.0", "ICD9:303.1", "ICD9:303.2", "ICD9:303.3", "ICD9:303.4", "ICD9:303.5", "ICD9:303.6", "ICD9:303.7", 
    "ICD9:303.8", "ICD9:303.9", "ICD9:304", "ICD9:304.0", "ICD9:304.1", "ICD9:304.2", "ICD9:304.3", "ICD9:304.4", "ICD9:304.5", "ICD9:304.6", 
    "ICD9:304.7", "ICD9:304.8", "ICD9:304.9", "ICD9:305", "ICD9:305.0", "ICD9:305.1", "ICD9:305.2", "ICD9:305.3", "ICD9:305.4", "ICD9:305.5", 
    "ICD9:305.6", "ICD9:305.7", "ICD9:305.8", "ICD9:305.9", "ICD9:306", "ICD9:306.0", "ICD9:306.1", "ICD9:306.2", "ICD9:306.3", "ICD9:306.4", 
    "ICD9:306.5", "ICD9:306.6", "ICD9:306.7", "ICD9:306.8", "ICD9:306.9", "ICD9:307.0", "ICD9:307.1", "ICD9:307.2", "ICD9:307.3", "ICD9:307.4", 
    "ICD9:307.5", "ICD9:307.5", "ICD9:307.6", "ICD9:307.7", "ICD9:307.9", "ICD9:308", "ICD9:308.0", "ICD9:308.1", "ICD9:308.2", "ICD9:308.3", 
    "ICD9:308.4", "ICD9:308.5", "ICD9:308.6", "ICD9:308.7", "ICD9:308.8", "ICD9:308.9", "ICD9:309", "ICD9:309.0", "ICD9:309.1", "ICD9:309.2", 
    "ICD9:309.3", "ICD9:309.4", "ICD9:309.5", "ICD9:309.6", "ICD9:309.7", "ICD9:309.8", "ICD9:309.9", "ICD9:309.2", "ICD9:309.8", "ICD9:310", 
    "ICD9:310.0", "ICD9:310.1", "ICD9:310.2", "ICD9:310.3", "ICD9:310.4", "ICD9:310.5", "ICD9:310.6", "ICD9:310.7", "ICD9:310.8", "ICD9:310.9", 
    "ICD9:312.0", "ICD9:312.1", "ICD9:312.2", "ICD9:312.3", "ICD9:312.8", "ICD9:312.9", "ICD9:313.0", "ICD9:313.3", "ICD9:313.8", "ICD9:313.9", 
    "ICD9:314.0", "ICD9:314.1", "ICD9:314.2", "ICD9:314.8", "ICD9:314.9", "ICD9:315.0", "ICD9:315.1", "ICD9:315.2", "ICD9:315.3", "ICD9:315.4", 
    "ICD9:315.5", "ICD9:315.8", "ICD9:315.9", "ICD9:316", "ICD9:316.0", "ICD9:316.1", "ICD9:316.2", "ICD9:316.3", "ICD9:316.4", "ICD9:316.5", 
    "ICD9:316.6", "ICD9:316.7", "ICD9:316.8", "ICD9:316.9", "ICD9:317", "ICD9:317.0", "ICD9:317.1", "ICD9:317.2", "ICD9:317.3", "ICD9:317.4", 
    "ICD9:317.5", "ICD9:317.6", "ICD9:317.7", "ICD9:317.8", "ICD9:317.9", "ICD9:318.0", "ICD9:318.1", "ICD9:318.2", "ICD9:319", "ICD9:319.0", 
    "ICD9:319.1", "ICD9:319.2", "ICD9:319.3", "ICD9:319.4", "ICD9:319.5", "ICD9:319.6", "ICD9:319.7", "ICD9:319.8", "ICD9:319.9", "ICD9:332.0", 
    "ICD9:333.4", "ICD9:V40.2", "ICD9:V40.3", "ICD9:V40.9", "ICD9:V61.1", "ICD9-CM:046.1", "ICD9-CM:279.1", "ICD9-CM:290", "ICD9-CM:290.0", 
    "ICD9-CM:290.1", "ICD9-CM:290.2", "ICD9-CM:290.3", "ICD9-CM:290.4", "ICD9-CM:290.5", "ICD9-CM:290.6", "ICD9-CM:290.7", "ICD9-CM:290.8", 
    "ICD9-CM:290.9", "ICD9-CM:291", "ICD9-CM:291.0", "ICD9-CM:291.1", "ICD9-CM:291.2", "ICD9-CM:291.3", "ICD9-CM:291.4", "ICD9-CM:291.5", 
    "ICD9-CM:291.6", "ICD9-CM:291.7", "ICD9-CM:291.8", "ICD9-CM:291.9", "ICD9-CM:292", "ICD9-CM:292.0", "ICD9-CM:292.1", "ICD9-CM:292.2", 
    "ICD9-CM:292.3", "ICD9-CM:292.4", "ICD9-CM:292.5", "ICD9-CM:292.6", "ICD9-CM:292.7", "ICD9-CM:292.8", "ICD9-CM:292.9", "ICD9-CM:293", 
    "ICD9-CM:293.0", "ICD9-CM:293.1", "ICD9-CM:293.2", "ICD9-CM:293.3", "ICD9-CM:293.4", "ICD9-CM:293.5", "ICD9-CM:293.6", "ICD9-CM:293.7", 
    "ICD9-CM:293.8", "ICD9-CM:293.9", "ICD9-CM:294.0", "ICD9-CM:294.8", "ICD9-CM:294.9", "ICD9-CM:296", "ICD9-CM:296.0", "ICD9-CM:296.00", 
    "ICD9-CM:296.01", "ICD9-CM:296.02", "ICD9-CM:296.03", "ICD9-CM:296.04", "ICD9-CM:296.05", "ICD9-CM:296.2", "ICD9-CM:296.20", "ICD9-CM:296.21", 
    "ICD9-CM:296.22", "ICD9-CM:296.23", "ICD9-CM:296.24", "ICD9-CM:296.25", "ICD9-CM:296.3", "ICD9-CM:296.30", "ICD9-CM:296.31", "ICD9-CM:296.32", 
    "ICD9-CM:296.33", "ICD9-CM:296.34", "ICD9-CM:296.35", "ICD9-CM:296.4", "ICD9-CM:296.40", "ICD9-CM:296.41", "ICD9-CM:296.42", "ICD9-CM:296.43", 
    "ICD9-CM:296.44", "ICD9-CM:296.45", "ICD9-CM:296.7", "ICD9-CM:296.70", "ICD9-CM:296.71", "ICD9-CM:296.72", "ICD9-CM:296.73", "ICD9-CM:296.74", 
    "ICD9-CM:296.75", "ICD9-CM:296.8", "ICD9-CM:296.80", "ICD9-CM:296.81", "ICD9-CM:296.82", "ICD9-CM:296.83", "ICD9-CM:296.84", "ICD9-CM:296.85", 
    "ICD9-CM:296.86", "ICD9-CM:296.87", "ICD9-CM:296.88", "ICD9-CM:296.89", "ICD9-CM:296.9", "ICD9-CM:296.90", "ICD9-CM:296.91", "ICD9-CM:296.92", 
    "ICD9-CM:296.93", "ICD9-CM:296.94", "ICD9-CM:296.95", "ICD9-CM:296.96", "ICD9-CM:296.97", "ICD9-CM:296.98", "ICD9-CM:296.99", "ICD9-CM:297", 
    "ICD9-CM:297.0", "ICD9-CM:297.1", "ICD9-CM:297.2", "ICD9-CM:297.3", "ICD9-CM:297.4", "ICD9-CM:297.5", "ICD9-CM:297.6", "ICD9-CM:297.7", "ICD9-CM:297.8", 
    "ICD9-CM:297.9", "ICD9-CM:298.1", "ICD9-CM:298.2", "ICD9-CM:298.3", "ICD9-CM:298.4", "ICD9-CM:298.5", "ICD9-CM:298.6", "ICD9-CM:298.7", "ICD9-CM:298.8", 
    "ICD9-CM:298.9", "ICD9-CM:299.0", "ICD9-CM:299.00", "ICD9-CM:299.01", "ICD9-CM:299.1", "ICD9-CM:299.10", "ICD9-CM:299.11", "ICD9-CM:299.8", "ICD9-CM:299.80", 
    "ICD9-CM:299.81", "ICD9-CM:299.9", "ICD9-CM:299.90", "ICD9-CM:299.91", "ICD9-CM:300.0", "ICD9-CM:300.00", "ICD9-CM:300.01", "ICD9-CM:300.02", "ICD9-CM:300.03", 
    "ICD9-CM:300.04", "ICD9-CM:300.05", "ICD9-CM:300.06", "ICD9-CM:300.07", "ICD9-CM:300.08", "ICD9-CM:300.09", "ICD9-CM:300.1", "ICD9-CM:300.2", "ICD9-CM:300.20", 
    "ICD9-CM:300.21", "ICD9-CM:300.22", "ICD9-CM:300.23", "ICD9-CM:300.24", "ICD9-CM:300.25", "ICD9-CM:300.26", "ICD9-CM:300.2", "ICD9-CM:300.28", "ICD9-CM:300.29", 
    "ICD9-CM:300.3", "ICD9-CM:300.30", "ICD9-CM:301", "ICD9-CM:301.0", "ICD9-CM:301.1", "ICD9-CM:301.2", "ICD9-CM:301.3", "ICD9-CM:301.4", "ICD9-CM:301.5", 
    "ICD9-CM:301.6", "ICD9-CM:301.7", "ICD9-CM:301.8", "ICD9-CM:301.81", "ICD9-CM:301.82", "ICD9-CM:301.83", "ICD9-CM:301.84", "ICD9-CM:301.85", "ICD9-CM:301.86", 
    "ICD9-CM:301.87", "ICD9-CM:301.88", "ICD9-CM:301.89", "ICD9-CM:301.9", "ICD9-CM:302.1", "ICD9-CM:302.2", "ICD9-CM:302.3", "ICD9-CM:302.4", "ICD9-CM:302.5", 
    "ICD9-CM:302.6", "ICD9-CM:302.7", "ICD9-CM:302.8", "ICD9-CM:302.9", "ICD9-CM:303", "ICD9-CM:303.0", "ICD9-CM:303.00", "ICD9-CM:303.01", "ICD9-CM:303.02", 
    "ICD9-CM:303.03", "ICD9-CM:303.04", "ICD9-CM:303.05", "ICD9-CM:303.06", "ICD9-CM:303.07", "ICD9-CM:303.08", "ICD9-CM:303.09", "ICD9-CM:303.1", "ICD9-CM:303.10", 
    "ICD9-CM:303.12", "ICD9-CM:303.13", "ICD9-CM:303.14", "ICD9-CM:303.15", "ICD9-CM:303.16", "ICD9-CM:303.17", "ICD9-CM:303.18", "ICD9-CM:303.19", "ICD9-CM:303.2", 
    "ICD9-CM:303.20", "ICD9-CM:303.21", "ICD9-CM:303.22", "ICD9-CM:303.23", "ICD9-CM:303.24", "ICD9-CM:303.25", "ICD9-CM:303.26", "ICD9-CM:303.27", "ICD9-CM:303.28", 
    "ICD9-CM:303.29", "ICD9-CM:303.3", "ICD9-CM:303.30", "ICD9-CM:303.31", "ICD9-CM:303.32", "ICD9-CM:303.33", "ICD9-CM:303.34", "ICD9-CM:303.35", "ICD9-CM:303.36", 
    "ICD9-CM:303.37", "ICD9-CM:303.38", "ICD9-CM:303.39", "ICD9-CM:303.4", "ICD9-CM:303.40", "ICD9-CM:303.41", "ICD9-CM:303.42", "ICD9-CM:303.43", "ICD9-CM:303.44", 
    "ICD9-CM:303.45", "ICD9-CM:303.46", "ICD9-CM:303.47", "ICD9-CM:303.48", "ICD9-CM:303.49", "ICD9-CM:303.5", "ICD9-CM:303.50", "ICD9-CM:303.51", "ICD9-CM:303.52", 
    "ICD9-CM:303.53", "ICD9-CM:303.54", "ICD9-CM:303.55", "ICD9-CM:303.56", "ICD9-CM:303.57", "ICD9-CM:303.58", "ICD9-CM:303.59", "ICD9-CM:303.6", "ICD9-CM:303.60", 
    "ICD9-CM:303.61", "ICD9-CM:303.62", "ICD9-CM:303.63", "ICD9-CM:303.64", "ICD9-CM:303.65", "ICD9-CM:303.66", "ICD9-CM:303.67", "ICD9-CM:303.68", "ICD9-CM:303.69", 
    "ICD9-CM:303.7", "ICD9-CM:303.70", "ICD9-CM:303.71", "ICD9-CM:303.72", "ICD9-CM:303.73", "ICD9-CM:303.74", "ICD9-CM:303.75", "ICD9-CM:303.76", "ICD9-CM:303.77", 
    "ICD9-CM:303.78", "ICD9-CM:303.79", "ICD9-CM:303.8", "ICD9-CM:303.80", "ICD9-CM:303.81", "ICD9-CM:303.82", "ICD9-CM:303.83", "ICD9-CM:303.84", "ICD9-CM:303.85", 
    "ICD9-CM:303.86", "ICD9-CM:303.87", "ICD9-CM:303.88", "ICD9-CM:303.89", "ICD9-CM:303.9", "ICD9-CM:303.90", "ICD9-CM:303.91", "ICD9-CM:304", "ICD9-CM:304.0", 
    "ICD9-CM:304.01", "ICD9-CM:304.02", "ICD9-CM:304.03", "ICD9-CM:304.04", "ICD9-CM:304.05", "ICD9-CM:304.06", "ICD9-CM:304.07", "ICD9-CM:304.08", "ICD9-CM:304.09", 
    "ICD9-CM:304.1", "ICD9-CM:304.10", "ICD9-CM:304.11", "ICD9-CM:304.12", "ICD9-CM:304.13", "ICD9-CM:304.14", "ICD9-CM:304.15", "ICD9-CM:304.16", "ICD9-CM:304.17", 
    "ICD9-CM:304.18", "ICD9-CM:304.19", "ICD9-CM:304.2", "ICD9-CM:304.20", "ICD9-CM:304.21", "ICD9-CM:304.22", "ICD9-CM:304.23", "ICD9-CM:304.24", "ICD9-CM:304.25", 
    "ICD9-CM:304.26", "ICD9-CM:304.27", "ICD9-CM:304.28", "ICD9-CM:304.29", "ICD9-CM:304.3", "ICD9-CM:304.30", "ICD9-CM:304.31", "ICD9-CM:304.32", "ICD9-CM:304.33", 
    "ICD9-CM:304.34", "ICD9-CM:304.35", "ICD9-CM:304.36", "ICD9-CM:304.37", "ICD9-CM:304.38", "ICD9-CM:304.39", "ICD9-CM:304.4", "ICD9-CM:304.40", "ICD9-CM:304.41", 
    "ICD9-CM:304.42", "ICD9-CM:304.43", "ICD9-CM:304.44", "ICD9-CM:304.45", "ICD9-CM:304.46", "ICD9-CM:304.47", "ICD9-CM:304.48", "ICD9-CM:304.49", "ICD9-CM:304.5", 
    "ICD9-CM:304.50", "ICD9-CM:304.51", "ICD9-CM:304.52", "ICD9-CM:304.53", "ICD9-CM:304.54", "ICD9-CM:304.55", "ICD9-CM:304.56", "ICD9-CM:304.57", "ICD9-CM:304.58", 
    "ICD9-CM:304.59", "ICD9-CM:304.6", "ICD9-CM:304.60", "ICD9-CM:304.61", "ICD9-CM:304.62", "ICD9-CM:304.63", "ICD9-CM:304.64", "ICD9-CM:304.65", "ICD9-CM:304.66", 
    "ICD9-CM:304.67", "ICD9-CM:304.68", "ICD9-CM:304.69", "ICD9-CM:304.7", "ICD9-CM:304.70", "ICD9-CM:304.71", "ICD9-CM:304.72", "ICD9-CM:304.73", "ICD9-CM:304.74", 
    "ICD9-CM:304.75", "ICD9-CM:304.76", "ICD9-CM:304.77", "ICD9-CM:304.78", "ICD9-CM:304.79", "ICD9-CM:304.8", "ICD9-CM:304.80", "ICD9-CM:304.81", "ICD9-CM:304.82", 
    "ICD9-CM:304.83", "ICD9-CM:304.84", "ICD9-CM:304.85", "ICD9-CM:304.86", "ICD9-CM:304.87", "ICD9-CM:304.88", "ICD9-CM:304.89", "ICD9-CM:304.9", "ICD9-CM:304.90", 
    "ICD9-CM:304.91", "ICD9-CM:304.92", "ICD9-CM:304.93", "ICD9-CM:304.94", "ICD9-CM:304.95", "ICD9-CM:304.96", "ICD9-CM:304.97", "ICD9-CM:304.98", "ICD9-CM:304.99", 
    "ICD9-CM:305", "ICD9-CM:305.0", "ICD9-CM:305.01", "ICD9-CM:305.02", "ICD9-CM:305.03", "ICD9-CM:305.1", "ICD9-CM:305.10", "ICD9-CM:305.11", "ICD9-CM:305.12", 
    "ICD9-CM:305.13", "ICD9-CM:305.2", "ICD9-CM:305.20", "ICD9-CM:305.21", "ICD9-CM:305.22", "ICD9-CM:305.23", "ICD9-CM:305.3", "ICD9-CM:305.30", "ICD9-CM:305.31", 
    "ICD9-CM:305.32", "ICD9-CM:305.33", "ICD9-CM:305.4", "ICD9-CM:305.40", "ICD9-CM:305.41", "ICD9-CM:305.42", "ICD9-CM:305.43", "ICD9-CM:305.5", "ICD9-CM:305.50", 
    "ICD9-CM:305.51", "ICD9-CM:305.52", "ICD9-CM:305.53", "ICD9-CM:305.6", "ICD9-CM:305.60", "ICD9-CM:305.61", "ICD9-CM:305.62", "ICD9-CM:305.63", "ICD9-CM:305.7", 
    "ICD9-CM:305.70", "ICD9-CM:305.71", "ICD9-CM:305.72", "ICD9-CM:305.73", "ICD9-CM:305.8", "ICD9-CM:305.80", "ICD9-CM:305.81", "ICD9-CM:305.82", "ICD9-CM:305.83", 
    "ICD9-CM:305.9", "ICD9-CM:305.90", "ICD9-CM:305.91", "ICD9-CM:305.92", "ICD9-CM:305.93", "ICD9-CM:306", "ICD9-CM:306.0", "ICD9-CM:306.1", "ICD9-CM:306.2", "ICD9-CM:306.3", 
    "ICD9-CM:306.4", "ICD9-CM:306.5", "ICD9-CM:306.6", "ICD9-CM:306.7", "ICD9-CM:306.8", "ICD9-CM:306.9", "ICD9-CM:307.0", "ICD9-CM:307.1", "ICD9-CM:307.2", "ICD9-CM:307.20", 
    "ICD9-CM:307.21", "ICD9-CM:307.22", "ICD9-CM:307.23", "ICD9-CM:307.24", "ICD9-CM:307.25", "ICD9-CM:307.26", "ICD9-CM:307.27", "ICD9-CM:307.28", "ICD9-CM:307.29", "ICD9-CM:307.3", 
    "ICD9-CM:307.4", "ICD9-CM:307.40", "ICD9-CM:307.41", "ICD9-CM:307.42", "ICD9-CM:307.43", "ICD9-CM:307.44", "ICD9-CM:307.45", "ICD9-CM:307.46", "ICD9-CM:307.47", "ICD9-CM:307.48", 
    "ICD9-CM:307.49", "ICD9-CM:307.5", "ICD9-CM:307.5", "ICD9-CM:307.6", "ICD9-CM:307.7", "ICD9-CM:307.9", "ICD9-CM:308", "ICD9-CM:308.0", "ICD9-CM:308.1", "ICD9-CM:308.2", "ICD9-CM:308.3", 
    "ICD9-CM:308.4", "ICD9-CM:308.5", "ICD9-CM:308.6", "ICD9-CM:308.7", "ICD9-CM:308.8", "ICD9-CM:308.9", "ICD9-CM:309", "ICD9-CM:309.0", "ICD9-CM:309.1", "ICD9-CM:309.2", "ICD9-CM:309.3", 
    "ICD9-CM:309.4", "ICD9-CM:309.5", "ICD9-CM:309.6", "ICD9-CM:309.7", "ICD9-CM:309.8", "ICD9-CM:309.9", "ICD9-CM:310", "ICD9-CM:310.0", "ICD9-CM:310.1", "ICD9-CM:310.2", "ICD9-CM:310.3", 
    "ICD9-CM:310.4", "ICD9-CM:310.5", "ICD9-CM:310.6", "ICD9-CM:310.7", "ICD9-CM:310.8", "ICD9-CM:310.9", "ICD9-CM:312.0", "ICD9-CM:312.00", "ICD9-CM:312.01", "ICD9-CM:312.02", "ICD9-CM:312.03", 
    "ICD9-CM:312.04", "ICD9-CM:312.05", "ICD9-CM:312.06", "ICD9-CM:312.07", "ICD9-CM:312.08", "ICD9-CM:312.09", "ICD9-CM:312.1", "ICD9-CM:312.10", "ICD9-CM:312.11", "ICD9-CM:312.12", "ICD9-CM:312.13", 
    "ICD9-CM:312.14", "ICD9-CM:312.15", "ICD9-CM:312.16", "ICD9-CM:312.17", "ICD9-CM:312.18", "ICD9-CM:312.19", "ICD9-CM:312.2", "ICD9-CM:312.20", "ICD9-CM:312.21", "ICD9-CM:312.22", "ICD9-CM:312.23", 
    "ICD9-CM:312.24", "ICD9-CM:312.25", "ICD9-CM:312.26", "ICD9-CM:312.27", "ICD9-CM:312.28", "ICD9-CM:312.29", "ICD9-CM:312.3", "ICD9-CM:312.30", "ICD9-CM:312.31", "ICD9-CM:312.32", "ICD9-CM:312.33", 
    "ICD9-CM:312.34", "ICD9-CM:312.35", "ICD9-CM:312.36", "ICD9-CM:312.37", "ICD9-CM:312.38", "ICD9-CM:312.39", "ICD9-CM:312.8", "ICD9-CM:312.80", "ICD9-CM:312.81", "ICD9-CM:312.82", "ICD9-CM:312.83", 
    "ICD9-CM:312.84", "ICD9-CM:312.85", "ICD9-CM:312.86", "ICD9-CM:312.87", "ICD9-CM:312.88", "ICD9-CM:312.89", "ICD9-CM:312.9", "ICD9-CM:312.90", "ICD9-CM:312.91", "ICD9-CM:312.92", "ICD9-CM:312.93", 
    "ICD9-CM:312.94", "ICD9-CM:312.95", "ICD9-CM:312.96", "ICD9-CM:312.97", "ICD9-CM:312.98", "ICD9-CM:312.99", "ICD9-CM:313.0", "ICD9-CM:313.2", "ICD9-CM:313.3", "ICD9-CM:313.8", "ICD9-CM:313.9", 
    "ICD9-CM:314.0", "ICD9-CM:314.00", "ICD9-CM:314.01", "ICD9-CM:314.1", "ICD9-CM:314.2", "ICD9-CM:314.8", "ICD9-CM:314.9", "ICD9-CM:315.0", "ICD9-CM:315.00", "ICD9-CM:315.01", "ICD9-CM:315.02", 
    "ICD9-CM:315.03", "ICD9-CM:315.04", "ICD9-CM:315.05", "ICD9-CM:315.06", "ICD9-CM:315.07", "ICD9-CM:315.08", "ICD9-CM:315.09", "ICD9-CM:315.1", "ICD9-CM:315.2", "ICD9-CM:315.3", "ICD9-CM:315.4", 
    "ICD9-CM:315.5", "ICD9-CM:315.8", "ICD9-CM:315.9", "ICD9-CM:316", "ICD9-CM:316.0", "ICD9-CM:316.1", "ICD9-CM:316.2", "ICD9-CM:316.3", "ICD9-CM:316.4", "ICD9-CM:316.5", "ICD9-CM:316.6", "ICD9-CM:316.7", 
    "ICD9-CM:316.8", "ICD9-CM:316.9", "ICD9-CM:317", "ICD9-CM:317.0", "ICD9-CM:317.1", "ICD9-CM:317.2", "ICD9-CM:317.3", "ICD9-CM:317.4", "ICD9-CM:317.5", "ICD9-CM:317.6", "ICD9-CM:317.7", "ICD9-CM:317.8", 
    "ICD9-CM:317.9", "ICD9-CM:318.0", "ICD9-CM:318.1", "ICD9-CM:318.2", "ICD9-CM:319", "ICD9-CM:319.0", "ICD9-CM:319.1", "ICD9-CM:319.2", "ICD9-CM:319.3", "ICD9-CM:319.4", "ICD9-CM:319.5", "ICD9-CM:319.6", 
    "ICD9-CM:319.7", "ICD9-CM:319.8", "ICD9-CM:319.9", "ICD9-CM:332.0", "ICD9-CM:333.4", "ICD9-CM:V40.2", "ICD9-CM:V40.3", "ICD9-CM:V40.9", "ICD9-CM:V61.1",
    "ICD8:290", "ICD8:290.1", "ICD8:291", "ICD8:291.2", "ICD8:291.3", "ICD8:291.9", "ICD8:292", "ICD8:292.9", "ICD8:293", "ICD8:293.1", "ICD8:293.4", "ICD8:293.9", "ICD8:294", "ICD8:294.3", "ICD8:294.4", 
    "ICD8:294.8", "ICD8:294.9", "ICD8:295", "ICD8:295.1", "ICD8:295.2", "ICD8:295.3", "ICD8:295.4", "ICD8:295.5", "ICD8:295.6", "ICD8:295.7", "ICD8:295.8", "ICD8:295.9", "ICD8:296.1", "ICD8:296.3", 
    "ICD8:297", "ICD8:297.1", "ICD8:297.9", "ICD8:298.1", "ICD8:298.2", "ICD8:298.3", "ICD8:298.9", "ICD8:299", "ICD8:300", "ICD8:300.1", "ICD8:300.2", "ICD8:300.3", "ICD8:300.5", "ICD8:300.6", "ICD8:300.7", 
    "ICD8:300.8", "ICD8:300.9", "ICD8:301", "ICD8:301.1", "ICD8:301.2", "ICD8:301.3", "ICD8:301.4", "ICD8:301.5", "ICD8:301.6", "ICD8:301.7", "ICD8:301.8", "ICD8:301.9", "ICD8:302.1", "ICD8:302.3", 
    "ICD8:302.4", "ICD8:302.8", "ICD8:302.9", "ICD8:303", "ICD8:303.1", "ICD8:303.2", "ICD8:303.9", "ICD8:304", "ICD8:304.1", "ICD8:304.2", "ICD8:304.3", "ICD8:304.4", "ICD8:304.5", "ICD8:304.7", "ICD8:304.8", 
    "ICD8:304.9", "ICD8:305", "ICD8:305.1", "ICD8:305.2", "ICD8:305.3", "ICD8:305.4", "ICD8:305.5", "ICD8:305.6", "ICD8:305.7", "ICD8:305.8", "ICD8:305.9", "ICD8:306", "ICD8:306.1", "ICD8:306.2", "ICD8:306.3", 
    "ICD8:306.4", "ICD8:306.5", "ICD8:306.6", "ICD8:306.7", "ICD8:306.8", "ICD8:306.9", "ICD8:307", "ICD8:308", "ICD8:308.1", "ICD8:308.2", "ICD8:308.4", "ICD8:309", "ICD8:309.2", "ICD8:309.9", "ICD8:311", 
    "ICD8:312", "ICD8:313", "ICD8:314", "ICD8:315", "ICD8:315.1", "ICD8:315.2", "ICD8:315.3", "ICD8:315.4", "ICD8:315.5", "ICD8:315.7", "ICD8:315.8", "ICD8:315.9", "ICD8:342", "ICD8:390.1", "ICD8:393"]

Psychotic_Grant_Codes = ["F32.2", "F33.3"]

NonPsychotic_Grant_Codes = ["F32.0", "F32.1", "F32.3", "F32.8", "F32.9", "F33.0", "F33.1", "F33.2", "F33.4", "F33.8", "F33.9", 296.2, 298.0, 300.4]

AUD_Grant_Codes = ["E24.4", "G31.2", "G62.1", "G72.1", "I42.6", "K29.2", "K70", "K85.2", "K86.0", "O35.4", "T51", "F10", 291.09, 291.19, 291.29, 291.39, 291.49, 291.59, 291.69, 291.79, 291.89, 291.99, 303.09, 303.19, 303.29, 303.39, 303.49, 303.59, 303.69, 303.79, 303.89, 303.99, 303.20, 303.38, 303.90]

DUD_Grant_Codes = ["F11", "F12", "F13", "F14", "F15", "F16", "F18", "F19", 304]


# Function to generate random alphanumeric string
def generate_cpr_enc():
    characters = '0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ/+'
    return ''.join(random.choices(characters, k=40)) + "=="

# Function to generate random date between start_date and end_date
def generate_random_date(start_date, end_date):
    days_difference = (end_date - start_date).days
    random_days = random.randint(0, days_difference)
    return start_date + timedelta(days=random_days)

def generate_test_dataset(odir):
    # Generate File1
    np.random.seed(0)
    num_entries = 1000
    cpr_enc = [generate_cpr_enc() for _ in range(num_entries)]
    sex = np.random.choice(['M', 'F'], num_entries)
    birthdate = [generate_random_date(datetime(1950, 1, 1), datetime(2000, 12, 31)) for _ in range(num_entries)]
    df1 = pd.DataFrame({'cpr_enc': cpr_enc,
                        'sex': sex,
                        'birthdate': birthdate})
    # Generate dbds, degen_old, degen_new
    choices = [True, False]
    df1['dbds'] = np.random.choice(choices, num_entries)
    df1['degen_old'] = np.random.choice(choices, num_entries)
    df1['degen_new'] = np.random.choice(choices, num_entries)
    # Generate File2
    dates_in = [generate_random_date(b, datetime.now()) for b in birthdate]
    dates_out = [generate_random_date(d, datetime.now()) for d in dates_in]
    dates_in2 = [generate_random_date(b, datetime.now()) for b in birthdate]
    dates_out2 = [generate_random_date(d, datetime.now()) for d in dates_in2]
    types = ['A', 'B', '+', 'G', 'H']
    diagnosis = ['ICD10:DR074', 'ICD10:DZ016', 'ICD10:DS011', 'ICD10:DR075', 'ICD10:DR021', 
                'ICD10:DF330', 'ICD10:DF32', 'ICD10:DF320', 'ICD10:DF321', 'ICD10:DF322', 
                'ICD10:DF323', 'ICD10:DF328', 'ICD10:DF329', 'ICD10:DF33', 'ICD10:DF330', 
                'ICD10:DF331', 'ICD10:DF332', 'ICD10:DF333', 'ICD10:DF334', 'ICD10:DF338', 
                'ICD10:DF339', 'ICD10:DF70', 'ICD10:DF700', 'ICD10:DF701', 'ICD10:DF708', 
                'ICD10:DF709', 'ICD10:DF71', 'ICD10:DF710', 'ICD10:DF122', 'ICD10:DF132', 
                'ICD10:DF142', 'ICD10:DF152', 'ICD10:DF162', 'ICD10:DF182', 'ICD10:DF00', 
                'ICD10:DF000', 'ICD10:DF001', 'ICD10:DF002', 'ICD10:DF009']
    source = ['lpr_diag', 'lpr_psyk_diag', 'lpr3_diagnoses']
    type_diagnosis = np.random.choice(diagnosis, num_entries)
    type_diagnosis2 = np.random.choice(diagnosis, num_entries)
    type_values = np.random.choice(types, num_entries)
    source_values = np.random.choice(source, num_entries)
    in_pheno_codes = pd.DataFrame({'cpr_enc': cpr_enc,
                        'source': source_values,
                        'date_in': dates_in,
                        'date_out': dates_out,
                        'type': type_values,
                        'diagnosis': type_diagnosis})
    in_pheno_codes2 = pd.DataFrame({'cpr_enc': cpr_enc,
                                'source': source_values,
                                'date_in': dates_in2,
                                'date_out': dates_out2,
                                'type': type_values,
                                'diagnosis': type_diagnosis2})
    in_pheno_codes = pd.concat([in_pheno_codes, in_pheno_codes2], ignore_index=True)
    # Generate File3
    c_status = [random.choice([0, 10, 20, 30, 50, 70, 80, 90]) for _ in range(num_entries)]
    d_foddato = birthdate
    d_status_hen_start = [generate_random_date(out, datetime.now()) for out in dates_out]
    c_kon = np.random.choice(['K', 'M'], num_entries)
    df3 = pd.DataFrame({'cpr_enc': cpr_enc,
                        'C_STATUS': c_status,
                        'D_STATUS_HEN_START': d_status_hen_start,
                        'D_FODDATO': d_foddato,
                        'C_KON': c_kon})
    # Save to CSV files
    in_pheno_codes.to_csv("./lpr_file.csv", index=False)
    df1.to_csv("./stam_file.csv", index=False)
    df3.to_csv("./addition_information_file.csv", index=False)
    test_mdd_pheno = "diagnosis\nICD10:DF32\nICD10:DF32.0\nICD10:DF32.1\nICD10:DF32.2\nICD10:DF32.3\nICD10:DF32.8\nICD10:DF32.9\nICD10:DF33\nICD10:DF33.0\nICD10:DF33.1\nICD10:DF33.2\nICD10:DF33.3\nICD10:DF33.4\nICD10:DF33.8\nICD10:DF33.9"
    test_mdd_pheno_df = pd.DataFrame({'diagnosis': test_mdd_pheno.split('\n')})
    # Save the DataFrame to a CSV file
    test_mdd_pheno_df.to_csv("./sample_pheno_request.txt", index=False)

def split_and_format(input_str, fill=False):
    # Use regex to split the string into letter, integer, and decimal parts
    if ":" in input_str:
        match = re.match(r"([A-Za-z]+)(\d+)(?:\.(\d+))?", input_str)
    else:
        match = re.match(r"([A-Za-z]*)(\d+)(?:\.(\d+))?", input_str)
    
    if match:
        string_part = str(match.group(1))
        integer_part = str(match.group(2))
        decimal_part = str(match.group(3))

        # Combine integer and decimal parts
        if decimal_part == 'None':
            combined_number = integer_part
        else:
            combined_number = integer_part + decimal_part
        
        # Ensure the combined number is 4 digits by adding trailing zeros if necessary
        if fill:
            combined_number = combined_number.ljust(4, '0')
        else:
            combined_number = combined_number
        
        # Reconstruct the final string
        final_string = string_part + combined_number
        #final_string = combined_number
        
        #return string_part, integer_part, decimal_part, final_string
        return final_string
    else:
        raise ValueError("Input string format is invalid: (",type(input_str),") ", input_str)

def add_items_to_comma_list(old_comma_list, new_list_item):
    temp_list = old_comma_list.apply(lambda x: [item for item in x.split(',') if item.strip() != ''])
    temp_list = old_comma_list + new_list_item
    return(','.join(temp_list))

# Function to get Memory usage
def usage():
    gc.collect()
    process = psutil.Process(os.getpid())
    mem_used = process.memory_info()[0] / float(1024 * 1024 * 1024) #2 ** 20)
    print("Memory usage in GB: "+str(mem_used))


# Check if the date is in a format so we can use pd.to_datetime or not
def is_valid_datetime(date_string, date_format):
    try:
        datetime.strptime(date_string, date_format)
        return True
    except ValueError:
        return False
    except TypeError:
        return False


# Function to convert DDMMYY10 to DD/MM/YYYY
def convert_date_old(date_str):
    if is_valid_datetime(date_str, "DD/MM/YYYY"):
        try:
            date_obj = datetime.strptime(str(date_str), "%d%m%y")
            formatted_date = date_obj.strftime("%d/%m/%Y")
            return formatted_date
        except ValueError:
            try:
                return pd.to_datetime(date_str, format=DateFormat)
            except:
                return "Invalid Date"  # Handle invalid date strings if needed
    else:
        try:
            return pd.to_datetime(date_str, format=DateFormat)
        except:
            return "Invalid Date"  # Handle invalid date strings if needed

# Function to convert DDMMYY10 to DD/MM/YYYY
def convert_date(date_str):
    global DateFormat
    if isinstance(date_str, pd.Timestamp):
        return date_str  # Return the Timestamp object directly
    try:
        # Try parsing as DDMMYY10 format
        date_obj = datetime.strptime(str(date_str), DateFormat)
        return pd.to_datetime(date_obj, format=DateFormat)
    except ValueError:
        try:
            # Try parsing as a datetime object using pandas
            return pd.to_datetime(date_str, errors='coerce', format=DateFormat)
        except ValueError:
            return None  # Return None if the date format is invalid

# Function to add appropriate coding to diagnostic codes for CHB/DBDS Data
def update_icd_coding_DBDSCHB(data):
    data = ','.join(map(str, data))
    data = data.split(",")
    data = [
        "ICD8:" + str(value) if isinstance(value, (int, float)) else "ICD10:D" + str(value).replace('.', '')
        for value in data
    ]
    return data

def update_icd_coding(data, dst=False, dbdschb=False, ipsych=False, eM=False, skip=False, remove_point_in_diag_request=False, ICDCM=False, RegisterRun=False, no_Fill=False, remove_leading_ICD=False):
    if (not skip):
        output_list = []
        updateICD9 = False
        updateICD8 = False
        cantupdateICD8 = False
        Unknown_system = False
        global ATC_Requested
        print_which_section = True
        if (not type(data) == list):
            data = data[data.columns[0]].values.tolist()
        for entry in data:
            #print(f"In update_icd_coding with: {entry}")
            skipEntry=False
            if ICDCM and not dbdschb and not ipsych and not dst:
                if print_which_section:
                    print_which_section = False
                    print("In ICD10CM ICDupdate")
                if isinstance(entry, str) and entry.startswith('ATC:'):
                    entry = entry
                    #ATC_Requested = "All"
                elif isinstance(entry, str) and entry.startswith('ICD8:'):
                    if remove_leading_ICD:
                        entry_str = str(entry).replace('ICD8:', '', 1).upper()
                    else:
                        entry_str = str(entry)
                    integer_part, _, decimal_part = entry_str.partition('.')
                    integer_part = integer_part.zfill(3)
                    entry = f"ICD8:{integer_part}.{decimal_part.zfill(2) if eM else decimal_part}"
                elif isinstance(entry, str) and entry.startswith('ICD9-CM:'):
                    if remove_leading_ICD:
                        entry = entry.replace('ICD9-CM:', '', 1).upper()
                    else:
                        entry = 'ICD9:' + entry.replace('ICD9-CM:', '', 1).upper()
                elif isinstance(entry, str) and entry.startswith('ICD10-CM:'):
                    if remove_leading_ICD:
                        entry = entry.replace('ICD10-CM:', '', 1).upper()
                    else:
                        entry = 'ICD10:' + entry.replace('ICD10-CM:', '', 1).upper()
                elif isinstance(entry, (int, float)) and str(entry).isdigit():
                    updateICD8 = True
                    entry_str = str(entry)
                    integer_part, _, decimal_part = entry_str.partition('.')
                    integer_part = integer_part.zfill(3)
                    if remove_leading_ICD:
                        entry = f"{integer_part}.{decimal_part.zfill(2) if eM else decimal_part}"
                    else:
                        entry = f"ICD8:{integer_part}.{decimal_part.zfill(2) if eM else decimal_part}"
            # Check if 'ICD9:' or 'ICD10:D' is already added
            elif dbdschb: #https://medinfo.dk/sks/brows.php check this for information about the Danish codes
                if print_which_section:
                    print("In DBDS ICDupdate")
                    print_which_section = False
                if isinstance(entry, str) and entry.startswith('ATC:'):
                    entry = entry.replace('ATC:', '', 1).upper()
                elif isinstance(entry, str) and entry.startswith('ICD9-CM:'):
                    skipEntry=True
                elif isinstance(entry, str) and entry.startswith('ICD10-CM:'):
                    skipEntry=True
                elif isinstance(entry, str) and entry.startswith('ICD8:'):
                    entry_str = str(entry).upper()#.replace('ICD8:', '', 1).upper()
                    integer_part, _, decimal_part = entry_str.partition('.')
                    integer_part = integer_part.zfill(3)
                    entry = f"{integer_part}.{decimal_part.zfill(2) if eM else decimal_part}"
                elif isinstance(entry, str) and entry.startswith('ICD9:'):
                    skipEntry=True
                elif isinstance(entry, str) and not str(entry).isdigit() and not entry.startswith('ICD10:') and not entry.startswith('ICD10:D'):
                    # Check if the string starts with exactly two letters followed by numbers
                    if re.match(r'^[A-Z]{2}\d', entry, re.IGNORECASE):
                        entry = 'ICD10:' + entry.upper()  # String already starts with two letters
                    elif re.match(r'^[A-Z]\d', entry, re.IGNORECASE):
                        entry = 'ICD10:D' + entry.upper()  # Add 'D' in front if it starts with one letter
                    else:
                        print(f"{entry} has more than two letters in before a numeric value. We don't know how to handle this and will leave it as is.") # No modification for other cases
                elif isinstance(entry, str) and entry.startswith('ICD10:') and not entry.startswith('ICD10:D'):
                    entry = 'ICD10:D' + entry.replace('ICD10:', '', 1).upper()
                elif isinstance(entry, str) and entry.startswith('ICD10:D'):
                    entry = 'ICD10:' + entry.replace('ICD10:', '', 1).upper()
                elif isinstance(entry, (int, float)) and str(entry).isdigit():
                    updateICD8 = True
                    entry_str = str(entry)
                    integer_part, _, decimal_part = entry_str.partition('.')
                    integer_part = integer_part.zfill(3)
                    entry = f"ICD8:{integer_part}.{decimal_part.zfill(2) if eM else decimal_part}"
                entry = str(entry).replace('.', '')  # remove '.' from the entry
            elif RegisterRun:
                if print_which_section:
                    print_which_section = False
                    print("You did select --RegisterRun in your flags. Thus, we will only focus on ICD10 and ICD8 input data. ICD10 will be reformmated to DYXXxx and ICD8 will be just the numeric value. points within the data will be replaced and the length of the code will be adjusted to 5.")
                if isinstance(entry, str) and entry.startswith('ATC:'):
                    entry = entry
                elif isinstance(entry, str) and entry.startswith('ICD9-CM:'):
                    skipEntry=True
                elif isinstance(entry, str) and entry.startswith('ICD10-CM:'):
                    skipEntry=True
                elif isinstance(entry, str) and entry.startswith('ICD8:'):
                    entry = entry.replace('ICD8:', '', 1)
                    updateICD8 = True
                    entry_str = str(entry)
                    integer_part, _, decimal_part = entry_str.partition('.')
                    integer_part = integer_part.zfill(3)
                    entry = f"{integer_part}.{decimal_part.zfill(2) if eM else decimal_part}"
                elif isinstance(entry, str) and entry.startswith('ICD9:') and not entry.startswith('ICD9-CM:'):
                    skipEntry=True
                elif isinstance(entry, str) and not str(entry).isdigit() and not entry.startswith('ICD10:') and not entry.startswith('ICD10:D'):
                    # Check if the string starts with exactly two letters followed by numbers
                    if re.match(r'^[A-Z]{2}\d', entry, re.IGNORECASE):
                        entry = entry.upper()  # String already starts with two letters
                    elif re.match(r'^[A-Z]\d', entry, re.IGNORECASE):
                        entry = 'D' + entry.upper()  # Add 'D' in front if it starts with one letter
                    else:
                        print(f"{entry} has more than two letters in before a numeric value. We don't know how to handle this and will leave it as is.") # No modification for other cases
                elif isinstance(entry, str) and entry.startswith('ICD10:') and not entry.startswith('ICD10:D') and not entry.startswith('ICD10-CM:'):
                    entry = 'D' + entry.replace('ICD10:', '', 1).upper()
                elif isinstance(entry, str) and not entry.startswith('ICD10:') and entry.startswith('ICD10:D') and not entry.startswith('ICD10-CM:'):
                    entry = entry.replace('ICD10:', '', 1).upper()
                elif isinstance(entry, (int, float)) and str(entry).isdigit() and not entry.startswith('ICD10-CM:') and not entry.startswith('ICD9-CM:'):
                    entry_str = str(entry)
                    integer_part, _, decimal_part = entry_str.partition('.')
                    integer_part = integer_part.zfill(3)
                    entry = f"{integer_part}.{decimal_part.zfill(2) if eM else decimal_part}"
                    entry = str(entry).replace('.', '')  # remove '.' from the entry
            elif ipsych or dst:
                if print_which_section:
                    print_which_section = False
                    print(f"In ipsych or dst ICDupdate: {entry}")
                if isinstance(entry, str) and entry.startswith('ATC:'):
                    entry = entry
                elif isinstance(entry, str) and entry.startswith('ICD9-CM:'):
                    skipEntry=True
                elif isinstance(entry, str) and entry.startswith('ICD10-CM:'):
                    skipEntry=True
                elif isinstance(entry, str) and entry.startswith('ICD8:'):
                    entry = entry.replace('ICD8:', '', 1)
                    entry_str = str(entry)
                    integer_part, _, decimal_part = entry_str.partition('.')
                    integer_part = integer_part.zfill(3)
                    entry = f"{integer_part}.{decimal_part.zfill(2) if eM else decimal_part}"
                elif isinstance(entry, str) and entry.startswith('ICD9:') and not entry.startswith('ICD9-CM:'):
                    skipEntry=True
                elif isinstance(entry, str) and not entry.startswith('ICD10:') and not entry.startswith('ICD10:D') and not str(entry).isdigit():
                    # Check if the string starts with exactly two letters followed by numbers
                    if re.match(r'^[A-Z]{2}\d', entry, re.IGNORECASE):
                        entry = entry.upper()  # String already starts with two letters
                    elif re.match(r'^[A-Z]\d', entry, re.IGNORECASE):
                        entry = 'D' + entry.upper()  # Add 'D' in front if it starts with one letter
                    else:
                        print(f"{entry} has more than two letters in before a numeric value. We don't know how to handle this and will leave it as is.") # No modification for other cases
                    entry = split_and_format(entry, fill=eM)
                elif isinstance(entry, str) and not str(entry).isdigit() and entry.startswith('ICD10:') and not str(entry).isdigit():
                    # Check if the string starts with exactly two letters followed by numbers
                    if re.match(r'^ICD10:D[A-Z]\d+', entry, re.IGNORECASE):
                        entry = entry.replace('ICD10:D', '', 1).upper()  # Case 1: Starts with ICD10:D
                    elif re.match(r'^ICD10:[A-Z]\d+', entry, re.IGNORECASE):
                        entry = 'D' + entry.replace('ICD10:', '', 1).upper()  # Case 2: Starts with ICD10:
                    else:
                        print(f"{entry} has more than two letters in before a numeric value. We don't know how to handle this and will leave it as is.") # No modification for other cases
                    entry = split_and_format(entry, fill=eM)
                elif isinstance(entry, (int, float)) and str(entry).isdigit() and not entry.startswith('ICD10-CM:') and not entry.startswith('ICD9-CM:'):
                    updateICD8 = True
                    entry_str = str(entry)
                    integer_part, _, decimal_part = entry_str.partition('.')
                    integer_part = integer_part.zfill(3)
                    entry = f"{integer_part}.{decimal_part.zfill(2) if eM else decimal_part}"
                entry = str(entry).replace('.', '')  # remove '.' from the entry
            else:
                if print_which_section:
                    print_which_section = False
                    print("In ELSE ICDupdate")
                if isinstance(entry, str) and entry.upper().startswith('ATC:'):
                    entry = entry
                elif isinstance(entry, str) and entry.upper().startswith('ICD8'):
                    if remove_leading_ICD:
                        entry_str = entry.replace('ICD8:', '', 1)
                    else:
                        entry_str = entry
                elif isinstance(entry, str) and entry.upper().startswith('ICD9'):
                    if remove_leading_ICD:
                        entry_str = entry.replace('ICD9:', '', 1)
                    else:
                        entry = entry
                elif isinstance(entry, str) and entry.upper().startswith('ICD10') :
                    if remove_leading_ICD:
                        entry_str = entry.replace('ICD10:', '', 1)
                    else:
                        entry = entry
                elif isinstance(entry, str) and entry.upper().startswith('ICD9-CM'):
                    if remove_leading_ICD:
                        entry_str = entry.replace('ICD9-CM:', '', 1)
                    else:
                        entry = entry
                elif isinstance(entry, str) and entry.upper().startswith('ICD10-CM') :
                    if remove_leading_ICD:
                        entry_str = entry.replace('ICD10-CM:', '', 1)
                    else:
                        entry = entry
                elif isinstance(entry, str) and entry.upper().startswith('ICD9CM'):
                    if remove_leading_ICD:
                        entry_str = entry.replace('ICD9CM:', '', 1)
                    else:
                        entry = entry
                elif isinstance(entry, str) and entry.upper().startswith('ICD10CM') :
                    if remove_leading_ICD:
                        entry_str = entry.replace('ICD10CM:', '', 1)
                    else:
                        entry = entry
                elif isinstance(entry, str):
                    entry = entry
                elif isinstance(entry, (int, float)) and str(entry).isdigit():
                    entry = entry
                Unknown_system = True
                if remove_point_in_diag_request or RegisterRun:
                    entry = str(entry).replace('.', '')  # remove '.' from the entry
            if (not skipEntry):
                output_list.append(entry)
            if(dbdschb or ipsych):
                #Remove -CM entries
                output_list = [entry for entry in output_list if not (entry.startswith('ICD10-CM:') or entry.startswith('ICD9-CM:'))]
        print(f"Final updated ICD codes: {output_list}")
        if (Unknown_system):
            print("INFO: As this tool does not know how to process your diagnostic codes, we will keep them as they are.")
        if (updateICD9):
            print("Caution! Some (or all) of your diagnostic codes (numeric) did not specify if they are ICD9 or ICD8. Thus we will interpret them for both. This may lead to spurious Case/Control behaviour! It is better to correctly specify your diagnostic codes, e.g. ICD8:290.1, ICD9:290.1, ICD10:DF30")
        if (updateICD8):
            if (dbdschb):
                print("Caution! Some (or all) of your diagnostic codes (numeric) did not specify if they are ICD9 or ICD8. As you are running this script on CHB/DBDS data, we will interpret them as ICD8 and (if needed) correct your input to the correct format [88.0 will become ICD8:08800 (if --eM) or otherwise ICD8:0880]. This may lead to spurious Case/Control behaviour! It is better to correctly specify your diagnostic codes, e.g. ICD8:088.1, ICD8:290.1, ICD9:290.1, ICD10:DF30")
            else:
                print("Caution! Some (or all) of your diagnostic codes (numeric) did not specify if they are ICD9 or ICD8.  Thus we will interpret them as ICD8 [88.0 will become ICD8:08800 (if --eM) or otherwise ICD8:0880]. This may lead to spurious Case/Control behaviour! It is better to correctly specify your diagnostic codes, e.g. ICD8:088.1, ICD8:290.1, ICD9:290.1, ICD10:DF30")
        if (cantupdateICD8):
            print("Caution! You supplied ICD8 codes. We can't check if they were given in the correct format. Plase make sure to have them written either in ICD8:XXXYY or ICD8:XXX format (X refers to the numbers before the decimal point and Y behind). If you are not supplying two Y, there may be a large proportion of codes been missed as the current codes in the database rely on the 5 digits, thus do NOT use the --eM flag!")
        return(output_list)
    else:
        return(data)


# Function to calculate the age at a givent timepoint
def calculate_age(row, date_column, dob_column):
    print("Current row: "+str(row))
    try:
        birth_date = row[dob_column]
        given_date = row[date_column]
        age = (given_date - birth_date).days // 365  # Calculate age in years
        return age
    except (TypeError, ValueError):
        return 0 # Handle missing or invalid dates

# Function to convert strings to datetime objects and find the earliest date
def get_earliest_date(date_list):
    return min(date_list)

def process_ExDEP_exclusions(data, iidcol, diagcol, dx_name, dates_name, verbose, update_diag_col_name_to_diag, get_earliest_date_from_data=False):
    # CHB/DBDS: A is main diagnosis, B is secondary, G is grundmorbus, H is referral - Excluding suspected diagnosis (through general practitionair who refers to Hospital) with coding H 
    if ('type' in data.columns):
        data = data.loc[data["type"].isin(["A","B","G","H"])]
    if (update_diag_col_name_to_diag and diagcol in data.columns):
        data.rename(columns={diagcol:"diagnosis"},inplace=True)
    if (verbose):
        print("data: ",data.columns)
        print(data.head(2))
    grouped = data.groupby([iidcol],group_keys=False)
    if (verbose):
        print(data.columns)
        print(iidcol)
        print(diagcol)
        print(dx_name)
        print(verbose)
        print(get_earliest_date_from_data)
        print(grouped["diagnoses"].head(5))
        print(grouped["in_dates"].head(5))
    dx_string = grouped[['diagnoses']].first().reset_index()
    in_dates = grouped[['in_dates']].first().reset_index()
    if (get_earliest_date_from_data):
        if "first_dx" in data.columns:
            earliest_in_dates = grouped['first_dx']
        else:
            earliest_in_dates = grouped['in_dates'].apply(get_earliest_date).reset_index(name=(dx_name+'_earliest_date'))
        id_dx_date_df = pd.merge(dx_string, in_dates, on=iidcol).merge(earliest_in_dates, on=iidcol)
    else:
        id_dx_date_df = pd.merge(dx_string, in_dates, on=iidcol)
    id_dx_date_df.rename(columns={"diagnoses":dx_name},inplace=True)
    id_dx_date_df.rename(columns={"in_dates":dates_name},inplace=True)
    if (verbose):
        print(id_dx_date_df.head(2))
        print(id_dx_date_df.columns)
    del data
    del grouped
    del dx_string
    del in_dates
    return(id_dx_date_df)
    
def reformat_to_tsv(file):
    # Define the command to process the file
    command = f"""
    sed -e "s/\\[\\]//g" \
        -e "s/', '/,/g" \
        -e "s/\\['//g" \
        -e "s/'\\]//g" \
        -e "s/Timestamp('//g" \
        -e "s/ 00:00:00')//g" \
        -e "s/, /,/g" \
        -e "s/\\[//g" \
        -e "s/\\]//g" {file} > {file}.temp
    mv {file}.temp {file}
    """
    # Run the command
    subprocess.run(command, shell=True, check=True)


# Use boolean indexing to extract rows from the first file that match the values #values_to_match = '|'.join(map(re.escape, values_to_match_in))
#if (exact_match):
#    tmp_result_df = df1[df1[diagnostic_col].isin(values_to_match)].copy() #df1[df1[diagnostic_col].isin(values_to_match)].copy()
#else:
#    #tmp_result_df = df1[df1[diagnostic_col].str.contains(values_to_match, na=False)].copy()
#    tmp_result_df = df1[df1[diagnostic_col].str.contains('|'.join(str(values_to_match)), na=False)].copy()
#TODO: ATC checks and differing case identification
def build_phenotype_cases(df1, exact_match, values_to_match, diagnostic_col, birthdatecol, iidcol, input_date_in_name, input_date_out_name, verbose, Covariates=False, Covar_Name="", general_results=pd.DataFrame(), BuildEntryExitDates=False):
    if (type(df1) == list):
        if (type(values_to_match[0]) == list):
            if (type(diagnostic_col) == list):
                if (type(input_date_in_name) == list):
                    df1_noatc = df1[0]
                    df1_atc = df1[1]
                    values_to_match_noatc = values_to_match[0]
                    values_to_match_atc = values_to_match[1]
                    if (exact_match):
                        tmp_result_df = df1_noatc[df1_noatc[diagnostic_col[0]].isin(values_to_match_noatc)].copy()
                        tmp_result_df_atc = df1_atc[df1_atc[diagnostic_col[1]].isin(values_to_match_atc)].copy()
                        tmp_result_df_atc.rename(columns={diagnostic_col[0]:diagnostic_col[1]},inplace=True)
                        tmp_result_df_atc.rename(columns={input_date_in_name[0]:input_date_in_name[1]},inplace=True)
                    else:
                        tmp_result_df = df1_noatc[df1_noatc[diagnostic_col[0]].str.contains('|'.join(str(values_to_match_noatc)))].copy()
                        tmp_result_df_atc = df1_atc[df1_atc[diagnostic_col[1]].str.contains('|'.join(str(values_to_match_atc)))].copy()
                        tmp_result_df_atc.rename(columns={diagnostic_col[0]:diagnostic_col[1]},inplace=True)
                        tmp_result_df_atc.rename(columns={input_date_in_name[0]:input_date_in_name[1]},inplace=True)
                    tmp_result_df = pd.concat([tmp_result_df, tmp_result_df_atc], ignore_index=True, sort=False)
                else:
                    print("ERROR: Supplied Standard Diagnosis and ATC files and Codes for both and the column from which we should extract the codes, but not the date columns.")
                    sys.exit()
            else:
                print("ERROR: Supplied Standard Diagnosis and ATC files and Codes for both, but not the column from which we should extract the codes.")
                sys.exit()
        else:
            print("ERROR: Supplied Standard Diagnosis and ATC files, but not Codes for both.")
            sys.exit()
    else:
        if exact_match:
            tmp_result_df = df1[df1[diagnostic_col].isin(values_to_match)].copy()
        else:
            #pattern = '|'.join(map(str, values_to_match))
            #tmp_result_df = df1[df1[diagnostic_col].str.startswith(values_to_match, na=False)].copy()
            patterns = tuple(p.lower() for p in values_to_match)  # Normalize patterns to lowercase
            tmp_result_df = df1[df1[diagnostic_col].str.lower().str.startswith(patterns, na=False)].copy()
            del(patterns)
    if (verbose):
        print(tmp_result_df.columns)
        print(input_date_in_name, ", ",iidcol, ", ", diagnostic_col )
    if (tmp_result_df[iidcol].nunique() == 0 and Covariates == False):
        print("Error: No Cases found.")
        if Covariates == False:
            sys.exit()
    if Covariates == False:
        print("Identified initially ",str(tmp_result_df[iidcol].nunique())," IIDs that do have an overlapping diagnostic code.")
    # Build the Entry and Exit date based on first and last Entry
    if (BuildEntryExitDates):
        print("Estimating the Entry and Exit date for Cases based on the first and last available diagnosis (not specific to the currently requested diagnosis codes, but generally available data).")
        iid_list_cases = tmp_result_df[iidcol]
        tmp_cases_entry_exit_df = df1[df1[iidcol].isin(iid_list_cases)].copy()
        cols_to_delete = tmp_cases_entry_exit_df.columns.tolist()  # Convert Index object to list
        cols_to_delete.remove(iidcol)  # Remove 'iidcol' from the list of columns
        cols_to_delete.remove(input_date_in_name)
        cols_to_delete.remove(input_date_out_name)
        tmp_cases_entry_exit_df.drop(cols_to_delete, inplace=True, axis=1) 
        grouped = tmp_cases_entry_exit_df.groupby([iidcol],group_keys=False) 
        in_dates = grouped[input_date_in_name].min().reset_index(name='Entry_Date')
        if (input_date_out_name != input_date_in_name):
            out_dates = grouped[input_date_out_name].max().reset_index(name='Exit_Date')
        else:
            out_dates = grouped[input_date_in_name].max().reset_index(name='Exit_Date')
        Entry_Exit_date_df = in_dates.merge(out_dates, on=iidcol)
        tmp_result_df = tmp_result_df.merge(Entry_Exit_date_df, on=iidcol)
    if Covariates == False:
        return merge_IIDs(tmp_result_df, diagnostic_col, birthdatecol, input_date_in_name, input_date_out_name, iidcol, verbose, Cases=True, Covariates=Covariates, BuildEntryExitDates=BuildEntryExitDates)
    else:
        covar = merge_IIDs(tmp_result_df, diagnostic_col, birthdatecol, input_date_in_name, input_date_out_name, iidcol, verbose, Cases=True, Covariates=Covariates, BuildEntryExitDates=BuildEntryExitDates)
        #keep: "diagnoses" ,"in_dates", "first_dx"
        covar_cols = covar.columns
        if verbose:
            print("Columns in Covar file:",covar_cols)
            print("Columns in tmp_result_df file:",tmp_result_df.columns)
        cols_to_delete = covar.columns.tolist()  # Convert Index object to list
        cols_to_delete.remove(iidcol)  # Remove 'iidcol' from the list of columns
        cols_to_delete.remove("diagnosis")
        cols_to_delete.remove("diagnoses")
        cols_to_delete.remove("in_dates")
        cols_to_delete.remove("first_dx")
        cols_to_delete.remove("n_diags")
        cols_to_delete.remove("n_unique_in_days")
        if "type" in cols_to_delete:
            cols_to_delete.remove("type")
        covar.drop(cols_to_delete, inplace=True, axis=1)  # Drop the columns except 'iidcol'
        if (verbose):
            print("covar contains duplicated iids: ",covar[iidcol].duplicated().any())
        covar.drop_duplicates(subset=[iidcol], inplace=True)
        if (verbose):
            print(general_results.columns)
            print(general_results.shape[0])
            print(covar.head())
            print(covar.columns)
        if (general_results.shape[0] > 1):
            print("Merging previous DF with covar DF.")
            covar = pd.merge(general_results, covar, on=iidcol)
        if (verbose):
            print(covar.columns)
            print("covar_final contains duplicated iids: ",covar[iidcol].duplicated().any())
        covar.drop_duplicates(subset=[iidcol], inplace=True)
        if verbose:
            print("Identified",covar.shape[0],Covar_Name,"Covariate Cases.")
            print(covar.columns)
        return covar


def build_phenotype_controls(df1, exact_match, values_to_match, diagnostic_col, birthdatecol, iidcol, input_date_in_name, input_date_out_name, verbose, BuildEntryExitDates=False):
    global DateFormat
    # Use boolean indexing to extract rows from the first file that match the values 
    if (exact_match):
        tmp_result_df = df1[~df1[diagnostic_col].isin(values_to_match)].copy()
    else:
        tmp_result_df = df1[~df1[diagnostic_col].str.contains('|'.join(values_to_match), na=False)].copy()
    if (verbose):
        print(tmp_result_df.columns)
        print(input_date_in_name, ", ",iidcol, ", ", diagnostic_col )
    print("Identified initially ",str(tmp_result_df[iidcol].nunique())," IIDs that do not have an overlapping diagnostic code.")
    return merge_IIDs(tmp_result_df, diagnostic_col, birthdatecol, input_date_in_name, input_date_out_name, iidcol, verbose, Cases=False, BuildEntryExitDates=BuildEntryExitDates)
    

def merge_IIDs(tmp_result_df, diagnostic_col, birthdatecol, input_date_in_name, input_date_out_name, iidcol, verbose, Cases, Covariates=False, BuildEntryExitDates=False):
    if verbose:
        print("In merge_IIDs: Cases:",Cases,", Covariates:",Covariates,", ",input_date_in_name,", ", input_date_out_name,", ", tmp_result_df.columns,", BuildEntryExitDates:",BuildEntryExitDates)    
    if Cases == False:
        cols_to_use = [iidcol, diagnostic_col, input_date_in_name, input_date_out_name]
        if (birthdatecol in tmp_result_df.columns):
            cols_to_use += [birthdatecol]
        if ("type" in tmp_result_df.columns):
            cols_to_use += ["type"]
        cols_to_use = list(set([col for col in cols_to_use if col in tmp_result_df.columns]))
        tmp_result_df = tmp_result_df.loc[:, cols_to_use].copy()
    if Covariates:
        cols_to_use = [iidcol, diagnostic_col, input_date_in_name, input_date_out_name]
        if (birthdatecol in tmp_result_df.columns):
            cols_to_use += [birthdatecol]
        if ("type" in tmp_result_df.columns):
            cols_to_use += ["type"]
        cols_to_use = list(set([col for col in cols_to_use if col in tmp_result_df.columns]))
        tmp_result_df = tmp_result_df.loc[:, cols_to_use].copy()
    if ('date_in_y' in tmp_result_df.columns):
        print("WARNING: date_in_y detected in tmp_result_df. This will be dropped (merge_IIDs)")
        tmp_result_df.drop('date_in_y', inplace=True, axis=1)
    if('date_in_x' in tmp_result_df.columns and not 'date_in' in tmp_result_df.columns):
        print("WARNING: date_in_x detected in tmp_result_df but not date_in. This will be renamed (merge_IIDs)")
        tmp_result_df['date_in'] = tmp_result_df['date_in_x'].copy()
    if('date_in_x' in tmp_result_df.columns):
        print("WARNING: date_in_x detected in tmp_result_df. This will be dropped (merge_IIDs)")
        tmp_result_df.drop('date_in_x', inplace=True, axis=1)
    if (input_date_in_name != 'date_in' and not "date_in" in tmp_result_df.columns):
        if (verbose):
            print("Updating (date_in)",input_date_in_name," to date_in and dropping ",input_date_in_name)
        tmp_result_df['date_in'] = tmp_result_df[input_date_in_name].copy()
        tmp_result_df.drop(input_date_in_name, axis=1, inplace=True)
    if (input_date_out_name != input_date_in_name and input_date_out_name != 'date_out'):
        if (verbose):
            print("Updating (date_out)",input_date_out_name," to date_out and dropping ",input_date_out_name)
        tmp_result_df['date_out'] = tmp_result_df[input_date_out_name].copy()
        tmp_result_df.drop(input_date_out_name, axis=1, inplace=True)
    else:
        if (verbose):
            print("No (date_out) given. Duplicating date_in")
            print(tmp_result_df.columns)
        tmp_result_df['date_out'] = tmp_result_df['date_in'].copy()
        if (verbose):
            print(tmp_result_df.columns)
    if (verbose):
        print("Mem after updating tmp_result_df and tmp_controls_df:")
        usage()
        print(tmp_result_df.columns)
    #Build the First entry and last entry for all individuals (i.e., Cases+Controls)
    if (BuildEntryExitDates):
        print("Estimating the Entry and Exit date for Individuals based on the first and last available diagnosis (not specific to the currently requested diagnosis codes, but generally available data).")
        iid_list_cases = tmp_result_df[iidcol]
        tmp_entry_exit_df = tmp_result_df[tmp_result_df[iidcol].isin(iid_list_cases)].copy()
        cols_to_delete = tmp_entry_exit_df.columns.tolist()  # Convert Index object to list
        if (verbose):
            print("Columns in tmp_entry_exit_df: ",cols_to_delete)
        if ( not input_date_in_name in cols_to_delete ):
            if ("date_in" in cols_to_delete):
                input_date_in_name = "date_in"
        if ( not input_date_out_name in cols_to_delete ):
            if ("date_out" in cols_to_delete):
                input_date_out_name = "date_out"

        cols_to_delete.remove(iidcol)  # Remove 'iidcol' from the list of columns
        if (input_date_in_name in cols_to_delete):
            cols_to_delete.remove(input_date_in_name)
        if (input_date_out_name in cols_to_delete):
            cols_to_delete.remove(input_date_out_name)
        print(tmp_entry_exit_df)
        tmp_entry_exit_df.drop(cols_to_delete, inplace=True, axis=1) 
        print(tmp_entry_exit_df)
        grouped = tmp_entry_exit_df.groupby([iidcol],group_keys=False) 
        print(grouped)
        in_dates = grouped[input_date_in_name].min().reset_index(name='Entry_Date')
        print(in_dates)
        #in_dates.rename(columns={input_date_in_name: 'Entry_Date'})
        if (input_date_out_name != input_date_in_name):
            out_dates = grouped[input_date_out_name].max().reset_index(name='Exit_Date')
            print(out_dates)
            #out_dates.rename(columns={input_date_out_name: 'Exit_Date'})
        else:
            out_dates = grouped[input_date_in_name].max().reset_index(name='Exit_Date')
            #out_dates.rename(columns={input_date_in_name: 'Exit_Date'})
        Entry_Exit_date_df = in_dates.merge(out_dates, on=iidcol)
        print(Entry_Exit_date_df)
    # Add first/last diagnosis date, with date_out as the last diagnosis date 
    grouped = tmp_result_df.groupby([iidcol],group_keys=False) # Group by ID, then extract the first and last dates for each group
    if (verbose):
        print("Finished grouping tmp_result_df. Identifying now top variables for each IID. ",str(tmp_result_df[iidcol].nunique())," rows")
        print(grouped.head())
        print(tmp_result_df.columns)
        print(tmp_result_df.head())
        print(type(grouped['date_in']))
    first_dates = grouped['date_in'].min()
    if (input_date_out_name != input_date_in_name and input_date_out_name == "date_out"):
        last_dates = grouped['date_out'].max()
    elif ('date_out' in tmp_result_df.columns):
        last_dates = grouped['date_out'].max()
    else:
        print("WARNING: no column",input_date_out_name,"given. Duplicating date_in column.")
        if ("date_in" in tmp_result_df.columns):
            last_dates = grouped['date_in'].max()
            last_dates = last_dates.rename('date_out')
        else:
            print("WARNING: No date_in column given!")
            last_dates = first_dates
    if Cases or Covariates:
        dx_string = grouped[diagnostic_col].apply(list).reset_index(name='diagnoses') #dx_string = grouped[diagnostic_col].apply(lambda x: ','.join(x)).reset_index(name='diagnoses')
    in_dates = grouped['date_in'].apply(list).reset_index(name='in_dates')
    if (input_date_out_name != input_date_in_name):
        out_dates = grouped['date_out'].apply(list).reset_index(name='out_dates') 
    else:
        out_dates = in_dates.rename(columns={'in_dates': 'out_dates'})
    if Cases:
        n_diags = grouped['date_in'].count().reset_index(name="n_diags")
        n_unique_in_days = grouped['date_in'].nunique().reset_index(name="n_unique_in_days")
    if (verbose):
        print("Mem after grouping and building some additional variables:")
        usage()
    # Delete the date in and date out columns 
    cols_to_delete = tmp_result_df.columns.tolist()  # Convert Index object to list
    cols_to_delete.remove(iidcol)  # Remove 'iidcol' from the list of columns
    if (Cases and birthdatecol in cols_to_delete):
        cols_to_delete.remove(birthdatecol)  # Remove 'birthdatecol' from the list of columns
    tmp_result_df.drop(cols_to_delete, inplace=True, axis=1)  # Drop the columns except 'iidcol'
    tmp_result_df.drop_duplicates(inplace=True)
    if (verbose):
        print("tmp_result_df.columns: ",tmp_result_df.columns)
        print("Mem after dropping not needed entries in tmp_result_df:")
        usage()
    # Merge the unique IDs and DX DataFrame with the calculated first and last dates; Reset index for first_dates and last_dates
    first_dates2 = first_dates.reset_index()
    del first_dates
    last_dates2 = last_dates.reset_index()
    del last_dates
    id_dx_date_df = first_dates2.merge(last_dates2, on=iidcol)
    if (verbose):
        print(id_dx_date_df.head())
        if Cases or Covariates:
            print(dx_string.head())
        print(in_dates.head())
        print(out_dates.head())
    if Cases or Covariates:
        id_dx_date_df = id_dx_date_df.merge(dx_string, on=iidcol)
    if (verbose):
        print(id_dx_date_df.head())
    id_dx_date_df = id_dx_date_df.merge(in_dates, on=iidcol) 
    id_dx_date_df = id_dx_date_df.merge(out_dates, on=iidcol) 
    # Rename 'first_dx': first_dates, 'last_dx': last_dates, 'diagnoses': dx_string})
    id_dx_date_df.rename(columns={'date_in': 'first_dx'},inplace=True)
    id_dx_date_df.rename(columns={'date_out': 'last_dx'},inplace=True)
    if (verbose):
        print(id_dx_date_df.head())
    # Group by ID, then extract the first and last dates for each group
    grouped = tmp_result_df.groupby([iidcol],group_keys=False)
    if ("source" in tmp_result_df.columns):
        #sources = grouped['source'].apply(lambda x: ','.join(x.astype(str))).reset_index(name='source')
        sources = grouped['source'].apply(list).reset_index(name='source')
        id_dx_date_df = id_dx_date_df.merge(sources, on=iidcol) 
        # Delete the source column 
        tmp_result_df.drop('source', inplace=True, axis=1) 
    if ("type" in tmp_result_df.columns):
        #types = grouped['type'].apply(lambda x: ','.join(x.astype(str))).reset_index(name='c_diagtype') 
        types = grouped['type'].apply(list).reset_index(name='c_diagtype') 
        id_dx_date_df = id_dx_date_df.merge(types, on=iidcol) 
        # Delete the type column 
        tmp_result_df.drop('type', inplace=True, axis=1)
    del grouped
    if (verbose):
        print("Mem after deleting grouped tmp_result_df:")
        usage()
        print(id_dx_date_df.head())
        if Cases:
            print(n_diags.head())
            print(n_unique_in_days.head())
    if Cases:
        id_dx_date_df = id_dx_date_df.merge(n_diags, on=iidcol)
        id_dx_date_df = id_dx_date_df.merge(n_unique_in_days, on=iidcol) 
    if (verbose):
        print("Mem after merging id_dx_date_df with n_diags and n_unique_in_days:")
        usage()
        print(id_dx_date_df.head())
        print(tmp_result_df.head())
        print("id_dx_date_df contains duplicated iids: ",id_dx_date_df[iidcol].duplicated().any())
    id_dx_date_df.drop_duplicates(subset=[iidcol], inplace=True)
    if (verbose):
        print("Mem after building id_dx_date_df:")
        usage()
    # Set DX column of tmp_result_df to Case or Control
    if Cases:
        tmp_result_df["diagnosis"] = "Case"
    else:
        tmp_result_df["diagnosis"] = "Control"
    if (verbose):
        print(id_dx_date_df.head())
        print(tmp_result_df.head())
    # Merge the result with the original merged DataFrame
    dx_result_df = pd.merge(tmp_result_df, id_dx_date_df, on=iidcol)
    if (verbose):
        print("dx_result_df contains duplicated iids: ",dx_result_df[iidcol].duplicated().any())
    dx_result_df.drop_duplicates(subset=[iidcol], inplace=True)
    if (verbose):
        print("len(dx_result_df[[iidcol]].drop_duplicates()): ",len(dx_result_df[iidcol].drop_duplicates()),"len(dx_result_df): ",len(dx_result_df),", len(id_dx_date_df): ",len(id_dx_date_df),", len(tmp_result_df): ",len(tmp_result_df))
    del tmp_result_df
    del id_dx_date_df
    if (BuildEntryExitDates):
        print(dx_result_df)
        dx_result_df = dx_result_df.merge(Entry_Exit_date_df, on=iidcol)
        del(tmp_entry_exit_df)
    if (verbose):
        print("Mem after deleting id_dx_date_df and tmp_result_df and building dx_result_df:")
        usage()
    if (verbose):
        print(dx_result_df.head(5))
    if (verbose):
        print("IIDs:")
        print(dx_result_df.head())
    return dx_result_df

def process_ophold(ophold, stam, tmp_result_df, ophold_out_file, birthdatecol, iidcol, verbose):
    #As exclusion criterion
    min_code = 5000
    max_code = 7000
    print("process_ophold: This will generate a new ophold combined file, outlining the date moved to Denmark.")
    if verbose:
        print("In process_ophold: This will be using the codes [5000,7000] (fkode) to determine if born in DK or not.\nInformation about the codes can be found here: https://www.dst.dk/da/Statistik/dokumentation/Times/cpr-oplysninger/foedreg-kode")
    stam = stam[(stam['fkode'] >= min_code) & (stam['fkode'] <= max_code)]
    stam['both_parents_DK'] = False
    stam.loc[(stam['fkode_m'] >= min_code) & (stam['fkode_m'] <= max_code) & (stam['fkode_f'] >= min_code) & (stam['fkode_f'] <= max_code), 'both_parents_DK'] = True
    stam = stam[[iidcol,'fkode',birthdatecol,"both_parents_DK"]]
    ophold = pd.merge(stam,ophold,how="left", on=iidcol)
    grouped = ophold.groupby([iidcol],group_keys=False) 
    del(ophold)
    # < 20 refers to alive and residing in DK https://www.dst.dk/da/Statistik/dokumentation/Times/cpr-oplysninger
    stat_lists = grouped['stat'].apply(list).reset_index(name='stat')
    statd_lists = grouped['statd'].apply(list).reset_index(name='statd')
    ophold_df = stat_lists.merge(statd_lists, on=iidcol)
    del(statd_lists)
    del(stat_lists)
    tflytd_lists = grouped['tflytd'].apply(list).reset_index(name='tflytd')
    ophold_df = ophold_df.merge(tflytd_lists, on=iidcol)
    del(tflytd_lists)
    fflytd_lists = grouped['fflytd'].apply(list).reset_index(name='fflytd')
    ophold_df = ophold_df.merge(fflytd_lists, on=iidcol)
    del(fflytd_lists)
    komkod_lists = grouped['komkod'].apply(list).reset_index(name='komkod')
    ophold_df = ophold_df.merge(komkod_lists, on=iidcol)
    del(komkod_lists)
    orig_lists = grouped['orig'].apply(list).reset_index(name='orig')
    ophold_df = ophold_df.merge(orig_lists, on=iidcol)
    del(orig_lists)
    opholdnr_lists = grouped['opholdnr'].apply(list).reset_index(name='opholdnr')
    ophold_df = ophold_df.merge(opholdnr_lists, on=iidcol)
    del(opholdnr_lists)
    moved_to_dk = grouped['fflytd'].min().reset_index(name='moved_to_dk')
    ophold_df = ophold_df.merge(moved_to_dk, on=iidcol)
    del(moved_to_dk)
    if (ophold_out_file != ""):
        ophold_df.to_csv(ophold_out_file, sep="\t", index=False, quoting=False)
        reformat_to_tsv(ophold_out_file)
    print("Info: New Ophold file stored here: ", ophold_out_file)
    if(tmp_result_df != ""):
        dx_result_df = pd.merge(tmp_result_df, ophold_df, on=iidcol)
    else: 
        dx_result_df = ophold_df.copy()
    if (verbose):
        print("ophold_df contains duplicated iids: ",dx_result_df[iidcol].duplicated().any())
    dx_result_df.drop_duplicates(subset=[iidcol], inplace=True)
    del tmp_result_df
    del ophold_df
    if (verbose):
        print("Mem after building opholds and deleting unneeded variables:")
        usage()
    return dx_result_df


def Sankey_build_processor(row):
    sankey_labels = ["MDD","Age","ID","SCZ","BPD","DEM","AUD","DUD","MCI","CTI","ClinPlausMDD"]
    # Split the comma-separated dates in diag_date and convert them to datetime objects
    temp_exc_diag_date = row['temp_exc_diag_date']
    # Convert exc_diag string to a list
    temp_exc_diag = row['temp_exc_diag']
    previous_exc_diag = ""
    if row['Level2_AgeExclusion'] == "TRUE":
        row['Level3_Sankey'] = ["MDD","Age"]
        row['Level3_Sankey_data'] = [0,1]
        row['Level3_Sankey_source'] = [0]
        row['Level3_Sankey_target'] = [1]
        row['Level3_Sankey_value'] = [1]
        # Return the updated row
        row['temp_exc_diag_date'] = []
        row['temp_exc_diag'] = []
    elif len(row['diagnoses_Level2_modifier']) == 0:
        row['Level3_Sankey'] = ["MDD"]
        row['Level3_Sankey_data'] = [0]
        row['Level3_Sankey_source'] = [0]
        row['Level3_Sankey_target'] = [len(sankey_labels)-1]
        row['Level3_Sankey_value'] = [1]
        # Return the updated row
        row['temp_exc_diag_date'] = []
        row['temp_exc_diag'] = []
    else:
        while temp_exc_diag:
            if row['Level3_Sankey'] == ["MDD"]:
                previous_exc_diag = 0
            # Get the earliest point of any of the exclusion diagnostic criteria
            currently_earliest_exc_diag_date = min(temp_exc_diag_date) if temp_exc_diag_date else pd.NaT
            current_exc_diag_date_position = [i for i, date in enumerate(temp_exc_diag_date) if date == currently_earliest_exc_diag_date]
            # Get those positions to keep
            current_exc_diag_date_position_to_keep = [i for i in range(len(temp_exc_diag_date)) if i not in current_exc_diag_date_position]
            # Get the current exclusion diagnosis and the date
            currently_earliest_exc_diag = [temp_exc_diag[i] for i in current_exc_diag_date_position if temp_exc_diag[i] != ""]
            is_lifetime_exclusion = False
            if temp_exc_diag == "ID" or temp_exc_diag == "SCZ" or temp_exc_diag == "BPD":
                is_lifetime_exclusion = True
            if (len(currently_earliest_exc_diag) > 0):
                # Identify which disorder this code refers to and add it to a list
                currently_earliest_exc_disorder = [row['disorder_Level2_modifier'][i] for i in current_exc_diag_date_position if temp_exc_diag[i] != ""]
                currently_earliest_exc_disorder_numeric = next((i for i, date in enumerate(sankey_labels) if date == currently_earliest_exc_disorder[0]), None)
                #lifetime 2, 3, 4; post 5, 9
                if currently_earliest_exc_disorder == 5 or currently_earliest_exc_disorder == 9:
                     is_lifetime_exclusion = True
                if currently_earliest_exc_disorder:
                    row['Level3_Sankey'] = row['Level3_Sankey'] + currently_earliest_exc_disorder
                    row['Level3_Sankey_data'].append(previous_exc_diag) 
                    row['Level3_Sankey_data'].append(currently_earliest_exc_disorder_numeric) 
                    row['Level3_Sankey_source'].append(previous_exc_diag) 
                    row['Level3_Sankey_target'].append(currently_earliest_exc_disorder_numeric) 
                    row['Level3_Sankey_value'].append(1) 
                    previous_exc_diag = currently_earliest_exc_disorder_numeric
            # Update temp_exc_diag and temp_exc_diag_date
            temp_exc_diag = [temp_exc_diag[i] for i in current_exc_diag_date_position_to_keep]
            temp_exc_diag_date = [temp_exc_diag_date[i] for i in current_exc_diag_date_position_to_keep]
            if is_lifetime_exclusion:
                temp_exc_diag = []
                temp_exc_diag_date = []
        # Return the updated row
        row['temp_exc_diag_date'] = temp_exc_diag_date
        row['temp_exc_diag'] = temp_exc_diag
    return row

# Function to remove duplicates while preserving order
def remove_duplicates_preserve_order(lst):
    seen = set()
    result = []
    for item in lst:
        if item not in seen:
            result.append(item)
            seen.add(item)
    return result

def split_comma_string(data):
    return data.apply(lambda x: x.split(','))


def Build_sankey_data(data, fn, iidcol, verbose=False):
    data['Level3_Sankey'] = [[] for _ in range(len(data))]
    data['Level3_Sankey_data'] = [[] for _ in range(len(data))]
    data['Level3_Sankey_source'] = [[] for _ in range(len(data))]
    data['Level3_Sankey_target'] = [[] for _ in range(len(data))]
    data['Level3_Sankey_value'] = [[] for _ in range(len(data))]
    if (verbose):
        print("data: ",data.columns)
    data.loc[data['diagnosis'] == "Case",'Level3_Sankey'] = data.loc[data['diagnosis'] == "Case",'Level3_Sankey'].apply(lambda x: x + ["MDD"])
    # Split the comma-separated dates in diag_date and convert them to datetime objects
    data['temp_exc_diag_date'] = data['date_Level2_modifier'].copy() #apply(lambda x: [pd.to_datetime(date) for date in x.split(',') if date.strip() != ''])
    # Convert exc_diag string to a list
    data['temp_exc_diag'] = data['diagnoses_Level2_modifier'].copy() #apply(lambda x: x.split(','))
    while data.apply(lambda row: len(row['temp_exc_diag']) > 0, axis=1).any():
        ### Build the lists of covar diagnostic codes in general and use this then in the phenotype generating step and here.
        data = data.apply(Sankey_build_processor, axis=1)
        ##### Assuming these are called AUD, DUD ...
    data.loc[data['Level3_CaseControl'] == "Case", 'Level3_Sankey'] = data.loc[data['Level3_CaseControl'] == "Case"].apply(lambda row: row['Level3_Sankey'] + ["ClinPlausMDD"], axis=1)
    data['Level3_Sankey'] = data['Level3_Sankey'].apply(remove_duplicates_preserve_order)
    if (verbose):
        print("Sankey Target: ",data.loc[(data['diagnosis'] == "Case") & (data['Level3_Sankey_target'].apply(len) == 0),[iidcol,'temp_exc_diag']])
        print("data.loc[(data[\'diagnosis\'] == \"Case\") & (data[\'diagnoses_Level2_modifier\'].apply(len) != 0),]: ",data.loc[(data['diagnosis'] == "Case") & (data['diagnoses_Level2_modifier'].apply(len) != 0),])
        print("Level3_Sankey_source: ",data.loc[(data['diagnosis'] == "Case") & (data['diagnoses_Level2_modifier'].apply(len) != 0), 'Level3_Sankey_source'])
    data.loc[(data['diagnosis'] == "Case") & (data['diagnoses_Level2_modifier'].apply(len) != 0), 'Level3_Sankey_source'] = data.loc[(data['diagnosis'] == "Case")  & (data['diagnoses_Level2_modifier'].apply(len) != 0)].apply(lambda row: row['Level3_Sankey_source'] + [row['Level3_Sankey_target'][-1] if row['Level3_Sankey_target'] else -1], axis=1)
    data.loc[(data['diagnosis'] == "Case") & (data['diagnoses_Level2_modifier'].apply(len) != 0), 'Level3_Sankey_target'] = data.loc[(data['diagnosis'] == "Case")  & (data['diagnoses_Level2_modifier'].apply(len) != 0)].apply(lambda row: row['Level3_Sankey_target'] + [10], axis=1)
    data.loc[(data['diagnosis'] == "Case") & (data['diagnoses_Level2_modifier'].apply(len) != 0), 'Level3_Sankey_value'] = data.loc[(data['diagnosis'] == "Case")  & (data['diagnoses_Level2_modifier'].apply(len) != 0)].apply(lambda row: row['Level3_Sankey_value'] + [1], axis=1)
    data['Level3_Sankey_source'] = data['Level3_Sankey_source'].apply(remove_duplicates_preserve_order)
    data['Level3_Sankey_target'] = data['Level3_Sankey_target'].apply(remove_duplicates_preserve_order)
    data['Level3_Sankey_value'] = data['Level3_Sankey_value'].apply(remove_duplicates_preserve_order)
    if (verbose):
        print(data.loc[data['diagnosis'] == "Case",['Level3_Sankey']])
        print(data.loc[data['diagnosis'] == "Case",['Level3_Sankey_data']])
        print(data.loc[data['diagnosis'] == "Case",['Level3_Sankey_source','Level3_Sankey_target','Level3_Sankey_value']])
    # Initialize a dictionary to store the counts
    counts = {}
    for _, row in data.loc[data['diagnosis'] == "Case"].iterrows():
        for a, b in zip(row['Level3_Sankey_source'], row['Level3_Sankey_target']):
            key = (a, b)
            counts[key] = counts.get(key, 0) + 1
    # Convert the dictionary to a DataFrame
    result_df = pd.DataFrame(list(counts.keys()), columns=['Source', 'Target'])
    result_df['Value'] = result_df.apply(lambda row: counts.get((row['Source'], row['Target']), 0) if not result_df.empty else 0, axis=1)
    if (verbose):
        print(result_df)
    try:    
        sankey_fn = os.path.splitext(os.path.basename(fn))[0]+".sankey.tsv"
        result_df.to_csv(sankey_fn, sep="\t", index=False)
    except:
        print("Couldn't create sankey output file:",sankey_fn)
    data['Level3_Sankey_source'] = data['Level3_Sankey_source'].apply(lambda row: [entry for entry in row if entry != ""])
    data['Level3_Sankey_target'] = data['Level3_Sankey_target'].apply(lambda row: [entry for entry in row if entry != ""])
    data['Level3_Sankey_value'] = data['Level3_Sankey_value'].apply(lambda row: [entry for entry in row if entry != ""])
    data['Level3_Sankey_data'] = data['Level3_Sankey_data'].apply(lambda row: [entry for entry in row if entry != ""])
    data = data.drop(['temp_exc_diag_date','temp_exc_diag'], axis=1)
    return(data)

def Exclusion_interpreter(data, excl, type, min_Age, max_Age, exclnumber, verbose=False):
    global DateFormat
    data['diagnoses_Level2_modifier'] = [[] for _ in range(len(data))]
    data['disorder_Level2_modifier'] = [[] for _ in range(len(data))]
    data['date_Level2_modifier'] = [[] for _ in range(len(data))]
    data['Level2_diagnoses'] = data['diagnoses']
    data['Level2_dates'] = data['in_dates'] 
    data['Level2_AgeExclusion'] = "FALSE"
    data['Level2_ExclusionReason'] = [[] for _ in range(len(data))]
    data['Level2_FirstDx'] = [[] for _ in range(len(data))]
    if (verbose):
        print(data.head(2))
    if (verbose):
        print(data.loc[(data['diagnoses'] != "") & (data[excl] != "")])
        print(data['diagnoses'])
        print(data[excl])
    # Calculate (max) 1 year prior exclusions and add dates to date_Level2_modifier and diagnostic codes to diagnoses_Level2_modifier, also, delete the appropriate codes/dates from the Level2_diagnoses and Level2_dates
    if (type in ["1yprior","post","lifetime"]):
        if (data.loc[(data['diagnoses'] != "") & (data[excl] != "")].shape[0] > 0):
            print("Updating Case-Diagnoses based on ",excl," exclusions.")
            data = update_DxDates_ExDEP_multi_exclusion(data, type, excl, excl+"_Dates", excl+"_Inflicted_changes", exclnumber, "Level2_diagnoses", "Level2_dates", "diagnoses_Level2_modifier", "date_Level2_modifier", 'disorder_Level2_modifier', verbose)
        else:
            print("No Case overlap with ",excl,". Skipping update_DxDates_ExDEP_multi_exclusion")
    else:
        print("Warning: selected type ",type," not recognized.")
        sys.exit()
    ##### Exclude Cases based on their Age of first diagnosis
    # Set date for first diagnosis
    data['temp_Level2_dates'] = data['Level2_dates'].apply(lambda x: [pd.to_datetime(date, format=DateFormat) for date in (x if isinstance(x, list) else [x]) if str(date).strip() != '']).copy()
    if (verbose):
        print("temp_Level2_dates: ",data.loc[(data['diagnosis'] == "Case"),'temp_Level2_dates'])
    case_rows = data['temp_Level2_dates'].apply(lambda x: len(x) > 0 if isinstance(x, list) else False)
    if (verbose):
        print("case_rows (standard): ",case_rows)
    
    data['temp_birthdate'] = data['birthdate']
    if (len(data.loc[case_rows])>0):
        if (verbose):
            print(data.loc[case_rows,['diagnosis','temp_Level2_dates','temp_birthdate','birthdate']])
        data.loc[case_rows, "Level2_FirstDx"] = data.loc[case_rows, "Level2_FirstDx"] + data.loc[case_rows, "temp_Level2_dates"].apply(lambda x: [min(x)] if x else [])
        # Recalculate age at first Diagnosis
        if (verbose):
            print(data.loc[case_rows])
            print(data.loc[case_rows,['Level2_FirstDx','temp_birthdate','birthdate']])
        data['Level3_Age_FirstDx'] = 0
        data.loc[case_rows, 'Level3_Age_FirstDx'] = (data.loc[case_rows].apply(lambda row: 
            row['Level2_FirstDx'][0] - row['temp_birthdate'], axis=1)).dt.days // 365
    else:
        print("Warning: No cases left (Exclusion_interpreter)!")
    if (min_Age != 0 or max_Age != 0):
        print("Updating Case-Diagnoses based on Age , min:"+str(min_Age)+", max:"+str(max_Age))
        data.loc[(data['Level3_Age_FirstDx'] < min_Age) | ((data['Level3_Age_FirstDx'] > max_Age) & (data['Level3_Age_FirstDx'] > 0)), 'Level2_AgeExclusion'] = "TRUE"
    if (verbose):
        print(data.head(2))
    print(data.loc[(data['diagnosis'] == "Case"),'Level2_FirstDx'])
    data.loc[:,'Level3_CaseControl'] = "Control"
    data.loc[(data['diagnosis'] == "Case"),'Level3_CaseControl'] = "Case_Excluded"
    data.loc[data['Level2_FirstDx'].apply(len) > 0, 'Level3_CaseControl'] = "Case"
    data.loc[(data['diagnosis'] == "Control"),'Level3_CaseControl'] = "Control"
    case_rows = data['Level3_CaseControl'] == 'Case'
    data.loc[:,'Level3_CaseControl_AgeExclusions'] = "Control"
    data.loc[(data['diagnosis'] == "Case"),'Level3_CaseControl_AgeExclusions'] = "Case_Excluded"
    data.loc[(data['Level2_FirstDx'].apply(len) > 0) and (data['Level2_AgeExclusion'] == "FALSE"), 'Level3_CaseControl_AgeExclusions'] = "Case"
    #data.loc[(data['diagnosis'] == "Control"),'Level3_CaseControl_AgeExclusions'] = "Control"
    #case_rows = data['Level3_CaseControl_AgeExclusions'] == 'Case'
    data.drop('temp_Level2_dates', inplace=True, axis=1)
    data.drop('diagnosis', inplace=True, axis=1) 
    data.drop('diagnoses', inplace=True, axis=1) 
    data.drop('first_dx', inplace=True, axis=1) 
    data.drop('in_dates', inplace=True, axis=1) 
    if ('c_diagtype' in data.columns):
        data.drop('c_diagtype', inplace=True, axis=1) 
    data.drop('Age_FirstDx', inplace=True, axis=1)  
    data.drop('birthdate', inplace=True, axis=1)  
    data.drop('temp_birthdate', inplace=True, axis=1)  
    data.drop(excl, inplace=True, axis=1) 
    data.drop(excl+"_Dates", inplace=True, axis=1) 
    if (verbose):
        print(data.head(2))
    return(data)


def ExDEP_exclusion_interpreter(data, min_Age, max_Age, verbose):
    global DateFormat
    data['diagnoses_Level2_modifier'] = [[] for _ in range(len(data))]
    data['disorder_Level2_modifier'] = [[] for _ in range(len(data))]
    data['date_Level2_modifier'] = [[] for _ in range(len(data))]
    data['Level2_diagnoses'] = data['diagnoses']
    data['Level2_dates'] = data['in_dates'] #data['in_dates']
    data['Level2_AgeExclusion'] = "FALSE"
    data['Level2_ExclusionReason'] = [[] for _ in range(len(data))]
    data['Level2_FirstDx'] = [[] for _ in range(len(data))]
    if (verbose):
        print(data.head(2))
    print("Updating Case-Diagnoses based on Age , min:"+str(min_Age)+", max:"+str(max_Age))
    if( min_Age > 0 and max_Age > 0):
        data.loc[(data['Age_FirstDx'] < min_Age) | ((data['Age_FirstDx'] > max_Age) & (data['Age_FirstDx'] > 0)), 'Level2_AgeExclusion'] = "TRUE"
    data.loc[data['Level2_AgeExclusion'] == "TRUE", 'Level2_ExclusionReason'] = data.loc[data['Level2_AgeExclusion'] == "TRUE", 'Level2_ExclusionReason'].apply(lambda x: x + [1])
    # Calculate (max) 1 year prior exclusions and add dates to date_Level2_modifier and diagnostic codes to diagnoses_Level2_modifier, also, delete the appropriate codes/dates from the Level2_diagnoses and Level2_dates
    if (data.loc[(data['diagnoses'] != "") & (data['AUD'] != "")].shape[0] > 0):
        print("Updating Case-Diagnoses based on AUD ExDEP exclusions.")
        data = update_DxDates_ExDEP_multi_exclusion(data, "1yprior", "AUD", "AUD_Dates", "AUD_Inflicted_changes", 6, "Level2_diagnoses", "Level2_dates", "diagnoses_Level2_modifier", "date_Level2_modifier", 'disorder_Level2_modifier', verbose)
    else:
        print("No Case overlap with AUD. Skipping update_DxDates_ExDEP_1y_prior_exclusion")
    # DUD
    if (data.loc[(data['diagnoses'] != "") & (data['DUD'] != "")].shape[0] > 0):
        print("Updating Case-Diagnoses based on DUD ExDEP exclusions.")
        data = update_DxDates_ExDEP_multi_exclusion(data, "1yprior", "DUD", "DUD_Dates", "DUD_Inflicted_changes", 7, "Level2_diagnoses", "Level2_dates", "diagnoses_Level2_modifier", "date_Level2_modifier", 'disorder_Level2_modifier', verbose)
    else:
        print("No Case overlap with DUD. Skipping update_DxDates_ExDEP_1y_prior_exclusion")
    # MCI
    if (data.loc[(data['diagnoses'] != "") & (data['MCI'] != "")].shape[0] > 0):
        print("Updating Case-Diagnoses based on MCI ExDEP exclusions.")
        data = update_DxDates_ExDEP_multi_exclusion(data, "1yprior", "MCI", "MCI_Dates", "MCI_Inflicted_changes", 8, "Level2_diagnoses", "Level2_dates", "diagnoses_Level2_modifier", "date_Level2_modifier", 'disorder_Level2_modifier', verbose)
    else:
        print("No Case overlap with MCI. Skipping update_DxDates_ExDEP_1y_prior_exclusion")
    # Calculate exclusions for diagnoses that occured after ... - DEM
    if (data.loc[(data['diagnoses'] != "") & (data['DEM'] != "")].shape[0] > 0):
        print("Updating Case-Diagnoses based on DEM ExDEP exclusions.")
        data = update_DxDates_ExDEP_multi_exclusion(data, "post", "DEM", "DEM_Dates", "DEM_Inflicted_changes", 5, "Level2_diagnoses", "Level2_dates", "diagnoses_Level2_modifier", "date_Level2_modifier", 'disorder_Level2_modifier', verbose)
    else:
        print("No Case overlap with DEM. Skipping update_DxDates_ExDEP_post_exclusion")
    # CTI
    if (data.loc[(data['diagnoses'] != "") & (data['CTI'] != "")].shape[0] > 0):
        print("Updating Case-Diagnoses based on CTI ExDEP exclusions.")
        data = update_DxDates_ExDEP_multi_exclusion(data, "post", "CTI", "CTI_Dates", "CTI_Inflicted_changes", 9, "Level2_diagnoses", "Level2_dates", "diagnoses_Level2_modifier", "date_Level2_modifier", 'disorder_Level2_modifier', verbose)
    else:
        print("No Case overlap with CTI. Skipping update_DxDates_ExDEP_post_exclusion")
    # Update Lifetime exclusions and add dates to date_Level2_modifier and diagnostic codes to diagnoses_Level2_modifier, also, delete the appropriate codes/dates from the Level2_diagnoses and Level2_dates
    diag="ID"
    diag_date="ID_Dates"
    diag_inflicted="ID_Inflicted_changes"
    print("Updating Case-Diagnoses based on " + diag + " ExDEP exclusions.")
    data = update_DxDates_ExDEP_multi_exclusion(data, "lifetime", diag, diag_date, diag_inflicted, 2, "Level2_diagnoses", "Level2_dates", "diagnoses_Level2_modifier", "date_Level2_modifier", 'disorder_Level2_modifier', verbose)
    # SCZ
    diag="SCZ"
    diag_date="SCZ_Dates"
    diag_inflicted="SCZ_Inflicted_changes"
    print("Updating Case-Diagnoses based on " + diag + " ExDEP exclusions.")
    data = update_DxDates_ExDEP_multi_exclusion(data, "lifetime", diag, diag_date, diag_inflicted, 3, "Level2_diagnoses", "Level2_dates", "date_Level2_modifier", "diagnoses_Level2_modifier", 'disorder_Level2_modifier', verbose)
    # BPD
    diag="BPD"
    diag_date="BPD_Dates"
    diag_inflicted="BPD_Inflicted_changes"
    print("Updating Case-Diagnoses based on " + diag + " ExDEP exclusions.")
    data = update_DxDates_ExDEP_multi_exclusion(data, "lifetime", diag, diag_date, diag_inflicted, 4, "Level2_diagnoses", "Level2_dates", "diagnoses_Level2_modifier", "date_Level2_modifier", 'disorder_Level2_modifier', verbose)
    # Set date for first diagnosis
    data['temp_Level2_dates'] = data['Level2_dates'].apply(lambda x: [pd.to_datetime(date, format=DateFormat) for date in (x if isinstance(x, list) else [x]) if str(date).strip() != '']).copy()
    case_rows = data['Level2_dates'].apply(lambda x: len(x) > 0)
    data['temp_birthdate'] = data['birthdate']
    if (len(data.loc[case_rows])>0):
        data.loc[case_rows, "Level2_FirstDx"] = data.loc[case_rows, "Level2_FirstDx"] + data.loc[case_rows, "temp_Level2_dates"].apply(lambda x: [min(x)] if x else [])
        # Recalculate age at first Diagnosis
        if (verbose):
            print(data.loc[case_rows])
            print(data.loc[case_rows,['Level2_FirstDx','temp_birthdate','birthdate']])
        data['Level3_Age_FirstDx'] = 0
        data['temp_birthdate'] = pd.to_datetime(data['temp_birthdate'], errors='coerce')
        data.loc[case_rows, 'Level3_Age_FirstDx'] = (data.loc[case_rows].apply(lambda row: 
            row['Level2_FirstDx'][0] - row['temp_birthdate'], axis=1)).dt.days // 365
    else:
        print("Warning: No cases left (ExDEP_exclusion_interpreter)!")
    if (verbose):
        print(data.head(2))
        print(data.loc[(data['diagnosis'] == "Case"),'Level2_FirstDx'])
    data.loc[:,'Level3_CaseControl'] = "Control"
    data.loc[(data['diagnosis'] == "Case"),'Level3_CaseControl'] = "Case_Excluded"
    data.loc[data['Level2_FirstDx'].apply(len) > 0, 'Level3_CaseControl'] = "Case"
    data.loc[(data['diagnosis'] == "Control"),'Level3_CaseControl'] = "Control"
    case_rows = data['Level3_CaseControl'] == 'Case'
    data.drop('temp_Level2_dates', inplace=True, axis=1)
    data.drop('diagnosis', inplace=True, axis=1) 
    data.drop('diagnoses', inplace=True, axis=1) 
    data.drop('first_dx', inplace=True, axis=1) 
    data.drop('in_dates', inplace=True, axis=1) 
    if ('c_diagtype' in data.columns):
        data.drop('c_diagtype', inplace=True, axis=1) 
    data.drop('Age_FirstDx', inplace=True, axis=1)  
    data.drop('birthdate', inplace=True, axis=1)  
    data.drop('temp_birthdate', inplace=True, axis=1)  
    data.drop("ID", inplace=True, axis=1) 
    data.drop("ID_Dates", inplace=True, axis=1) 
    data.drop("SCZ", inplace=True, axis=1) 
    data.drop("SCZ_Dates", inplace=True, axis=1) 
    data.drop("BPD", inplace=True, axis=1) 
    data.drop("BPD_Dates", inplace=True, axis=1) 
    data.drop("DEM", inplace=True, axis=1) 
    data.drop("DEM_Dates", inplace=True, axis=1) 
    data.drop("AUD", inplace=True, axis=1) 
    data.drop("AUD_Dates", inplace=True, axis=1) 
    data.drop("DUD", inplace=True, axis=1) 
    data.drop("DUD_Dates", inplace=True, axis=1) 
    data.drop("MCI", inplace=True, axis=1) 
    data.drop("MCI_Dates", inplace=True, axis=1) 
    data.drop("CTI", inplace=True, axis=1) 
    data.drop("CTI_Dates", inplace=True, axis=1) 
    data.drop("CP", inplace=True, axis=1) 
    data.drop("CP_Dates", inplace=True, axis=1) 
    if (verbose):
        print(data.head(2))
    return(data)


def build_ExDEP_exclusions(casecontrol_df, df1, diagnostic_col, iidcol, birthdatecol, input_date_in_name, input_date_out_name, diag_df, diag, diag_dates, exact_match, verbose, get_earliest_date_from_data, dbds_run=False, update_diag_col_name_to_diag=False):
    if (dbds_run):
        filtered_df = build_phenotype_cases(df1, exact_match, diag_df, diagnostic_col, birthdatecol, iidcol, input_date_in_name, input_date_out_name, verbose, Covariates=True, Covar_Name=diag)
        if "type" in filtered_df.columns:
            filtered_df = filtered_df[[iidcol, "diagnoses", "in_dates", "first_dx", "type"]].copy()
        else:
            filtered_df = filtered_df[[iidcol, "diagnoses", "in_dates", "first_dx"]].copy()
    else:
        if (verbose):
            print(df1.head())
            print(isinstance(diag_df, pd.DataFrame))
            if len(diag_df) >= 5:
                print(diag_df)
                print(diag_df[:5])
        filtered_df = build_phenotype_cases(df1, exact_match, diag_df, diagnostic_col, birthdatecol, iidcol, input_date_in_name, input_date_out_name, verbose, Covariates=True, Covar_Name=diag)[[iidcol, "diagnoses", "in_dates", "first_dx"]]
        # filtered_df = build_phenotype_cases(df1, exact_match, diag_df, diagnostic_col)[[iidcol, diagnostic_col, "date_in"]]
        update_diag_col_name_to_diag = True
    if (verbose):
        print(filtered_df.head(5))
    print("Identified ",filtered_df.shape[0]," Cases for",diag)
    if not filtered_df.empty:
        filtered_df.rename(columns={"diagnoses":diag},inplace=True)
        filtered_df.rename(columns={"in_dates":diag_dates},inplace=True)
        filtered_df.rename(columns={"first_dx":diag+'_earliest_date'},inplace=True)
        casecontrol_df = casecontrol_df.merge(filtered_df, on=iidcol, how='outer')
        #casecontrol_df = casecontrol_df.merge(filtered_df, on=iidcol, how='left')
        casecontrol_df[diag] = casecontrol_df[diag].fillna("")
        casecontrol_df[diag_dates] = casecontrol_df[diag_dates].fillna("")
    else:
        casecontrol_df[diag] = ""
        casecontrol_df[diag_dates] = ""
    del filtered_df
    return casecontrol_df


#input example: data, exclusion_type, "DUD", "DUD_Dates", "DUD_Inflicted_changes", 7, "Level2_diagnoses", "Level2_dates", "diagnoses_Level2_modifier", "date_Level2_modifier", 'disorder_Level2_modifier')
def update_DxDates_ExDEP_multi_exclusion(all_data, exclusion_type, exc_diag, exc_diag_date, exc_diag_inflicted_changes, diag_excode, level2codes, level2dates, level2datemodifiercodes, level2datemodifierdates, level2datemodifierDXs,verbose):
    global DateFormat
    diag_inflicted_codes = exc_diag_inflicted_changes+"_MDD_codes"
    diag_inflicted_dates = exc_diag_inflicted_changes+"_MDD_dates"
    diag_inflicted_N = exc_diag_inflicted_changes+"_MDD_dXnumber"
    diag_lost_due_to_exc = "MDD_diagnoses_in_percent_lost_due_to_"+exc_diag
    all_data[diag_lost_due_to_exc] = [0 for _ in range(len(all_data))]
    #exclusion_type: lifetime, 1yprior, post
    # Create a filtered DataFrame containing only non-empty 'exc_diag' rows. Basically, work only on those rows, that have a diagnostic code given, 
    # for which we should check if there are some level2codes to be excluded
    data = all_data[(all_data[exc_diag] != '') & (all_data['diagnoses'] != "")].copy()
    if len(data) > 0:
        # Split the comma-separated dates in diag_date and convert them to datetime objects
        data['temp_exc_diag_date'] = data[exc_diag_date].apply(lambda x: [pd.to_datetime(date, format=DateFormat) for date in (x if isinstance(x, list) else [x]) if str(date).strip() != '']).copy()
        # Convert exc_diag string to a list
        data['temp_exc_diag'] = data[exc_diag].copy() #.apply(lambda x: str(x).split(',')).copy()
        # Convert level2dates to a list of datetime objects
        data['temp_level2dates'] = data[level2dates].apply(lambda x: [pd.to_datetime(date, format=DateFormat) for date in (x if isinstance(x, list) else [x]) if str(date).strip() != '']).copy()
        if (verbose):
            print(data[level2dates])
            print(data['temp_level2dates'])
            print(data['diagnoses'])
        # Convert diagnoses string to a list
        data['temp_in_diagnoses'] = data[level2codes].copy() #.apply(lambda x: str(x).split(',')).copy()
        # Convert in_dates string into a list
        data['temp_original_in_dates'] = data['in_dates'].apply(lambda x: [pd.to_datetime(date, format=DateFormat) for date in (x if isinstance(x, list) else [x]) if str(date).strip() != '']).copy()
        # Convert diagnoses string to a list
        data['temp_original_in_diagnoses'] = data['diagnoses'].copy() #.apply(lambda x: str(x).split(',')).copy()
        print("-----------------------------------------------")
        # Get the level2dates/level2codes position that should be kept
        if exclusion_type == "1yprior":
            print("Building 1yprior exclusions for "+exc_diag)
            print("Build Modified_Position_in_dates for "+level2dates+" and "+level2codes)
            # Get the level2dates/level2codes position that should be kept
            data['Modified_Position_in_dates'] = data.apply(lambda row: [
                i for i, date in enumerate(row['temp_level2dates']) if any(((date - d).days <= 365 and (date - d).days >= 0) for d in row['temp_exc_diag_date'])], axis=1)
            print("Build Position_in_dates_keep for "+level2dates+" and "+level2codes)
            data['Position_in_dates_keep'] = data.apply(lambda row: [
                i for i, date in enumerate(row['temp_level2dates']) if not(any(((date - d).days <= 365 and (date - d).days >= 0) for d in row['temp_exc_diag_date']))], axis=1)
            print("Build Modifier_Position_exc_diagdate for "+level2datemodifierdates+" and "+level2datemodifiercodes)
            data['Modifier_Position_exc_diagdate'] = data.apply(lambda row: [
                i for i, date in enumerate(row['temp_exc_diag_date']) if any(((d - date).days <= 365 and (d - date).days >= 0) for d in row['temp_level2dates'])], axis=1)
            print("Build Modified_Position_original_in_dates based on in_dates")
            data['Modified_Position_original_in_dates'] = data.apply(lambda row: [
                i for i, date in enumerate(row['temp_original_in_dates']) if any(((date - d).days <= 365 and (date - d).days >= 0) for d in row['temp_exc_diag_date'])], axis=1) 
            print("Build diag_lost_due_to_exc based on in_dates")
            data[diag_lost_due_to_exc] = data.apply(lambda row: (len(row['Modified_Position_original_in_dates'])/len(row['temp_original_in_dates']))*100, axis=1)      
        elif exclusion_type == "post":
            print("Building post exclusions for "+exc_diag)
            data['earliest_exc_diag_date'] = data['temp_exc_diag_date'].apply(lambda x: min(x) if x else pd.NaT)
            print("Build Modified_Position_in_dates for "+level2dates+" and "+level2codes)
            data['Modified_Position_in_dates'] = data.apply(lambda row: [i for i, date in enumerate(row['temp_level2dates']) if date >= row['earliest_exc_diag_date']], axis=1)
            print("Build Position_in_dates_keep for "+level2dates+" and "+level2codes)
            data['Position_in_dates_keep'] = data.apply(lambda row: [i for i, date in enumerate(row['temp_level2dates']) if date < row['earliest_exc_diag_date']], axis=1)
            print("Build Modifier_Position_exc_diagdate for "+level2datemodifierdates+" and "+level2datemodifiercodes)
            data['Modifier_Position_exc_diagdate'] = data.apply(lambda row: [i for i, date in enumerate(row['temp_exc_diag_date']) if date == row['earliest_exc_diag_date']], axis=1)
            print("Build Modified_Position_original_in_dates based on in_dates")
            data['Modified_Position_original_in_dates'] = data.apply(lambda row: [i for i, date in enumerate(row['temp_original_in_dates']) if date >= row['earliest_exc_diag_date']], axis=1)
            data = data.drop('earliest_exc_diag_date', axis=1)
            print("Build diag_lost_due_to_exc based on in_dates")
            data[diag_lost_due_to_exc] = data.apply(lambda row: (len(row['Modified_Position_original_in_dates'])/len(row['temp_original_in_dates']))*100, axis=1)
        elif exclusion_type == "lifetime":
            print("Building Lifetime exclusions for "+exc_diag)
            print("Build Modified_Position_in_dates for "+level2dates+" and "+level2codes)
            data['Modified_Position_in_dates'] = data['temp_level2dates'].apply(lambda dates: list(range(len(dates))))
            print("Build Position_in_dates_keep for "+level2dates+" and "+level2codes)
            data['Position_in_dates_keep'] = [[] for _ in range(len(data))]
            print("Build Modifier_Position_exc_diagdate for "+level2datemodifierdates+" and "+level2datemodifiercodes)
            data['Modifier_Position_exc_diagdate'] = data['temp_exc_diag_date'].apply(lambda dates: list(range(len(dates))))
            print("Build Modified_Position_original_in_dates based on in_dates")
            data['Modified_Position_original_in_dates'] = data['temp_original_in_dates'].apply(lambda dates: list(range(len(dates))))
            print("Build diag_lost_due_to_exc based on in_dates")
            data[diag_lost_due_to_exc] = 100
        else:
            print("WARNING: supplied variable for exclusion type not known.")
        data['Modified_Position_in_dates'] = data['Modified_Position_in_dates'].fillna("")
        data['Position_in_dates_keep'] = data['Position_in_dates_keep'].fillna("")
        data['Modifier_Position_exc_diagdate'] = data['Modifier_Position_exc_diagdate'].fillna("")
        data['Modified_Position_original_in_dates'] = data['Modified_Position_original_in_dates'].fillna("")
        if (verbose):
            print(data)
        print("Build UPDATED "+level2dates+" and "+level2codes)
        # Keep only those level2dates/level2codes that should not be excluded
        data['temp_new_level2codes'] = data.apply(lambda row: [row['temp_in_diagnoses'][i] if i in row['Position_in_dates_keep'] else "" for i in range(len(row['temp_in_diagnoses']))], axis=1)
        data[level2codes] = data['temp_new_level2codes'].apply(lambda row: [entry for entry in row if entry != ""])
        data['temp_new_level2dates'] = data.apply(lambda row: [row['temp_level2dates'][i].strftime(DateFormat) if i in row['Position_in_dates_keep'] else "" for i in range(len(row['temp_level2dates']))], axis=1)
        data[level2dates] = data['temp_new_level2dates'].apply(lambda row: [entry for entry in row if entry != ""])
        print("Update "+level2datemodifierdates+" and "+level2datemodifiercodes)
        data['temp_new_level2datemodifierdates'] = data.apply(lambda row: [row['temp_exc_diag_date'][i].strftime(DateFormat) if not pd.isna(row['temp_exc_diag_date'][i]) and i in row['Modifier_Position_exc_diagdate'] else "" for i in range(len(row['temp_exc_diag_date']))], axis=1)
        data[level2datemodifierdates] = data.apply(lambda row: [
            date for date in row[level2datemodifierdates] + row['temp_new_level2datemodifierdates']
            if date.strip() != ''],
            axis=1)
        data['temp_new_level2datemodifiercodes'] = data.apply(lambda row: [row['temp_exc_diag'][i] if i in row['Modifier_Position_exc_diagdate'] else "" for i in range(len(row['temp_exc_diag']))], axis=1)
        data[level2datemodifiercodes] = data.apply(lambda row: [
            date for date in row[level2datemodifiercodes] + row['temp_new_level2datemodifiercodes']
            if date.strip() != ''
            ], axis=1)
        # Add information, which exclusion it was
        data['temp_new_level2datemodifierDX'] = data.apply(lambda row: [exc_diag] * len([1 for item in row['Modifier_Position_exc_diagdate'] if item != ""]), axis=1)
        print(data['temp_new_level2datemodifierDX'])
        data[level2datemodifierDXs] = data.apply(lambda row: [
            date for date in row[level2datemodifierDXs] + row['temp_new_level2datemodifierDX']
            if date.strip() != ''
            ], axis=1)
        # How many main diagnoses were effected by this exclusion based on the main unchange diagnoses
        data[exc_diag_inflicted_changes] = 0
        data.loc[(data['Modified_Position_original_in_dates'].apply(len) > 0) & (data['diagnosis'] == 'Case'), exc_diag_inflicted_changes] = data.loc[
            (data['Modified_Position_original_in_dates'].apply(len) > 0) & (data['diagnosis'] == 'Case'), 'Modified_Position_original_in_dates'].apply(len).fillna(0)
        data[diag_inflicted_dates] = data.apply(lambda row: [row['temp_original_in_dates'][i].strftime(DateFormat) if not pd.isna(row['temp_original_in_dates'][i]) and i in row['Modified_Position_original_in_dates'] else "" for i in range(len(row['temp_original_in_dates']))], axis=1)
        data[diag_inflicted_dates] = data[diag_inflicted_dates].apply(lambda row: [entry for entry in row if entry != ""])
        data[diag_inflicted_codes] = data.apply(lambda row: [row['temp_original_in_diagnoses'][i] if i in row['Modified_Position_original_in_dates'] else "" for i in range(len(row['temp_original_in_dates']))], axis=1)
        data[diag_inflicted_codes] = data[diag_inflicted_codes].apply(lambda row: [entry for entry in row if entry != ""])    
        data[diag_inflicted_N] = data.loc[(data[exc_diag] != "") & (data['diagnosis'] == 'Case'), 'in_dates'].apply(lambda x: len(x))
        # Update the Exclusion codes. This will add for relevant rows the currently supplied value to the list of Level2_ExclusionReason
        data.loc[(data['Modified_Position_in_dates'].apply(len) > 0) & (data['diagnosis'] == 'Case'), 'Level2_ExclusionReason'] = data.loc[(data['Modified_Position_in_dates'].apply(len) > 0) & (data['diagnosis'] == 'Case'), 'Level2_ExclusionReason'].apply(lambda x: x + [diag_excode])
        print("Drop unneeded columns")
        # Drop temporary
        data = data.drop(['temp_level2dates', 'temp_exc_diag_date', 'Modifier_Position_exc_diagdate', 'Position_in_dates_keep', 'Modified_Position_in_dates', 'temp_exc_diag', 'temp_in_diagnoses', 'temp_new_level2datemodifierdates', 'temp_new_level2datemodifiercodes', 'temp_new_level2dates', 'temp_new_level2codes', 'Modified_Position_original_in_dates', 'temp_original_in_dates', 'temp_original_in_diagnoses', 'temp_new_level2datemodifierDX'], axis=1)
        if (verbose):
            print(data.head(2))
    else:
        del(all_data[diag_lost_due_to_exc])
        print(Warning("No cases given for "+exc_diag+": Skipping this diagnosis."))
    out_data = pd.concat([all_data[~all_data.index.isin(data.index)], data])
    if (verbose):
        print(out_data.head(2))
    del data
    del all_data
    return out_data

# Function to read CSV file with pandas
def read_csv_chunk(filename, chunksize, sep):
    chunks = []
    for chunk in pd.read_csv(filename, sep=sep, chunksize=chunksize, dtype=str):
        chunks.append(chunk)
    return pd.concat(chunks, axis=0)

def parallelize_file_reading(fn,sep,chunksize=10000,num_threads=4):
    # Use ThreadPoolExecutor to read CSV file with multiple threads
    with ThreadPoolExecutor(max_workers=num_threads) as executor:
        futures = [executor.submit(read_csv_chunk, fn, chunksize, sep) for _ in range(num_threads)]
        results = [future.result() for future in futures]
    # Concatenate results from different threads
    final_result = pd.concat(results, axis=0)
    return final_result


# Reformat lists to concatenated strings
def combine_str_entries(row, col_prefix):
    if row[f"{col_prefix}"] == '':
        return ''
    else:
        if isinstance(row[f"{col_prefix}"], list) and len(row[f"{col_prefix}"]) > 1:
            return ','.join(str(entry) for entry in row[f"{col_prefix}"] if entry != "")
        else:
            return str(row[f"{col_prefix}"])
    

def combine_date_entries(row, col_prefix):
    if row[col_prefix] == '':
        return row[col_prefix]
    else:
        if isinstance(row[col_prefix],tuple) and np.issubdtype(row[col_prefix], np.datetime64):
            if not isinstance(row[col_prefix].item(), list):
                try:
                    return pd.to_datetime(np.datetime_as_string(row[col_prefix], unit='D').item(), format=DateFormat).strftime(DateFormat)
                except ValueError:
                    return row[col_prefix]  # Return empty string if conversion fails
            else:
                try:
                    return ','.join(pd.to_datetime(np.datetime_as_string(entry, unit='D').item(), format=DateFormat).strftime(DateFormat) for entry in row[col_prefix])
                except ValueError:
                    return row[col_prefix]  # Return empty string if conversion fails
        elif isinstance(row[col_prefix], object) and isinstance(row[col_prefix], list):
            try:
                return ','.join(entry.strftime(DateFormat) if isinstance(entry, datetime) else pd.to_datetime(entry, format=DateFormat).strftime(DateFormat) for entry in row[col_prefix] if hasattr(row[col_prefix], '__iter__'))
            except ValueError:
                return row[col_prefix]  # Return empty string if conversion fails
        elif isinstance(row[col_prefix], datetime):
            if not isinstance(row[col_prefix], list):
                try:
                    return row[col_prefix].strftime(DateFormat)
                except ValueError:
                    return row[col_prefix]  # Return empty string if conversion fails
            else:
                try:
                    return ','.join(entry.strftime(DateFormat) for entry in row[col_prefix])
                except ValueError:
                    return row[col_prefix]  # Return empty string if conversion fails
        elif isinstance(row[col_prefix], str):  # Check if row is already a string
            return row[col_prefix]  # Return empty string if conversion fails
        else:
            return str(row[col_prefix])  # Convert to string if not a date object or string


from multiprocessing import Pool

def extract_columns(df, pattern):
    matching_columns = []
    for column in df.columns:
        if pattern in column:
            matching_columns.append(column)
    return matching_columns

# Custom date parser function
def custom_date_parser(date_str):
    global DateFormat
    if date_str == "I":
        return "I"
    else:
        return pd.to_datetime(date_str, format=DateFormat)

#if (dbds_run):
        #    #Quality assurance, if the input for diagnostic codes does not contain ICD.
        #    if(type(in_pheno_codes) == list):
        #        output_list = update_icd_coding(dst=dst, RegisterRun=RegisterRun, data=in_pheno_codes, dbdschb=dbds_run, ipsych=ipsych_run, eM=exact_match, skip=skip_icd_update, remove_point_in_diag_request=remove_point_in_diag_request) 
        #    else:
        #        output_list = update_icd_coding(dst=dst, RegisterRun=RegisterRun, data=in_pheno_codes[pheno_requestcol], dbdschb=dbds_run, ipsych=ipsych_run, eM=exact_match, skip=skip_icd_update, remove_point_in_diag_request=remove_point_in_diag_request) 
        #    values_to_match = set(output_list)
        #elif (ipsych_run):
        #    #Quality assurance, if the input for diagnostic codes does not contain ICD.
        #    if(type(in_pheno_codes) == list):
        #        output_list = update_icd_coding(dst=dst, RegisterRun=RegisterRun, data=in_pheno_codes, dbdschb=dbds_run, ipsych=ipsych_run, eM=exact_match, skip=skip_icd_update, remove_point_in_diag_request=remove_point_in_diag_request) 
        #    else:
        #        output_list = update_icd_coding(dst=dst, RegisterRun=RegisterRun, data=in_pheno_codes[pheno_requestcol], dbdschb=dbds_run, ipsych=ipsych_run, eM=exact_match, skip=skip_icd_update, remove_point_in_diag_request=remove_point_in_diag_request) 
        #    values_to_match = set(output_list)
def get_values_to_match(multi_inclusions,in_pheno_codes,pheno_requestcol,remove_point_in_diag_request):
    values_to_match = ""
    if (not multi_inclusions):
        if isinstance(in_pheno_codes, str):
            # If it's a single string, convert it into a list with one element
            values_to_match = [in_pheno_codes]
        elif isinstance(in_pheno_codes, list):
            values_to_match = in_pheno_codes
        elif isinstance(in_pheno_codes, pd.Series) or isinstance(in_pheno_codes, pd.DataFrame):
            values_to_match = in_pheno_codes[pheno_requestcol]
        else:
            raise TypeError("Unexpected type for in_pheno_codes")
        # Assuming 'column_name' is the column where you want to check for the value" - Create a set of values from the second file's column 
        if (remove_point_in_diag_request and not values_to_match): #values_to_match.empty):
            print("INFO: Removing \".\" in your pheno request codes. If you don't want this, use --skip_icd_update")
            if isinstance(in_pheno_codes, list):
                values_to_match = set([value.replace('.', '') for value in in_pheno_codes])
            elif isinstance(in_pheno_codes, str):
                values_to_match = {in_pheno_codes.replace('.', '')}
            else:
                values_to_match = set(in_pheno_codes[pheno_requestcol].str.replace('.', '', regex=False))
        elif (isinstance(values_to_match, str) and values_to_match == ""):
            if (not isinstance(in_pheno_codes, list)):
                values_to_match = set(in_pheno_codes[pheno_requestcol])
            else:
                values_to_match = in_pheno_codes
        elif (isinstance(values_to_match, (pd.Series, pd.DataFrame)) and values_to_match.empty):
            if (not isinstance(in_pheno_codes, list)):
                values_to_match = set(in_pheno_codes[pheno_requestcol])
            else:
                values_to_match = in_pheno_codes
        values_to_match = set(str(value) for value in values_to_match)
        print(values_to_match)
    else:
        # Multi-inclusions case
        if isinstance(in_pheno_codes, str):
            # If it's a single string, treat it as a list with one element
            values_to_match = [in_pheno_codes]
        elif isinstance(in_pheno_codes, list):
            values_to_match = in_pheno_codes
        else:
            values_to_match = set(str(value) for value in in_pheno_codes.iloc[0]["Disorder Codes"])
    return values_to_match

def load_mapping_rows(file_path, iidcol, target_iids):
    """
    Load only rows mapping to a list of IIDs from a file.
    
    Parameters:
    - file_path: str, path to the CSV file
    - iidcol: str, column name containing IIDs
    - target_iids: list, list of IIDs to map
    
    Returns:
    - pd.DataFrame: DataFrame containing only the matching rows
    """
    # Step 1: Load only the `iidcol` column
    iids_df = pd.read_csv(file_path, usecols=[iidcol])
    # Ensure `iidcol` and `target_iids` are of the same type
    iids_df[iidcol] = iids_df[iidcol].astype(str)  # Cast column to string
    target_iids = [str(iid) for iid in target_iids]  # Cast target_iids to string
    
    # Step 2: Find the row indices for the target IIDs
    matching_rows = [0] + (iids_df[iids_df[iidcol].isin(target_iids)].index + 1).tolist()
    print(f"Identifying rows to load from df1 based on the current set of iids: matching_rows: {matching_rows[:10]}; iids in df1: {iids_df[:10]}; iids to grep: {target_iids[:10]}")
    # Step 3: Load only the matching rows
    # Prepare the `skiprows` argument to skip all rows except the header and matching rows
    skip_rows = lambda x: x not in matching_rows
    #skip_rows = set(range(len(iids_df))) - set(matching_rows)
    # final_df = pd.read_csv(file_path, skiprows=lambda x: x in skip_rows)
    
    return matching_rows


def build_temp_file(file_path, row_indices, temp_file="filtered_temp.csv", index_file="row_indices.txt", verbose=False):
    """
    Filter rows from a large CSV file using sed with a list of row indices.

    Parameters:
    - file_path: str, path to the CSV file
    - row_indices: list, list of row indices to extract (1-based indexing)
    - temp_file: str, path to save the filtered temporary file

    Returns:
    - None (creates the filtered temp file)
    """

    if index_file == "row_indices.txt":
        index_file = str(uuid.uuid4())[:4]+".row_indices.csv"
    # Write the row indices to a temporary file (convert to 1-based indexing)
    with open(index_file, "w") as f:
        f.write("\n".join([str(i + 1) for i in row_indices]))  # +1 for 1-based indexing

    # Use awk to extract rows with matching line numbers
    awk_command = f"awk 'NR==FNR{{lines[$1]; next}} FNR in lines' {index_file} {file_path} > {temp_file}"
    if verbose:
        print(f"Running awk command: {awk_command}")
    subprocess.run(awk_command, shell=True, check=True)

    # Clean up the temporary index file
    #os.remove(index_file)


def process_pheno_and_exclusions(MatchFI, df3, df1, iidcol, verbose, ctype_excl, ctype_incl, Filter_YoB, Filter_Gender, use_predefined_exdep_exclusions, RegisterRun, dst, ipsych_run, dbds_run, cluster_run, exact_match, skip_icd_update, remove_point_in_diag_request, ICDCM, qced_iids, general_exclusions, multi_inclusions, in_pheno_codes, pheno_requestcol, diagnostic_col, atc_diag_col, birthdatecol, atc_date_col, atc_cols_to_read_as_date, atc_file, fsep, BuildEntryExitDates, lifetime_exclusions_file, post_exclusions_file, oneYearPrior_exclusions_file, outfile, write_Plink2_format, write_fastGWA_format, write_pickle, n_stam_iids, exclCHBcontrols, iidstatus_col, addition_information_file, sexcol, input_date_in_name, input_date_out_name, append, df4):
    print(f"Starting process_pheno_and_exclusions with df1.head(5):{df1.head(5)}")
    if MatchFI:
        print("Restraining df1 and df3 (-f and -i) to only overlapping IIDs. This can be avoided by using --MatchFI")
        # use only those IIDs that are given in df3 and df1. 
        df3_N_before = str(df3[iidcol].nunique())
        df1_N_before = str(df1[iidcol].nunique())
        df3 = df3[df3[iidcol].isin(df1[iidcol])]
        df1 = df1[df1[iidcol].isin(df3[iidcol])]
        print("After Restraining df1 ",str(df1[iidcol].nunique()),"(",df1_N_before,") and df3 ",str(df3[iidcol].nunique()),"(",df3_N_before,"); now(before)")
    else:
        print("Restraining df1 to IIDs overlapping with df3 (-f to -i). As df3 (-i) is supposed to supply the information like Birthdate and so on and it would not make much sense to go forward without this information.")
        # use only those IIDs that are given in df3. 
        df1_N_before = str(df1[iidcol].nunique())
        df1 = df1[df1[iidcol].isin(df3[iidcol])]
        print("After Restraining df1 ",str(df1[iidcol].nunique()),"(",df1_N_before,") and df3 ",str(df3[iidcol].nunique()),"; now(before)")
    
    if (verbose):
        print(f"In process_pheno_and_exclusions. df1.head(5):{df1.head(5)}; df3.head(5):{df3.head(5)}")
        print(f"In process_pheno_and_exclusions. df1.columns:{df1.columns}; df3.columns:{df3.columns}")
    print(diagnostic_col)
    if diagnostic_col != "diagnosis":
        if diagnostic_col in df1.columns:
            df1.rename(columns={diagnostic_col: "diagnosis"}, inplace=True)
        if diagnostic_col in df3.columns:
            df3.rename(columns={diagnostic_col: "diagnosis"}, inplace=True)
        if (verbose):
            print(f"After renaming \"{diagnostic_col}\" to \"diagnosis\". df1.columns:{df1.columns}; df3.columns:{df3.columns}")
    diagnostic_col = "diagnosis"
    if (verbose):
        print("Mem after loading all input:")
        usage()
    if (ctype_excl != ""):
        if (dbds_run):
            ctype_excllusions = ctype_excl.split(",")
            len_before = len(df1)
            df1 = df1.loc[~df1["type"].isin(ctype_excllusions)]
            len_after = len(df1)
            print("Excluded ",len_before-len_after," Diagnoses from the main input file due to ctype_excllusions ",ctype_excl,". This does not reflect case/control diagnoses; This is only a general information.")
        else:
            print("Warning: You selected to exclude c_types but you are not running this method on a CHB/DBDS Server!")
    if (ctype_incl != ""):
        if (dbds_run):
            ctype_inclusions = ctype_incl.split(",")
            len_before = len(df1)
            df1 = df1.loc[df1["type"].isin(ctype_inclusions)]
            len_after = len(df1)
            print("Excluded ",len_before-len_after," Diagnoses from the main input file due to ctype_inclusions ",ctype_incl,". This does not reflect case/control diagnoses; This is only a general information.")
        else:
            print("Warning: You selected to include only certain c_types but you are not running this method on a CHB/DBDS Server!")
    if (Filter_YoB != ""):
        if dbds_run:
            Filter_YoB = Filter_YoB
            iids_to_keep = df3.loc[df3['birthdate'] > Filter_YoB,iidcol]
            len_before = len(df1)            
            df1 = df1.loc[df1[iidcol].isin(iids_to_keep)]
            len_after = len(df1)
            print("Excluded ",len_before-len_after," of ",len_before," IIDs due to Birthdate before selected date ",Filter_YoB,".")
            del iids_to_keep
        else:
            print("Warning: Selected filter is not implemented yet!")
    if (Filter_Gender != ""):
        if dbds_run:
            iids_to_keep = df3.loc[df3['sex'] == Filter_Gender,iidcol]
            len_before = len(df1)            
            df1 = df1.loc[df1[iidcol].isin(iids_to_keep)]
            len_after = len(df1)
            print("Excluded ",len_before-len_after," of ",len_before," IIDs due to Gender filter ",Filter_Gender,".")
            del iids_to_keep
        else:
            print("Warning: Selected filter is not implemented yet!")
    if (len(df1) == 0):
        print("Error: No IIDs left after initial Filtering. Consider using different Filters. Exiting")
        exit()
    if (qced_iids != ""):
        try:
            print("Filtering now for QC'ed Individuals ("+qced_iids+")")
            if ("fam" in qced_iids):
                qced_iids_to_keep = pd.read_csv(qced_iids, sep = "\t", header = None, dtype=str)
            else:
                qced_iids_to_keep = pd.read_csv(qced_iids, sep = None, header = 'infer', dtype=str)
            qced_iids_to_keep.rename(columns={qced_iids_to_keep.columns[0]: iidcol}, inplace=True)
            if (verbose):
                print("Assuming the first column contains the IID")
            rows_to_drop = df1[~df1[iidcol].isin(qced_iids_to_keep[iidcol])] 
            if len(rows_to_drop) != 0:
                tmp = rows_to_drop[iidcol].copy()
                tmp.drop_duplicates(inplace=True)
                n_exclusions = len(tmp)
                del tmp
            else:
                n_exclusions = 0
            del rows_to_drop
            df1 = df1[df1[iidcol].isin(qced_iids_to_keep[iidcol])]
            print("Dropping "+str(n_exclusions)+" Individual(s) due to QC ("+qced_iids+")")
        except Exception as e:
            print(f"An error occured while loading or processing the QC file. This step will now be skipped.\nHead of the file: {qced_iids_to_keep.head(5)}\nError message: {e}")
            qced_iids = ""
    if (general_exclusions != ""):
        try:
            print("Filtering now for general exclusion Individuals ("+general_exclusions+")")
            # Load the IIDs that should be excluded from file as a DataFrame
            iids_to_exclude = pd.read_csv(general_exclusions, sep=" ", dtype=str)
            if (verbose):
                print(iids_to_exclude.head(5))
            iids_to_exclude.rename(columns={ iids_to_exclude.columns[0]: iidcol}, inplace=True)
            # Identify rows in df1 that have matching rows in iids_to_exclude (where iids_to_exclude values are not NaN)
            rows_to_drop = df1[df1[iidcol].isin(iids_to_exclude[iidcol])]
            if len(rows_to_drop) != 0:
                tmp = rows_to_drop[iidcol].copy()
                tmp.drop_duplicates(inplace=True)
                n_exclusions = len(tmp)
                del tmp
            else:
                n_exclusions = 0
            del rows_to_drop
            df1 = df1[~df1[iidcol].isin(iids_to_exclude[iidcol])]
            print("Dropping "+str(n_exclusions)+" Individual(s) due to general exclusion (depending on input file, but could be e.g. due to withdrawal of consent)")
            if (verbose):
                print("Mem after building general exclusions and excluding these from the base file:")
                usage()
        except Exception as e:
            print(f"An error occured while loading or processing the General exclusion file. This step will now be skipped.\nError message: {e}")
            general_exclusions = ""
    if (len(df1) == 0):
        print("Error: No IIDs left after general exclusion Filtering. Consider using different Filters. Exiting")
        exit()
    values_to_match = get_values_to_match(multi_inclusions,in_pheno_codes,pheno_requestcol,remove_point_in_diag_request)
    if (diagnostic_col in df1.columns):
        df1.rename(columns={diagnostic_col: "diagnosis"},inplace=True)
    if (True):
        print("Phenotype codes to map: ")
        print(values_to_match)
        print("in_pheno_codes: ")
        print(in_pheno_codes)
        print("Mem after deleting in_pheno_codes:")
        usage()
    # Use boolean indexing to extract rows from the first file that match the values 
    print("## Build initial Phenotype cases")
    if multi_inclusions:
        tmp_cases_df = pd.DataFrame()
        print("WARNING: Using multiple phenotypes at once can't handle (as of now) ATC codes other than within CHB/DBDS.")
        for InclusionReason in in_pheno_codes['Disorder']:
            print("Building CaseControl list regarding ",InclusionReason)
            values_to_match = ""
            values_to_match = set(str(value) for value in in_pheno_codes.loc[in_pheno_codes['Disorder'] == InclusionReason, "Disorder Codes"].iloc[0])
            print(values_to_match)
            if InclusionReason == 'ATC':
                print("INFO: Identified that you are running ATC call based on the following Name: ",InclusionReason)
                if cluster_run == "CHB_DBDS":
                    try:
                        atc_df1 = pd.read_csv(atc_file, sep=fsep, dtype=object, parse_dates=atc_cols_to_read_as_date, date_format=DateFormat)
                    except TypeError:
                        atc_df1 = pd.read_csv(atc_file, sep=fsep, dtype=object, parse_dates=atc_cols_to_read_as_date)
                    print(atc_df1.columns)
                    if (qced_iids != ""):
                        print(qced_iids_to_keep.head(5))
                        atc_df1 = atc_df1[atc_df1[iidcol].isin(qced_iids_to_keep)]
                        del qced_iids_to_keep
                    if (general_exclusions != ""):
                        print(iids_to_exclude.head(5))
                        atc_df1 = atc_df1[~atc_df1[iidcol].isin(iids_to_exclude[iidcol])]
                        del iids_to_exclude
                print(len(atc_df1.index))
                print(atc_df1.head(5))
                filtered_df = build_phenotype_cases(atc_df1, exact_match, values_to_match, atc_diag_col, birthdatecol, iidcol, atc_date_col, atc_date_col, verbose, Covariates=True, Covar_Name=InclusionReason)[[iidcol, "diagnosis", "diagnoses", "in_dates", "first_dx", "n_diags", "n_unique_in_days"]]
                print("Identified ",str(len(filtered_df.index))," of Cases for ",InclusionReason)
                del atc_df1
            else:
                filtered_df = build_phenotype_cases(df1, exact_match, values_to_match, diagnostic_col, birthdatecol, iidcol, input_date_in_name, input_date_out_name, verbose, Covariates=True, Covar_Name=InclusionReason)[[iidcol, "diagnosis", "diagnoses", "in_dates", "first_dx", "n_diags", "n_unique_in_days"]]
                print("Identified ",str(len(filtered_df.index))," of Cases for ",InclusionReason)
            print(filtered_df.columns)
            if not filtered_df.empty:
                if tmp_cases_df.empty:
                    tmp_cases_df = filtered_df
                    tmp_cases_df[InclusionReason] = tmp_cases_df["diagnosis"].copy()
                    tmp_cases_df[InclusionReason+"_Codes"] = tmp_cases_df["diagnoses"].copy()
                    tmp_cases_df[InclusionReason+"_Dates"] = tmp_cases_df["in_dates"].copy()
                    tmp_cases_df[InclusionReason+'_earliest_date'] = tmp_cases_df["first_dx"].copy()
                    tmp_cases_df[InclusionReason+'_n_diags'] = tmp_cases_df["n_diags"].copy()
                    tmp_cases_df[InclusionReason+'_n_unique_in_days'] = tmp_cases_df["n_unique_in_days"].copy()
                else:
                    filtered_df.rename(columns={"diagnoses":InclusionReason+"_Codes"},inplace=True)
                    filtered_df.rename(columns={"in_dates":InclusionReason+"_Dates"},inplace=True)
                    filtered_df.rename(columns={"first_dx":InclusionReason+'_earliest_date'},inplace=True)
                    filtered_df.rename(columns={"n_diags":InclusionReason+'_n_diags'},inplace=True)
                    filtered_df.rename(columns={"n_unique_in_days":InclusionReason+'_n_unique_in_days'},inplace=True)
                    filtered_df.rename(columns={"diagnosis":InclusionReason},inplace=True)
                    tmp_cases_df = tmp_cases_df.merge(filtered_df, on=iidcol, how='outer')
                tmp_cases_df[InclusionReason] = tmp_cases_df[InclusionReason].fillna("")
                tmp_cases_df[InclusionReason+"_Dates"] = tmp_cases_df[InclusionReason+"_Dates"].fillna("")
            else:
                tmp_cases_df[InclusionReason] = ""
                tmp_cases_df[InclusionReason+"_Dates"] = ""
            del filtered_df
    else:
        print("ATC_Requested: ",ATC_Requested)
        if ATC_Requested == "All": #Add a global variable used to identify if all, some, or none of the codes are ATC based.
            if cluster_run == "CHB_DBDS":
                try:
                    atc_df1 = pd.read_csv(atc_file, sep=fsep, dtype=object, parse_dates=atc_cols_to_read_as_date, date_format=DateFormat)
                except TypeError:
                    atc_df1 = pd.read_csv(atc_file, sep=fsep, dtype=object, parse_dates=atc_cols_to_read_as_date)
                print(atc_df1.columns)
                if (qced_iids != ""):
                    print(qced_iids_to_keep.head(5))
                    atc_df1 = atc_df1[atc_df1[iidcol].isin(qced_iids_to_keep[iidcol])]
                    del qced_iids_to_keep
                if (general_exclusions != ""):
                    print(iids_to_exclude.head(5))
                    atc_df1 = atc_df1[~atc_df1[iidcol].isin(iids_to_exclude[iidcol])]
                    del iids_to_exclude
            print("Running (ALL) ATC identification using atc_diag_col",atc_diag_col,", atc_date_col",atc_date_col)
            print(atc_df1.head(5))
            tmp_cases_df = build_phenotype_cases(atc_df1, exact_match, values_to_match, atc_diag_col, birthdatecol, iidcol, atc_date_col, atc_date_col, verbose)
        elif ATC_Requested == "Some":
            if cluster_run == "CHB_DBDS":
                try:
                    atc_df1 = pd.read_csv(atc_file, sep=fsep, dtype=object, parse_dates=atc_cols_to_read_as_date, date_format=DateFormat)
                except TypeError:
                    atc_df1 = pd.read_csv(atc_file, sep=fsep, dtype=object, parse_dates=atc_cols_to_read_as_date)
                if (qced_iids != ""):
                    atc_df1 = atc_df1[atc_df1[iidcol].isin(iids_to_keep[iidcol])]
                if (general_exclusions != ""):
                    atc_df1 = atc_df1[~atc_df1[iidcol].isin(iids_to_exclude[iidcol])]
            atc_values_to_match = [value for value in values_to_match if str(value).startswith('ATC')]
            non_atc_values_to_match = [value for value in values_to_match if not str(value).startswith('ATC')]
            tmp_cases_df = build_phenotype_cases([df1,atc_df1], exact_match, [non_atc_values_to_match,atc_values_to_match], [diagnostic_col,atc_diag_col], birthdatecol, iidcol, [input_date_in_name,atc_date_col], [input_date_out_name,atc_date_col], verbose)
        else:
            tmp_cases_df = build_phenotype_cases(df1, exact_match, values_to_match, diagnostic_col, birthdatecol, iidcol, input_date_in_name, input_date_out_name, verbose)
    #Housekeeping
    del in_pheno_codes
    if (len(tmp_cases_df) == 0):
        print("Error: No Cases found. Are your input diagnostic codes given in the phenotype file? Exiting")
        print("Info: Phenotype codes to map: ")
        print(values_to_match)
        exit()
    if (input_date_out_name == input_date_in_name):
        input_date_out_name = 'date_out'
    if (BuildEntryExitDates):
        print("## Build initial Phenotype controls")
        tmp_controls_df = merge_IIDs(df1[~df1[iidcol].isin(tmp_cases_df[iidcol])].copy(), diagnostic_col, birthdatecol, input_date_in_name, input_date_out_name, iidcol, verbose, Cases=False, BuildEntryExitDates=True)
        print("Identified initially ",str(tmp_controls_df[iidcol].nunique())," IIDs that do not have an overlapping diagnostic code (Non-Cases).")
        print(len(tmp_cases_df)," Cases; ",len(tmp_controls_df)," Controls.")
        if use_predefined_exdep_exclusions & verbose:
            print("Mem after building tmp_cases_df and tmp_controls_df")
            usage()
        # Set diagnosis column of controls to "controls"
        tmp_controls_df[diagnostic_col] = "Control"
        if (diagnostic_col == "diagnoses"):
            diagnostic_col = "diagnosis"
        # Cleanup controls
        columns_to_keep = [iidcol, diagnostic_col, "first_dx", "last_dx","Entry_Date","Exit_Date"]
        if birthdatecol in tmp_controls_df.columns:
            columns_to_keep.insert(1, birthdatecol)
        tmp_controls_df = tmp_controls_df.loc[:, columns_to_keep]
        if (verbose):
            print("Mem after updating tmp_controls_df:")
            usage()
        # Add controls
        casecontrol_df = pd.concat([tmp_cases_df, tmp_controls_df], ignore_index=True, sort=False)
        casecontrol_df.drop_duplicates(subset=[iidcol], inplace=True)
        if (verbose):
            print("tmp_cases_df: (",str(tmp_cases_df[iidcol].nunique())," rows) - ",tmp_cases_df.columns)
            print("tmp_controls_df: (",str(tmp_controls_df[iidcol].nunique())," rows) - ",tmp_controls_df.columns)
            print("casecontrol_df: (",str(casecontrol_df[iidcol].nunique())," rows) - ",casecontrol_df.columns)
        del tmp_controls_df
    else:
        casecontrol_df = tmp_cases_df.copy()
    del tmp_cases_df
    if (verbose):
        print("Mem after building casecontrol_df and cleaning up tmp_cases_df and tmp_controls_df:")
        usage()
    if (not dbds_run and input_date_in_name != "date_in" and not isinstance(df1, type(None))):
        #Change colname of indate to 'date_in'
        if (input_date_in_name in df1.columns and "date_in" in df1.columns):
            print("WARNING: There exists a column \"date_in\" within your -f input as well as the column ",input_date_in_name," which will be renamed to \"date_in\". This can lead to issues, thus we will rename the original column \"date_in\" to \"date_in_original\".")
            df1.rename(columns={"date_in": "date_in_original"}, inplace=True)
        df1.rename(columns={input_date_in_name: "date_in"}, inplace=True)
        input_date_in_name = "date_in"
    if not use_predefined_exdep_exclusions and lifetime_exclusions_file == "" and post_exclusions_file == "" and oneYearPrior_exclusions_file == "":
        del df1
        if (verbose):
            print("Mem after building tmp_cases_df and tmp_controls_df; AND deleting df1:")
            usage()
    # Append CaseControl Dataframe with information regarding the ExDEP exclusions
    if (use_predefined_exdep_exclusions):
        casecontrol_df = build_ExDEP_exclusions(casecontrol_df, df1, diagnostic_col, iidcol, birthdatecol, input_date_in_name, input_date_out_name, id_diagnostics, "ID", "ID_Dates", exact_match, verbose, get_earliest_date_from_data=True, dbds_run=dbds_run)
        casecontrol_df = build_ExDEP_exclusions(casecontrol_df, df1, diagnostic_col, iidcol, birthdatecol, input_date_in_name, input_date_out_name, scz_diagnostics, "SCZ", "SCZ_Dates", exact_match, verbose, get_earliest_date_from_data=True, dbds_run=dbds_run)
        casecontrol_df = build_ExDEP_exclusions(casecontrol_df, df1, diagnostic_col, iidcol, birthdatecol, input_date_in_name, input_date_out_name, bpd_diagnostics, "BPD", "BPD_Dates", exact_match, verbose, get_earliest_date_from_data=True, dbds_run=dbds_run)
        casecontrol_df = build_ExDEP_exclusions(casecontrol_df, df1, diagnostic_col, iidcol, birthdatecol, input_date_in_name, input_date_out_name, dem_diagnostics, "DEM", "DEM_Dates", exact_match, verbose, get_earliest_date_from_data=True, dbds_run=dbds_run)
        casecontrol_df = build_ExDEP_exclusions(casecontrol_df, df1, diagnostic_col, iidcol, birthdatecol, input_date_in_name, input_date_out_name, aud_diagnostics, "AUD", "AUD_Dates", exact_match, verbose, get_earliest_date_from_data=True, dbds_run=dbds_run)
        casecontrol_df = build_ExDEP_exclusions(casecontrol_df, df1, diagnostic_col, iidcol, birthdatecol, input_date_in_name, input_date_out_name, dud_diagnostics, "DUD", "DUD_Dates", exact_match, verbose, get_earliest_date_from_data=True, dbds_run=dbds_run)
        casecontrol_df = build_ExDEP_exclusions(casecontrol_df, df1, diagnostic_col, iidcol, birthdatecol, input_date_in_name, input_date_out_name, mci_diagnostics, "MCI", "MCI_Dates", exact_match, verbose, get_earliest_date_from_data=True, dbds_run=dbds_run)
        casecontrol_df = build_ExDEP_exclusions(casecontrol_df, df1, diagnostic_col, iidcol, birthdatecol, input_date_in_name, input_date_out_name, cti_diagnostics, "CTI", "CTI_Dates", exact_match, verbose, get_earliest_date_from_data=True, dbds_run=dbds_run)
        casecontrol_df = build_ExDEP_exclusions(casecontrol_df, df1, diagnostic_col, iidcol, birthdatecol, input_date_in_name, input_date_out_name, pain_diagnostics, "CP", "CP_Dates", exact_match, verbose, get_earliest_date_from_data=True, dbds_run=dbds_run)
        casecontrol_df = build_ExDEP_exclusions(casecontrol_df, df1, diagnostic_col, iidcol, birthdatecol, input_date_in_name, input_date_out_name, gad_diagnostics, "GAD", "GAD_Dates", exact_match, verbose, get_earliest_date_from_data=True, dbds_run=dbds_run)
        casecontrol_df = build_ExDEP_exclusions(casecontrol_df, df1, diagnostic_col, iidcol, birthdatecol, input_date_in_name, input_date_out_name, pd_diagnostics, "PD", "PD_Dates", exact_match, verbose, get_earliest_date_from_data=True, dbds_run=dbds_run)
        casecontrol_df = build_ExDEP_exclusions(casecontrol_df, df1, diagnostic_col, iidcol, birthdatecol, input_date_in_name, input_date_out_name, phobias_diagnostics, "Phobias", "Phobias_Dates", exact_match, verbose, get_earliest_date_from_data=True, dbds_run=dbds_run)
        casecontrol_df = build_ExDEP_exclusions(casecontrol_df, df1, diagnostic_col, iidcol, birthdatecol, input_date_in_name, input_date_out_name, anx_diagnostics, "ANX", "ANX_Dates", exact_match, verbose, get_earliest_date_from_data=True, dbds_run=dbds_run)
        casecontrol_df = build_ExDEP_exclusions(casecontrol_df, df1, diagnostic_col, iidcol, birthdatecol, input_date_in_name, input_date_out_name, ptsd_diagnostics, "PTSD", "PTSD_Dates", exact_match, verbose, get_earliest_date_from_data=True, dbds_run=dbds_run)
        casecontrol_df = build_ExDEP_exclusions(casecontrol_df, df1, diagnostic_col, iidcol, birthdatecol, input_date_in_name, input_date_out_name, ocd_diagnostics, "OCD", "OCD_Dates", exact_match, verbose, get_earliest_date_from_data=True, dbds_run=dbds_run)
        casecontrol_df = build_ExDEP_exclusions(casecontrol_df, df1, diagnostic_col, iidcol, birthdatecol, input_date_in_name, input_date_out_name, adhd_diagnostics, "ADHD", "ADHD_Dates", False, verbose, get_earliest_date_from_data=True, dbds_run=dbds_run)
        casecontrol_df = build_ExDEP_exclusions(casecontrol_df, df1, diagnostic_col, iidcol, birthdatecol, input_date_in_name, input_date_out_name, asd_diagnostics, "ASD", "ASD_Dates", exact_match, verbose, get_earliest_date_from_data=True, dbds_run=dbds_run)
        casecontrol_df = build_ExDEP_exclusions(casecontrol_df, df1, diagnostic_col, iidcol, birthdatecol, input_date_in_name, input_date_out_name, bul_diagnostics, "BUL", "BUL_Dates", exact_match, verbose, get_earliest_date_from_data=True, dbds_run=dbds_run)
        casecontrol_df = build_ExDEP_exclusions(casecontrol_df, df1, diagnostic_col, iidcol, birthdatecol, input_date_in_name, input_date_out_name, ano_diagnostics, "ANO", "ANO_Dates", exact_match, verbose, get_earliest_date_from_data=True, dbds_run=dbds_run)
        casecontrol_df = build_ExDEP_exclusions(casecontrol_df, df1, diagnostic_col, iidcol, birthdatecol, input_date_in_name, input_date_out_name, sleepdisorder_diagnostics, "Sleep_Disorder", "Sleep_Disorder_Dates", exact_match, verbose, get_earliest_date_from_data=True, dbds_run=dbds_run)
        casecontrol_df = build_ExDEP_exclusions(casecontrol_df, df1, diagnostic_col, iidcol, birthdatecol, input_date_in_name, input_date_out_name, AD_diagnostics, "AD", "AD_Dates", exact_match, verbose, get_earliest_date_from_data=True, dbds_run=dbds_run)
        casecontrol_df = build_ExDEP_exclusions(casecontrol_df, df1, diagnostic_col, iidcol, birthdatecol, input_date_in_name, input_date_out_name, Pain_diagnostics, "Pain", "Pain_Dates", exact_match, verbose, get_earliest_date_from_data=True, dbds_run=dbds_run)
        casecontrol_df = build_ExDEP_exclusions(casecontrol_df, df1, diagnostic_col, iidcol, birthdatecol, input_date_in_name, input_date_out_name, Chronic_diagnostics, "Chronic_illness", "Chronic_illness_Dates", exact_match, verbose, get_earliest_date_from_data=True, dbds_run=dbds_run)
        casecontrol_df = build_ExDEP_exclusions(casecontrol_df, df1, diagnostic_col, iidcol, birthdatecol, input_date_in_name, input_date_out_name, Other_Mental_diagnostics, "Other_Mental_illnesses", "Other_Mental_illnesses_Dates", exact_match, verbose, get_earliest_date_from_data=True, dbds_run=dbds_run)
        del df1
        if (verbose):
            print ("Mem after building all ExDEP exclusions and deleteing df1.")
            usage()
    else:
        if lifetime_exclusions_file != "":
            multi_exclusions = False
            with open(lifetime_exclusions_file, 'r') as file:
                lifetime_exclusions_in = file.readlines()
                columns = lifetime_exclusions_in[0].strip().replace(" ", "").split('\t')
                if len(columns) == 2:
                    multi_exclusions = True
                    print("Detected 2 columns. Exclusions will be ", lifetime_exclusions_in)
                    # Use a list comprehension to create modified lines
                    modified_lines = [line.replace(' ', '') for line in lifetime_exclusions_in]
                    lifetime_exclusions_in = modified_lines.copy()
                elif len(columns) == 1:
                    multi_exclusions = False
                    print("WARNING: You supplied a file with only one column. "
                        "We will set the name of the exclusion automatically to \"LifeExcl\".")
                    # Use a list comprehension to create modified lines
                    modified_lines = ["LifeExcl{}\t{}".format(i, line.replace(" ", "")) for i, line in enumerate(lifetime_exclusions_in, start=1)]
                    lifetime_exclusions_in = modified_lines.copy()
                else:
                    print("ERROR: Could not identify the format of your Lifetime exclusion list. "
                        "It needs to be either a file with only codes, or a file with two columns, "
                        "in the first column the name for the exclusion and in the second column the codes. "
                        "The codes need to be comma-separated, and the columns need to be tab-separated.")
                    sys.exit()
            # Use a lambda function to process each line
            process_line = lambda line: (line.strip().split('\t')[0], line.strip().split('\t')[1].split(','))
            # Apply the lambda function to each line and convert the result to a dictionary
            disorder_dict = dict(map(process_line, lifetime_exclusions_in))
            # Create a dataframe from the dictionary
            lifetime_exclusions = pd.DataFrame(list(disorder_dict.items()), columns=['Disorder', 'Disorder Codes'])
            if (verbose):
                print(lifetime_exclusions['Disorder Codes'])
            # Add information about each lifetime exclusion to the casecontrol_df
            exclNumber = 1
            for lifetimeExclusionReason in lifetime_exclusions['Disorder']:
                print("Build IID list regarding ",lifetimeExclusionReason," Case exclusion")
                updated_codes = []
                updated_codes = update_icd_coding(dst=dst, RegisterRun=RegisterRun, data=disorder_dict.get(lifetimeExclusionReason), dbdschb=dbds_run, ipsych=ipsych_run, eM=exact_match, skip=skip_icd_update, remove_point_in_diag_request=remove_point_in_diag_request) 
                print("Codes used: ",updated_codes)
                casecontrol_df = build_ExDEP_exclusions(casecontrol_df, df1, diagnostic_col, iidcol, birthdatecol, input_date_in_name, input_date_out_name, updated_codes, lifetimeExclusionReason, lifetimeExclusionReason+"_Dates", exact_match, verbose, get_earliest_date_from_data=True, dbds_run=dbds_run)
                if (verbose):
                    print(casecontrol_df)
                exclNumber = exclNumber + 1
            # Clean up
            del(lifetime_exclusions_in, process_line, disorder_dict)
        if post_exclusions_file != "":
            multi_exclusions = False
            with open(post_exclusions_file, 'r') as file:
                post_exclusions_in = file.readlines()
                columns = post_exclusions_in[0].strip().replace(" ", "").split('\t')
                if len(columns) == 2:
                    multi_exclusions = True
                    print("Detected 2 columns. Exclusions will be ", post_exclusions_in)
                    # Use a list comprehension to create modified lines
                    modified_lines = [line.replace(' ', '') for line in post_exclusions_in]
                    post_exclusions_in = modified_lines.copy()
                elif len(columns) == 1:
                    multi_exclusions = False
                    print("WARNING: You supplied a file with only one column. "
                        "We will set the name of the exclusion automatically to \"PostExcl\".")
                    # Use a list comprehension to create modified lines
                    modified_lines = ["PostExcl{}\t{}".format(i, line.replace(" ", "")) for i, line in enumerate(post_exclusions_in, start=1)]
                    post_exclusions_in = modified_lines.copy()
                else:
                    print("ERROR: Could not identify the format of your Post exclusion list. "
                        "It needs to be either a file with only codes, or a file with two columns, "
                        "in the first column the name for the exclusion and in the second column the codes. "
                        "The codes need to be comma-separated, and the columns need to be tab-separated.")
                    sys.exit()
            # Use a lambda function to process each line
            process_line = lambda line: (line.strip().split('\t')[0], line.strip().split('\t')[1].split(','))
            # Apply the lambda function to each line and convert the result to a dictionary
            disorder_dict = dict(map(process_line, post_exclusions_in))
            # Create a dataframe from the dictionary
            post_exclusions = pd.DataFrame(list(disorder_dict.items()), columns=['Disorder', 'Disorder Codes'])
            if (verbose):
                print(post_exclusions['Disorder Codes'])
            # Add information about each lifetime exclusion to the casecontrol_df
            exclNumber = 1
            for ExclusionReason in post_exclusions['Disorder']:
                print("Build IID list regarding ",ExclusionReason," Case exclusion")
                updated_codes = []
                updated_codes = update_icd_coding(dst=dst, RegisterRun=RegisterRun, data=disorder_dict.get(ExclusionReason), dbdschb=dbds_run, ipsych=ipsych_run, eM=exact_match, skip=skip_icd_update, remove_point_in_diag_request=remove_point_in_diag_request) 
                print("Codes used: ",updated_codes)
                casecontrol_df = build_ExDEP_exclusions(casecontrol_df, df1, diagnostic_col, iidcol, birthdatecol, input_date_in_name, input_date_out_name, updated_codes, ExclusionReason, ExclusionReason+"_Dates", exact_match, verbose, get_earliest_date_from_data=True, dbds_run=dbds_run)
                if (verbose):
                    print(casecontrol_df)
                exclNumber = exclNumber + 1
            # Clean up
            del(post_exclusions_in, process_line, disorder_dict)
        if oneYearPrior_exclusions_file != "":
            multi_exclusions = False
            with open(oneYearPrior_exclusions_file, 'r') as file:
                oneYearPrior_exclusions_in = file.readlines()
                columns = oneYearPrior_exclusions_in[0].strip().replace(" ", "").split('\t')
                if len(columns) == 2:
                    multi_exclusions = True
                    print("Detected 2 columns. Exclusions will be ", oneYearPrior_exclusions_in)
                    # Use a list comprehension to create modified lines
                    modified_lines = [line.replace(' ', '') for line in oneYearPrior_exclusions_in]
                    oneYearPrior_exclusions_in = modified_lines.copy()
                elif len(columns) == 1:
                    multi_exclusions = False
                    print("WARNING: You supplied a file with only one column. "
                        "We will set the name of the exclusion automatically to \"OneYearPriorExcl\".")
                    # Use a list comprehension to create modified lines
                    modified_lines = ["OneYearPriorExcl{}\t{}".format(i, line.replace(" ", "")) for i, line in enumerate(oneYearPrior_exclusions_in, start=1)]
                    oneYearPrior_exclusions_in = modified_lines.copy()
                else:
                    print("ERROR: Could not identify the format of your One Year Prior exclusion list. "
                        "It needs to be either a file with only codes, or a file with two columns, "
                        "in the first column the name for the exclusion and in the second column the codes. "
                        "The codes need to be comma-separated, and the columns need to be tab-separated.")
                    sys.exit()
            # Use a lambda function to process each line
            process_line = lambda line: (line.strip().split('\t')[0], line.strip().split('\t')[1].split(','))
            # Apply the lambda function to each line and convert the result to a dictionary
            disorder_dict = dict(map(process_line, oneYearPrior_exclusions_in))
            # Create a dataframe from the dictionary
            oneYearPrior_exclusions = pd.DataFrame(list(disorder_dict.items()), columns=['Disorder', 'Disorder Codes'])
            if (verbose):
                print(oneYearPrior_exclusions['Disorder Codes'])
            # Add information about each lifetime exclusion to the casecontrol_df
            exclNumber = 1
            for ExclusionReason in oneYearPrior_exclusions['Disorder']:
                print("Build IID list regarding ",ExclusionReason," Case exclusion")
                updated_codes = []
                updated_codes = update_icd_coding(dst=dst, RegisterRun=RegisterRun, data=disorder_dict.get(ExclusionReason), dbdschb=dbds_run, ipsych=ipsych_run, eM=exact_match, skip=skip_icd_update, remove_point_in_diag_request=remove_point_in_diag_request) 
                print("Codes used: ",updated_codes)
                casecontrol_df = build_ExDEP_exclusions(casecontrol_df, df1, diagnostic_col, iidcol, birthdatecol, input_date_in_name, input_date_out_name, updated_codes, ExclusionReason, ExclusionReason+"_Dates", exact_match, verbose, get_earliest_date_from_data=True, dbds_run=dbds_run)
                if (verbose):
                    print(casecontrol_df)
                exclNumber = exclNumber + 1
            # Clean up
            del(post_exclusions_in, process_line, disorder_dict)
    if (verbose):
        print ("Mem after building Lifetime exclusions and deleteing df1.")
        usage()
    # Add information from stam_file and keep also IIDs that are not given in df1 (lpr_file).
    result_df = pd.merge(casecontrol_df, df3, on=iidcol, how='outer')
    result_df.loc[(result_df["diagnosis"] != "Case") & 
              (result_df["diagnosis"] != "Control") & 
              (result_df["diagnosis"] != "Case_Excluded"), "diagnosis"] = "Control"
    if (not("birthdate" in result_df.columns) and (birthdatecol in result_df.columns)):
        result_df.rename(columns={birthdatecol:"birthdate"}, inplace=True)
        if (verbose):
            print("Updating colname of result_df from "+birthdatecol+" to birthdate")
    if ("birthdate_x" in result_df.columns):
        if (verbose):
            print("Updating colname of result_df from birthdate_x to birthdate")
        result_df.rename(columns={"birthdate_x":"birthdate"}, inplace=True)
    if (not("birthdate" in result_df.columns) and (birthdatecol+"_x" in result_df.columns)):
        result_df.rename(columns={birthdatecol+"_x":"birthdate"}, inplace=True)
        if (verbose):
            print("Updating colname of result_df from "+birthdatecol+"_x to birthdate")
    if (not("birthdate" in result_df.columns) and (birthdatecol in result_df.columns)):
        result_df.rename(columns={birthdatecol:"birthdate"}, inplace=True)
        if (verbose):
            print("Updating colname of result_df from "+birthdatecol+" to birthdate")
    del df3
    if (verbose):
        print("Mem after deleting df3, updating tmp_controls_df, building dx_result_df, casecontrol_df and result_df:")
        usage()
    if (verbose):
        print(result_df.head(5))
        print(birthdatecol + " in result_df: " + str('birthdate' in result_df.columns))
        print("result_df: (",str(result_df[iidcol].nunique()),"rows) - ",result_df.columns)
    if ("date_in_x" in result_df.columns and not "first_dx" in result_df.columns):
            result_df.rename(columns={"date_in_x": "first_dx"}, inplace=True)
    if ("in_dates_x" in result_df.columns and not "in_dates" in result_df.columns):
            result_df.rename(columns={"in_dates_x": "in_dates"}, inplace=True)
    if ("date_in_y" in result_df.columns and not "first_dx" in result_df.columns):
            result_df.rename(columns={"date_in_y": "first_dx"}, inplace=True)
    if ("in_dates_y" in result_df.columns and not "in_dates" in result_df.columns):
            result_df.rename(columns={"in_dates_y": "in_dates"}, inplace=True)
    if ("date_in_x" in result_df.columns and "first_dx" in result_df.columns):
            result_df.drop("date_in_x", inplace=True)
    if ("in_dates_x" in result_df.columns and "in_dates" in result_df.columns):
            result_df.drop("in_dates_x", inplace=True)
    print(result_df.columns)
    if ("date_in_y" in result_df.columns and "first_dx" in result_df.columns):
            result_df.drop("date_in_y", inplace=True, axis=1)
    if ("in_dates_y" in result_df.columns and "in_dates" in result_df.columns):
            result_df.drop("in_dates_y", inplace=True, axis=1)
    # Calculate Age at first diagnosis; Convert date columns to datetime objects
    result_df['temp_birthdate'] = result_df['birthdate'].copy()
    result_df['temp_first_dx'] = result_df['first_dx'].copy()
    result_df['Age_FirstDx'] = 0
    if (verbose):
        print(result_df.head(5))
        print(result_df['diagnosis'].unique())  # Check unique values in 'diagnosis' column
        print(result_df.dtypes)  # Check data types of columns
        print(result_df[["diagnosis", "temp_first_dx", "temp_birthdate"]].head())
        print(result_df["temp_birthdate"].head())
    # Calculate age for 'Case' rows
    case_indices = np.where(result_df['diagnosis'] == 'Case')[0]
    case_rows = result_df.iloc[case_indices]
    # Identify case rows
    print(case_rows.head(5))
    print(type(case_rows))
    print(case_rows.shape)
    print(result_df.shape)
    print(result_df.loc[case_indices, 'temp_first_dx'].head(5))
    print(result_df.loc[case_indices, 'temp_birthdate'].head(5))
    print(type(result_df.loc[case_indices, 'temp_first_dx']))
    print(type(result_df.loc[case_indices, 'temp_birthdate']))
    # Calculate the age at first diagnosis
    age_first_dx = (pd.to_datetime(result_df.loc[case_indices, 'temp_first_dx'], format=DateFormat) - pd.to_datetime(result_df.loc[case_indices, 'temp_birthdate'], format=DateFormat)).dt.days // 365
    # Assign the calculated values to the DataFrame
    result_df.loc[case_indices, 'Age_FirstDx'] = age_first_dx
    print(result_df.loc[case_indices, ["diagnosis", "temp_first_dx", "temp_birthdate"]].head())
    result_df.drop("temp_first_dx", inplace=True, axis=1)
    result_df.drop("temp_birthdate", inplace=True, axis=1)
    if (verbose):
        print(result_df.head(5))
    if (verbose):
        print(result_df.head(5))
    # Create a DataFrame with unique combinations of ID and DX 
    final_df = result_df.drop_duplicates(subset=iidcol)
    if (verbose):
        print("final_df: (",str(final_df[iidcol].nunique()),"rows) - ",final_df.columns)
    del result_df
    if (sexcol != "sex" and sexcol in final_df.columns and not "sex" in final_df.columns ):
        # Rename sex column 
        final_df.rename(columns={sexcol: 'sex'}, inplace=True)
    elif (sexcol != "sex" and "sex" in final_df.columns and sexcol in final_df.columns ):
        #drop the duplicated column
        final_df.drop(sexcol, inplace=True, axis=1)
    if (verbose):
        print("Mem after deleting result_df:")
        usage()
    if (iidstatus_col != ""):
        if (iidstatus_col in df4):
            df4.rename(columns={iidstatus_col: "C_STATUS"}, inplace=True)
    if (addition_information_file != ''):
        if(len(df4[iidcol]) > df4[iidcol].nunique()):
            print("Warning: df4 contains duplicated iids.")
        # Add the person information
        final_df =  pd.merge(final_df, df4, on=iidcol, how='left')
        del df4
        if (verbose):
            print("Merged df4 (",addition_information_file,") to final_df (",str(final_df[iidcol].nunique())," rows)")
            print("Mem after merging with df4 and deleting df4 (right before saving output file):")
            usage()
    if (lifetime_exclusions_file != ""):
        # Add information about each lifetime exclusion to the casecontrol_df
        exclNumber = 1
        for lifetimeExclusionReason in lifetime_exclusions['Disorder']:
            if (dbds_run):
                tmp_final_df = final_df[[iidcol,'diagnosis','diagnoses','first_dx','in_dates','birthdate','c_diagtype','Age_FirstDx',lifetimeExclusionReason,lifetimeExclusionReason+"_Dates"]].copy()
            else:
                tmp_final_df = final_df[[iidcol,'diagnosis','diagnoses','first_dx','in_dates','birthdate','Age_FirstDx',lifetimeExclusionReason,lifetimeExclusionReason+"_Dates"]].copy()
            final_df = final_df.merge(Exclusion_interpreter(data=tmp_final_df[tmp_final_df["diagnosis"] == "Case"].copy(),excl=lifetimeExclusionReason,min_Age=min_Age,max_Age=max_Age,exclnumber=exclNumber,type="lifetime",verbose=verbose), on=iidcol, how='left')
            exclNumber = exclNumber + 1
    if (oneYearPrior_exclusions_file != ""):
        # Add information about each lifetime exclusion to the casecontrol_df
        exclNumber = 1
        for lifetimeExclusionReason in lifetime_exclusions['Disorder']:
            final_df = Exclusion_interpreter(data=final_df.copy(),excl=lifetimeExclusionReason,min_Age=min_Age,max_Age=max_Age,exclnumber=exclNumber,type="1yprior",verbose=False)
            exclNumber = exclNumber + 1
    if (post_exclusions_file != ""):
        # Add information about each lifetime exclusion to the casecontrol_df
        exclNumber = 1
        for lifetimeExclusionReason in lifetime_exclusions['Disorder']:
            final_df = Exclusion_interpreter(data=final_df.copy(),excl=lifetimeExclusionReason,min_Age=min_Age,max_Age=max_Age,exclnumber=exclNumber,type="post",verbose=False)
            exclNumber = exclNumber + 1
    if (lifetime_exclusions_file != "" or oneYearPrior_exclusions_file != "" or post_exclusions_file != ""):
        final_df.fillna('', inplace=True)
        final_df.loc[(final_df["diagnosis"] == "Control"),"Level3_CaseControl"] = "Control"
        final_df.loc[(final_df['diagnosis'] == "Control"),'Level3_Age_FirstDx'] = 0
        final_df.loc[final_df['diagnosis'] == "Control",'Level2_FirstDx'] = ""
        final_df.loc[final_df['Level2_AgeExclusion'].isna(),'Level2_AgeExclusion'] = "FALSE"
        final_df.replace(np.nan, '') 
        if (verbose):
            print(final_df.head(5))
            print(final_df[[iidcol,'diagnosis','diagnoses','Level3_CaseControl','date_Level2_modifier']].head(10))
        final_df = Build_sankey_data(final_df,outfile,iidcol,verbose)
        # Reformat data for output into comma separates strings isntead of list
        final_df['Level2_ExclusionReason'] = final_df['Level2_ExclusionReason'].apply(sorted)
        final_df['Level2_FirstDx'] = final_df['Level2_FirstDx'].apply(sorted)
    # Identify IIDs that got diagnosed outside the age of <18 and >50
    if (use_predefined_exdep_exclusions):
        print("Interpreting the ExDEP exclusions.")
        if (verbose):
            print("final_df: (",str(final_df[iidcol].nunique())," rows) - ",final_df.columns)
        final_df.fillna('',inplace=True)
        if ('c_diagtype' in final_df.columns):
            tmp_final_df = final_df[[iidcol,'diagnosis','diagnoses','first_dx','in_dates','birthdate','c_diagtype','Age_FirstDx',"ID","ID_Dates","SCZ","SCZ_Dates","BPD","BPD_Dates","DEM","DEM_Dates","AUD","AUD_Dates","DUD","DUD_Dates","MCI","MCI_Dates","CTI","CTI_Dates","CP","CP_Dates"]].copy()
        else:
            tmp_final_df = final_df[[iidcol,'diagnosis','diagnoses','first_dx','in_dates','birthdate','Age_FirstDx',"ID","ID_Dates","SCZ","SCZ_Dates","BPD","BPD_Dates","DEM","DEM_Dates","AUD","AUD_Dates","DUD","DUD_Dates","MCI","MCI_Dates","CTI","CTI_Dates","CP","CP_Dates"]].copy()
        final_df = final_df.merge(ExDEP_exclusion_interpreter(tmp_final_df[tmp_final_df["diagnosis"] == "Case"].copy(), min_Age, max_Age, verbose), on=iidcol, how='left')
        final_df.loc[final_df['Level2_AgeExclusion'].isna(),'Level2_AgeExclusion'] = "FALSE"
        final_df.fillna('', inplace=True)
        final_df.loc[(final_df["diagnosis"] == "Control"),"Level3_CaseControl"] = "Control"
        final_df.loc[(final_df['diagnosis'] == "Control"),'Level3_Age_FirstDx'] = 0
        final_df.loc[:,"Level3_CaseControl_AgeExclusions"] = "Control"
        final_df.loc[(final_df["diagnosis"] == "Case"),"Level3_CaseControl_AgeExclusions"] = "Case_Excluded"
        #if (verbose):
        print(final_df[["Level3_CaseControl", "Level2_AgeExclusion", "Level3_CaseControl_AgeExclusions"]].head(5))
        # Check types
        print(final_df["Level3_CaseControl"].dtypes)
        print(final_df["Level2_AgeExclusion"].dtypes)
        print(final_df["Level3_CaseControl_AgeExclusions"].dtypes)
        print(final_df["Level3_CaseControl"].value_counts())
        print(final_df["Level2_AgeExclusion"].value_counts())
        print(final_df["Level3_CaseControl_AgeExclusions"].value_counts())
        print(final_df[["Level3_CaseControl","Level2_AgeExclusion"]].value_counts())
        print(final_df.loc[
                (final_df["Level3_CaseControl"] == "Case") & 
                (final_df["Level2_AgeExclusion"] == "FALSE"),["Level3_CaseControl", "Level2_AgeExclusion", "Level3_CaseControl_AgeExclusions"]].head(5))
        try:
            final_df.loc[
                (final_df["Level3_CaseControl"] == "Case") & 
                (final_df["Level2_AgeExclusion"] == "FALSE"),
                "Level3_CaseControl_AgeExclusions"] = "Case"
        except Exception as e:
            print(f"Error: {e}")
        print(final_df.loc[
                (final_df["Level3_CaseControl"] == "Case") & 
                (final_df["Level2_AgeExclusion"] == "FALSE"),["Level3_CaseControl", "Level2_AgeExclusion", "Level3_CaseControl_AgeExclusions"]].head(5))
        
        final_df.loc[final_df['diagnosis'] == "Control",'Level2_FirstDx'] = ""
        final_df.loc[final_df['Level2_AgeExclusion'].isna(),'Level2_AgeExclusion'] = "TRUE"
        final_df.replace(np.nan, '') 
        if (verbose):
            print(final_df.head(5))
            print(final_df[[iidcol,'diagnosis','diagnoses','Level3_CaseControl','date_Level2_modifier']].head(10))
        final_df = Build_sankey_data(final_df,outfile,iidcol,verbose)
        # Reformat data for output into comma separates strings isntead of list
        final_df['Level2_ExclusionReason'] = final_df['Level2_ExclusionReason'].apply(sorted)
        final_df['Level2_FirstDx'] = final_df['Level2_FirstDx'].apply(sorted)
    if 'dbds' in final_df.columns:
        n_before = len(final_df.index)
        if("degen_new" in final_df.columns):
            final_df = final_df.loc[~((final_df['dbds'] == "FALSE") & (final_df['degen_new'] == "FALSE") & (final_df['degen_old'] == "FALSE")),].copy()
        else:
            final_df = final_df.loc[~((final_df['dbds'] == "FALSE") & (final_df['degen'] == "FALSE")),].copy()
        n_after = len(final_df.index)
        print("Excluding", str(n_before - n_after), "Individuals as they are not part of CHB or DBDS.")
        if("degen_new" in final_df.columns):
            final_df.loc[(((final_df['degen_new'] == "TRUE") | (final_df['degen_old'] == "TRUE")) & (final_df['diagnosis'] == "Control")), 'diagnosis'] = "Non-case"
        else:
            final_df.loc[((final_df['degen'] == "TRUE") & (final_df['diagnosis'] == "Control")), 'diagnosis'] = "Non-case"
    if 'dbds' in final_df.columns and exclCHBcontrols:
        iids_to_exclude_CHB_controls = final_df.loc[(final_df['diagnosis'] == "Control") & (final_df['degen_new'] == "TRUE") & (final_df['dbds'] == "FALSE"), iidcol]
        all_controls = len(final_df.loc[((final_df['diagnosis'] == "Control") | (final_df['diagnosis'] == "Non-case")), iidcol])
        final_df = final_df[~final_df[iidcol].isin(iids_to_exclude_CHB_controls)]
        print("Excluding ",str(len(iids_to_exclude_CHB_controls))," CHB controls (of ",all_controls," controls), as we are not allowed to use them.")
    # Printing a warning, that one should take care and check that the used phenotype is well within their protocol and otherwise they will need to exclude all CHB Non-cases.
    print_CHB_warning = False
    if 'degen_new' in final_df.columns:
        print(any(((final_df['degen_new'] == True) | (final_df['degen_old'] == True)) & (final_df['diagnosis'] == "Control")))
        print(len(final_df.loc[(((final_df['degen_new'] == "TRUE") | (final_df['degen_old'] == "TRUE")) & (final_df['diagnosis'] == "Control")),]))
        # Count the occurrences of "Case" and "Control" in the "diagnosis" column
        if any(((final_df['degen_new'] == "TRUE") | (final_df['degen_old'] == "TRUE")) & (final_df['diagnosis'] == "Control")):
            temp_df = final_df.copy()
            # Update the diagnosis in the temporary dataframe for display purposes
            temp_df.loc[temp_df['diagnosis'] == "Control", 'diagnosis'] = "Non-case"
            counts = temp_df['diagnosis'].value_counts()
            del(temp_df) 
            print("Case - Non-case counts")
            print(counts)
            print_CHB_warning = True
        else:
            # Count the occurrences of "Case" and "Control" in the "diagnosis" column
            counts = final_df['diagnosis'].value_counts()
            print("Case-Control counts")
            print(counts)
        # Count the occurrences of "Case" and "Control" in the "diagnosis" column
        counts = final_df.loc[final_df['dbds'] == "TRUE",'diagnosis'].value_counts()
        print("Case-Control counts for DBDS only")
        print(counts)
        # Count the occurrences of "Case" and "Control" in the "diagnosis" column
        counts = final_df.loc[final_df['degen_new'] == "TRUE",'diagnosis'].value_counts()
        print("Case - Non-case counts for CHB only")
        print(counts)
        # Count the occurrences of "Case" and "Control" in the "diagnosis" column
        if (len(final_df.loc[(final_df['degen_new'] == "TRUE") & (final_df['dbds'] == "TRUE"),'diagnosis']) > 0):
            counts = final_df.loc[(final_df['degen_new'] == "TRUE") & (final_df['dbds'] == "TRUE"),'diagnosis'].value_counts()
            print("Case-Control counts for IIDs in CHB and DBDS")
            print(counts)
        else:
            print("No overlapping IIDs between CHB and DBDS")
    else:
        # Count the occurrences of "Case" and "Control" in the "diagnosis" column
        counts = final_df['diagnosis'].value_counts()
        print("Case-Control counts")
        print(counts)
    if (use_predefined_exdep_exclusions):
        # Count the occurrences of "Case" and "Control" in the "diagnosis" column
        counts = final_df['Level3_CaseControl'].value_counts()
        print("Case-Control counts based on ExDEP Exclusions")
        print(counts)
    if (lifetime_exclusions_file != ""):
        # Count the occurrences of "Case" and "Control" in the "diagnosis" column
        counts = final_df['Level3_CaseControl'].value_counts()
        print("Case-Control counts based on Exclusions")
        print(counts)
    if (print_CHB_warning):
        print(50*"#")
        print(50*"!")
        print("Warning: Please take care and check that the requested phenotype is well within the underlying protocol!\nOtherwise you will need to exclude all CHB Non-cases to comply with the restricitions.")
        print(50*"!")
        print(50*"#")
    n_final_iids = final_df[iidcol].nunique()
    print("INFO: The input STAM file had ",n_stam_iids," IIDs listed. The Output file of this script has ",n_final_iids," IIDs listed.")
    if "fkode" in final_df.columns:
        #As exclusion criterion
        min_code = 5000
        max_code = 7000
        print("INFO: Updating Information about DK born or not. This uses everything between 5000-7000 on the fkode as non-DK. Find more details here https://www.dst.dk/da/Statistik/dokumentation/Times/cpr-oplysninger/foedreg-kode")
        final_df['both_parents_DK'] = False
        final_df.loc[(final_df['fkode_m'] >= min_code) & (final_df['fkode_m'] <= max_code) & (final_df['fkode_f'] >= min_code) & (stam['fkode_f'] <= max_code), 'both_parents_DK'] = True
        final_df['DK_born'] = False
        final_df.loc[(final_df['fkode'] >= min_code) & (final_df['fkode'] <= max_code), 'DK_born'] = True
    write_mode = 'w'
    write_header = True
    if append:
        write_mode = 'a'
        write_header = False
    if verbose:
        print(f"In process_pheno_and_exclusions, append is set to {append}. write_mode is set to {write_mode}. write_header is set to {write_header}.")
    # Compute the result and save it to a new file 
    final_df.to_csv(outfile, sep="\t", index=False, quoting=False, header=write_header, mode=write_mode)
    reformat_to_tsv(outfile)
    print("INFO: You can find your output file here ",outfile)
    if write_pickle:
        print("Building PICKLE file.")
        with open(outfile+'.pickle', 'wb') as handle:
            pickle.dump(final_df, handle, protocol=pickle.HIGHEST_PROTOCOL)
    if iidcol != "IID":
        final_df['IID'] = final_df[iidcol].copy
    if write_fastGWA_format:
        print("Building FastGWA phenotype file.")
        # Create the 'FID' and 'IID' columns based on 'IID'
        final_df['FID'] = final_df['IID'].copy
        print("Mapping CaseControl status to 0/1")
        # Map 'diagnosis' to 'CaseControl'
        final_df['CaseControl'] = final_df['diagnosis'].map({'Case': "1", 'Control': "0", 'Non-case': "0", 'Case_Excluded': "0"})
        # Subset the DataFrame to the desired columns
        fgwa = final_df[['FID', 'IID', 'CaseControl']]
        print(fgwa)
        print(final_df[['FID', 'IID', 'CaseControl']])
        fgwa.to_csv(outfile+".fgwa.pheno", sep="\t", mode=write_mode, index=False, quoting=False, header=write_header)
    if write_Plink2_format:
        print("Building PLINK2 phenotype file.")
        # Create the 'FID' and 'IID' columns based on 'IID'
        final_df['#FID'] = final_df['IID'].copy
        # Map 'diagnosis' to 'CaseControl'
        final_df['CaseControl'] = final_df['diagnosis'].map({'Case': "2", 'Control': "1", 'Non-case': "1", 'Case_Excluded': "1"})
        # Subset the DataFrame to the desired columns
        plink2 = final_df[['#FID', 'IID', 'CaseControl']]
        plink2.to_csv(outfile+".plink2.pheno", sep="\t", mode=write_mode, index=False, quoting=False, header=write_header)
    # if append == False:
    #     # Compute the result and save it to a new file 
    #     final_df.to_csv(outfile, sep="\t", index=False, quoting=False)
    #     reformat_to_tsv(outfile)
    #     if write_pickle:
    #         with open(outfile+'.pickle', 'wb') as handle:
    #             pickle.dump(final_df, handle, protocol=pickle.HIGHEST_PROTOCOL)
    #     if iidcol != "IID":
    #         final_df['IID'] = final_df[iidcol].copy
    #     if write_fastGWA_format:
    #         # Create the 'FID' and 'IID' columns based on 'IID'
    #         final_df['FID'] = final_df['IID'].copy
    #         # Map 'diagnosis' to 'CaseControl'
    #         final_df['CaseControl'] = final_df['diagnosis'].map({'Case': "1", 'Control': "0", 'Non-case': "0", 'Case_Excluded': "0"})
    #         # Subset the DataFrame to the desired columns
    #         fgwa = final_df[['FID', 'IID', 'CaseControl']]
    #         fgwa.to_csv(outfile+".fgwa.pheno", sep="\t", index=False, quoting=False)
    #     if write_Plink2_format:
    #         # Create the 'FID' and 'IID' columns based on 'IID'
    #         final_df['#FID'] = final_df['IID'].copy
    #         # Map 'diagnosis' to 'CaseControl'
    #         final_df['CaseControl'] = final_df['diagnosis'].map({'Case': "2", 'Control': "1", 'Non-case': "1", 'Case_Excluded': "1"})
    #         # Subset the DataFrame to the desired columns
    #         plink2 = final_df[['#FID', 'IID', 'CaseControl']]
    #         plink2.to_csv(outfile+".plink2.pheno", sep="\t", index=False, quoting=False)
    # else:
    #     # Compute the result and save it to a new file 
    #     final_df.to_csv(outfile, sep="\t", index=False, quoting=False, mode='a', header=False)
    #     reformat_to_tsv(outfile)
    #     if write_pickle:
    #         print("ERROR: writing pickle is not implemented for --lowmem.")
    #     if iidcol != "IID":
    #         final_df['IID'] = final_df[iidcol].copy
    #     if write_fastGWA_format:
    #         # Create the 'FID' and 'IID' columns based on 'IID'
    #         final_df['FID'] = final_df['IID'].copy
    #         # Map 'diagnosis' to 'CaseControl'
    #         final_df['CaseControl'] = final_df['diagnosis'].map({'Case': "1", 'Control': "0", 'Non-case': "0", 'Case_Excluded': "0"})
    #         # Subset the DataFrame to the desired columns
    #         fgwa = final_df[['FID', 'IID', 'CaseControl']]
    #         fgwa.to_csv(outfile+".fgwa.pheno", sep="\t", index=False, quoting=False, mode='a', header=False)
    #     if write_Plink2_format:
    #         # Create the 'FID' and 'IID' columns based on 'IID'
    #         final_df['#FID'] = final_df['IID']
    #         # Map 'diagnosis' to 'CaseControl'
    #         final_df['CaseControl'] = final_df['diagnosis'].map({'Case': "2", 'Control': "1", 'Non-case': "1", 'Case_Excluded': "1"})
    #         # Subset the DataFrame to the desired columns
    #         plink2 = final_df[['#FID', 'IID', 'CaseControl']]
    #         plink2.to_csv(outfile+".plink2.pheno", sep="\t", index=False, quoting=False, mode='a', header=False)



def batch_load_df1_process_pheno_and_exclusions(lpr_file, batch_size, diagnostic_col, lpr_cols_to_read_as_date, potential_lpr_cols_to_read_as_date, dta_input, MatchFI, df3, iidcol, verbose, ctype_excl, ctype_incl, Filter_YoB, Filter_Gender, use_predefined_exdep_exclusions, RegisterRun, dst, ipsych_run, dbds_run, cluster_run, exact_match, skip_icd_update, remove_point_in_diag_request, ICDCM, qced_iids, general_exclusions, multi_inclusions, in_pheno_codes, pheno_requestcol, atc_diag_col, birthdatecol, atc_date_col, atc_cols_to_read_as_date, atc_file, fsep, BuildEntryExitDates, lifetime_exclusions_file, post_exclusions_file, oneYearPrior_exclusions_file, outfile, write_Plink2_format, write_fastGWA_format, write_pickle, n_stam_iids, exclCHBcontrols, iidstatus_col, addition_information_file, sexcol, input_date_in_name, input_date_out_name, df4):
    iids = df3[iidcol].unique()  # Get unique IIDs from df3
    batch_size = int(batch_size)
    num_batches = int(np.ceil(len(iids) / batch_size))  # Calculate the number of batches
    first_write = True  # Control whether to overwrite or append to the file
    df3backup = df3
    for batch_num in range(num_batches):
        # Get the current batch of IIDs
        start_idx = batch_num * batch_size
        end_idx = start_idx + batch_size
        iid_batch = iids[start_idx:end_idx]
        df3 = df3backup[df3backup[iidcol].isin(iid_batch)]
        n_stam_iids=len(df3[iidcol])
        print(f"Processing batch {batch_num + 1}/{num_batches} with {len(iid_batch)} IIDs... {iid_batch[:5]}")
        recnums_to_keep = []
        if ',' in lpr_file:
            # Split the string by comma
            file_paths = lpr_file.split(',')
            multi_diag_cols = False
            if ',' in diagnostic_col:
                diag_cols = diagnostic_col.split(',')
                multi_diag_cols = True
                diagnostic_col = diag_cols[0]
                curr=0
            # Now file_paths is a list containing individual file paths; Initialize an empty DataFrame to store the concatenated data
            df1 = pd.DataFrame()
            # Iterate over each file path
            for lprfile in file_paths:
                df = pd.DataFrame()
                # Read the file in chunks and filter rows; Read the CSV file and append it to df1
                if dta_input:
                    chunk = pd.read_stata(lprfile)
                    # Filter rows where the first column (index 0) matches the IIDs
                    filtered_chunk = chunk[chunk[iidcol].isin(iid_batch)]
                    df = pd.concat([df, filtered_chunk], ignore_index=True, sort=False)
                    del chunk
                    del filtered_chunk
                    #gc.collect()
                else:
                    if verbose:
                        print(f"Loading lprfile {lprfile} in batch loop {batch_num + 1}")
                    df1_rows_to_keep = load_mapping_rows(lprfile, iidcol, iid_batch)
                    temp_file = str(uuid.uuid4())[:4]+".filtered_temp.csv"
                    build_temp_file(lprfile, df1_rows_to_keep, temp_file=temp_file, verbose=verbose)
                    lprfile = temp_file
                    # Identify the set of columns that are dates
                    df_header = pd.read_csv(lprfile, sep=fsep, dtype=object, nrows=0)
                    if verbose:
                        print(f"Header of the current lprfile: {df_header.columns}")
                    if(df_header.columns.empty):
                        print("ERROR: Could not load -f file: ",lprfile)
                        sys.exit()
                    if lpr_cols_to_read_as_date:
                        available_date_cols = list(set([col for col in lpr_cols_to_read_as_date if col in df_header.columns]))
                        if verbose:
                            print("INFO: The following cols are given in current lpr file and will be read as dates: ",available_date_cols)
                    else:
                        available_date_cols = list(set([col for col in potential_lpr_cols_to_read_as_date if col in df_header.columns]))
                        if verbose:
                            print("INFO: No cols supplied (lpr) that should be read as dates. Trying to infer them: ",available_date_cols)
                    del df_header
                    if available_date_cols:
                        try:
                            filtered_chunk = pd.read_csv(lprfile, sep=fsep, dtype=object, engine='python', parse_dates=available_date_cols, date_format=DateFormat)
                            if verbose:
                                print(f"Finished loading chunk {batch_num}. Appending it now to the dataframe (as multiple lpr files were supplied).")
                            df = pd.concat([df, filtered_chunk], ignore_index=True, sort=False)
                        except TypeError:
                            filtered_chunk = pd.read_csv(lprfile, sep=fsep, dtype=object, engine='python', parse_dates=available_date_cols)
                            if verbose:
                                print(f"Finished loading chunk {batch_num}. (Within TypeError) Updating the dateformat and appending it to the dataframe (as multiple lpr files were supplied).")
                            for col in available_date_cols:
                                filtered_chunk[col] = pd.to_datetime(filtered_chunk[col], format=DateFormat, errors='coerce')
                            df = pd.concat([df, filtered_chunk], ignore_index=True, sort=False)
                        except ValueError:
                            filtered_chunk = pd.read_csv(lprfile, sep=fsep, dtype=object, engine='python')
                            if verbose:
                                print(f"Finished loading chunk {batch_num}. (Within ValueError) Updating the dateformat and appending it to the dataframe (as multiple lpr files were supplied).")
                            for col in available_date_cols:
                                if col in filtered_chunk.columns:
                                    filtered_chunk[col] = pd.to_datetime(filtered_chunk[col], format=DateFormat, errors='coerce')
                            df = pd.concat([df, filtered_chunk], ignore_index=True, sort=False)
                        #gc.collect()
                    else:
                        filtered_chunk = pd.read_csv(lprfile, sep=fsep, dtype=str)
                        if verbose:
                            print(f"Finished loading chunk {batch_num}. (Within else) Appending it to the dataframe (as multiple lpr files were supplied).")
                        df = pd.concat([df, filtered_chunk], ignore_index=True, sort=False)
                        #gc.collect()
                    if verbose:
                        print(f"In batch loop of batch_load_df1_process_pheno_and_exclusions with filtered_chunk.head(5):{filtered_chunk.head(5)}")
                    del filtered_chunk
                    #gc.collect()
                    #os.remove(temp_file)
                if multi_diag_cols:
                    df.rename(columns={diag_cols[curr]: diagnostic_col},inplace=True)
                    curr += 1
                print(f"Mem at the end of loading {lprfile} within batch loop {batch_num + 1}/{num_batches}:")
                usage()
                if lpr2nd_file != "":
                    print("INFO: As you are also loading the files containing the secondary diagnoses, we will remove all rows in the standard LPR that refer to DZ03* or DZ763 as these may lead to issues. In addition, we will also remove all \'H\' diag_types.")
                    # Remove rows where diagnostic_col refers to accompanying persons. Recommended by Dorte Helenius on 10.01.2025
                    before_rows = len(df)
                    df = df[~df[diagnostic_col].str.startswith(('DZ03', 'DZ763'), na=False)]
                    print(f"Removed {before_rows - len(df)} rows of the current batch due to DZ03 or DZ763 diagnosis.")
                    recnums_to_keep.extend(df[lpr_recnummer].unique())
                    recnums_to_keep = list(dict.fromkeys(recnums_to_keep))  # Ensure uniqueness while preserving order
                df1 = pd.concat([df1, df], ignore_index=True, sort=False)
                del(df)
            del(file_paths)
        #TODO: Add the check if lpr2nd_file is a list or a single file.
        if lpr2nd_file != "" and recnums_to_keep:
            file_paths = lpr2nd_file.split(',')
            recnum_batch = recnums_to_keep #sorted(df[lpr_recnummer].unique())
            recnums_to_keep = []
            print(f"Identified {len(recnum_batch)} recnums to load from secondary diagnosis files within batch {batch_num + 1}.")
            for lprfile in file_paths:
                if verbose:
                    print(f"Loading lprfile {lprfile} in batch loop {batch_num + 1}")
                df1_rows_to_keep = load_mapping_rows(lprfile, lpr2nd_recnummer, recnum_batch)
                print(f"Keeping the following rows from secondary diagnosis file {lprfile} in batch loop {batch_num + 1}: {df1_rows_to_keep[:10]}")
                temp_file = str(uuid.uuid4())[:4]+".filtered_temp.csv"
                build_temp_file(lprfile, df1_rows_to_keep, temp_file=temp_file, verbose=verbose)
                lprfile = temp_file
                # Identify the set of columns that are dates
                df_header = pd.read_csv(lprfile, sep=fsep, dtype=object, nrows=0)
                if verbose:
                    print(f"Header of the current lprfile: {df_header.columns}")
                if(df_header.columns.empty):
                    print("ERROR: Could not load -f file: ",lprfile)
                    sys.exit()
                filtered_chunk = pd.read_csv(lprfile, sep=fsep, dtype=str)
                if verbose:
                    print(f"Finished loading chunk {batch_num + 1}. (Within else) Appending it to the dataframe (as multiple lpr files were supplied).")
                before_rows = len(filtered_chunk)
                print(f"Identified {before_rows} rows of secondary diagnoses to be added to the main dataframe. {filtered_chunk.head(5)}.")
                # Remove rows where 'diag_type' column has 'H'
                if 'c_diagtype' in filtered_chunk.columns:
                    filtered_chunk = filtered_chunk[filtered_chunk['c_diagtype'] != 'H']
                    print(f"Removed {before_rows - len(filtered_chunk)} rows of secondary diagnoses as they were a referral diagnosis from the General Practitioner (\"H\").")
                if lpr_recnummer != lpr2nd_recnummer:
                    filtered_chunk.rename(columns={lpr2nd_recnummer: lpr_recnummer},inplace=True)
                if diagnostic_col != diagnostic2nd_col:
                    filtered_chunk.rename(columns={diagnostic2nd_col: diagnostic_col},inplace=True)
                df1_temp = pd.merge(
                    filtered_chunk[[diagnostic_col, lpr_recnummer]],  # Base is filtered_chunk with potential duplicates
                    df1.drop(columns=[diagnostic_col], errors='ignore'),  # Add information from df
                    on=lpr_recnummer, 
                    how='inner'  # Only keep rows with matching recnum values
                )
                before_rows = len(df1_temp)
                df1_temp.drop_duplicates(inplace=True)
                print(f"After merging df with filtered_chunk, we have {before_rows} entries. {before_rows - len(df1_temp)} entries were deleted due to duplication. {df1_temp.head(5)}.\n{df1_temp.describe(include='all')}")
                before_rows = len(df1)
                df1 = pd.concat([df1, df1_temp], ignore_index=True, sort=False)
                print(f"Finally, batch {batch_num + 1} resulted in {len(df1)} ({before_rows}+{len(df1_temp)}) entries to be used to determine the CaseControl status.")
                #gc.collect()
                if verbose:
                    print(f"In batch loop of batch_load_df1_process_pheno_and_exclusions with filtered_chunk.head(5):{filtered_chunk.head(5)}\ndf:{df1.head(5)}\ndf1_temp:{df1_temp.head(5)}")
                del filtered_chunk
                #gc.collect()
                os.remove(temp_file)
                del(df1_temp)
                del(lprfile)
            del(file_paths)
        else:
            # Load the first file as a DataFrame(should be phenotype file) 
            if dta_input:
                chunk = pd.read_stata(lpr_file)
                # Filter rows where the first column (index 0) matches the IIDs
                filtered_chunk = chunk[chunk[iidcol].isin(iid_batch)]
                df1 = pd.concat([df1, filtered_chunk], ignore_index=True, sort=False)
                del chunk
                del filtered_chunk
                gc.collect()
            else:
                skip_rows = load_mapping_rows(lpr_file, iidcol, iid_batch)
                if lpr_cols_to_read_as_date:
                    try:
                        filtered_chunk = pd.read_csv(lpr_file, skiprows=skip_rows, sep=fsep, dtype=object, engine='python', parse_dates=lpr_cols_to_read_as_date, date_format=DateFormat)
                        df1 = pd.concat([df1, filtered_chunk], ignore_index=True, sort=False)
                    except TypeError:
                        filtered_chunk = pd.read_csv(lpr_file, skiprows=skip_rows, sep=fsep, dtype=object, engine='python', parse_dates=lpr_cols_to_read_as_date)
                        for col in lpr_cols_to_read_as_date:
                            filtered_chunk[col] = pd.to_datetime(filtered_chunk[col], format=DateFormat, errors='coerce')
                        df1 = pd.concat([df1, filtered_chunk], ignore_index=True, sort=False)
                    del filtered_chunk
                    gc.collect()
                else: 
                    # Identify the set of columns that are dates
                    df1 = pd.read_csv(lpr_file, sep=fsep, engine='python', dtype=object, nrows=0)
                    if (df1.columns.empty):
                        print("ERROR: Could not load -f file: ",lpr_file)
                        sys.exit()
                    available_date_cols = list(set([col for col in potential_lpr_cols_to_read_as_date if col in df1.columns]))
                    print("INFO: No cols supplied (lpr) that should be read as dates. Trying to infer them: ",available_date_cols)
                    if available_date_cols:
                        try:
                            filtered_chunk = pd.read_csv(lpr_file, skiprows=skip_rows, sep=fsep, dtype=object, engine='python', parse_dates=available_date_cols, date_format=DateFormat)
                            df1 = pd.concat([df1, filtered_chunk], ignore_index=True, sort=False)
                        except TypeError:
                            filtered_chunk = pd.read_csv(lpr_file, skiprows=skip_rows, sep=fsep, dtype=object, engine='python', parse_dates=available_date_cols)
                            for col in lpr_cols_to_read_as_date:
                                filtered_chunk[col] = pd.to_datetime(filtered_chunk[col], format=DateFormat, errors='coerce')
                            df1 = pd.concat([df1, filtered_chunk], ignore_index=True, sort=False)
                        del filtered_chunk 
                        gc.collect()
                    else:
                        filtered_chunk = pd.read_csv(lpr_file, skiprows=skip_rows, sep=fsep, dtype=str)
                        df1 = pd.concat([df1, filtered_chunk], ignore_index=True, sort=False)
                        del filtered_chunk 
                        gc.collect()
        gc.collect()
        if(exact_match):
            print("Info: Updating the diagnostic codes to be all Uppercase to be able to run --eM")
            for index, value in df1[diagnostic_col].items():
                if isinstance(value, str):
                    df1.at[index, diagnostic_col] = value.upper()
        print(f"At the end of batch_load_df1_process_pheno_and_exclusions with df1.head(5):{df1.head(5)}")
        if first_write == True and len(df1) > 0:
            process_pheno_and_exclusions(MatchFI=MatchFI, df1=df1, df3=df3, df4=df4, iidcol=iidcol, verbose=verbose, ctype_excl=ctype_excl, ctype_incl=ctype_incl, Filter_YoB=Filter_YoB, Filter_Gender=Filter_Gender, use_predefined_exdep_exclusions=use_predefined_exdep_exclusions, RegisterRun=RegisterRun, dst=dst, ipsych_run=ipsych_run, dbds_run=dbds_run, cluster_run=cluster_run, exact_match=exact_match, skip_icd_update=skip_icd_update, remove_point_in_diag_request=remove_point_in_diag_request, ICDCM=ICDCM, qced_iids=qced_iids, general_exclusions=general_exclusions, multi_inclusions=multi_inclusions, in_pheno_codes=in_pheno_codes, pheno_requestcol=pheno_requestcol, diagnostic_col=diagnostic_col, atc_diag_col=atc_diag_col, birthdatecol=birthdatecol, atc_date_col=atc_date_col, atc_cols_to_read_as_date=atc_cols_to_read_as_date, atc_file=atc_file, fsep=fsep, BuildEntryExitDates=BuildEntryExitDates, lifetime_exclusions_file=lifetime_exclusions_file, post_exclusions_file=post_exclusions_file, oneYearPrior_exclusions_file=oneYearPrior_exclusions_file, outfile=outfile, write_Plink2_format=write_Plink2_format, write_fastGWA_format=write_fastGWA_format, write_pickle=write_pickle, n_stam_iids=n_stam_iids, exclCHBcontrols=exclCHBcontrols, iidstatus_col=iidstatus_col, addition_information_file=addition_information_file, sexcol=sexcol, input_date_in_name=input_date_in_name, input_date_out_name=input_date_out_name, append=False)
        elif len(df1) > 0:
            process_pheno_and_exclusions(MatchFI=MatchFI, df1=df1, df3=df3, df4=df4, iidcol=iidcol, verbose=verbose, ctype_excl=ctype_excl, ctype_incl=ctype_incl, Filter_YoB=Filter_YoB, Filter_Gender=Filter_Gender, use_predefined_exdep_exclusions=use_predefined_exdep_exclusions, RegisterRun=RegisterRun, dst=dst, ipsych_run=ipsych_run, dbds_run=dbds_run, cluster_run=cluster_run, exact_match=exact_match, skip_icd_update=skip_icd_update, remove_point_in_diag_request=remove_point_in_diag_request, ICDCM=ICDCM, qced_iids=qced_iids, general_exclusions=general_exclusions, multi_inclusions=multi_inclusions, in_pheno_codes=in_pheno_codes, pheno_requestcol=pheno_requestcol, diagnostic_col=diagnostic_col, atc_diag_col=atc_diag_col, birthdatecol=birthdatecol, atc_date_col=atc_date_col, atc_cols_to_read_as_date=atc_cols_to_read_as_date, atc_file=atc_file, fsep=fsep, BuildEntryExitDates=BuildEntryExitDates, lifetime_exclusions_file=lifetime_exclusions_file, post_exclusions_file=post_exclusions_file, oneYearPrior_exclusions_file=oneYearPrior_exclusions_file, outfile=outfile, write_Plink2_format=write_Plink2_format, write_fastGWA_format=write_fastGWA_format, write_pickle=write_pickle, n_stam_iids=n_stam_iids, exclCHBcontrols=exclCHBcontrols, iidstatus_col=iidstatus_col, addition_information_file=addition_information_file, sexcol=sexcol, input_date_in_name=input_date_in_name, input_date_out_name=input_date_out_name, append=True)
        else:
            #TODO IIDs without being case should still be included
            print(f"INFO: Skipping batch {batch_num + 1} as there are no overlapping cases.")
        first_write = False


#need to handle double counts and exclusions
def main(lpr_file, pheno_request, stam_file, addition_information_file, use_predefined_exdep_exclusions, general_exclusions, diagnostic_col, pheno_requestcol, iidcol, birthdatecol, sexcol, fsep, gsep, outfile, exact_match, input_date_in_name, input_date_out_name, qced_iids, ctype_excl, ctype_incl, lifetime_exclusions_file, post_exclusions_file, oneYearPrior_exclusions_file, exclCHBcontrols, Filter_YoB, Filter_Gender, verbose, Build_Test_Set, test_run, MatchFI, skip_icd_update, DateFormat_in, iidstatus_col, remove_point_in_diag_request, num_threads, main_pheno_name, BuildEntryExitDates, build_ophold, write_pickle, write_fastGWA_format, write_Plink2_format, lpr_cols_to_read_as_date, stam_cols_to_read_as_date, MinMaxAge, ICDCM, load_precreated_phenotypes, RegisterRun, lowMem, batchsize, noLeadingICD, lpr_file2, recnum, recnum2, f2col, atc_file, atc_diag_col, argstring):
    global DateFormat
    global ATC_Requested
    global id_diagnostics
    global scz_diagnostics
    global bpd_diagnostics
    global dem_diagnostics
    global aud_diagnostics
    global dud_diagnostics
    global mci_diagnostics
    global cti_diagnostics 
    global pain_diagnostics
    global gad_diagnostics
    global pd_diagnostics
    global phobias_diagnostics
    global anx_diagnostics
    global ptsd_diagnostics
    global ocd_diagnostics
    global adhd_diagnostics 
    global asd_diagnostics
    global bul_diagnostics
    global ano_diagnostics
    global sleepdisorder_diagnostics
    global AD_diagnostics
    global Pain_diagnostics
    global Chronic_diagnostics
    global Other_Mental_diagnostics
    global min_Age
    global max_Age
    global lpr2nd_file
    global lpr_recnummer
    global lpr2nd_recnummer
    global diagnostic2nd_col

    lpr_recnummer = recnum
    lpr2nd_recnummer = recnum2
    lpr2nd_file = lpr_file2
    diagnostic2nd_col = f2col
    num_threads = int(num_threads)
    min_Age = int(MinMaxAge.split(',')[0])
    max_Age = int(MinMaxAge.split(',')[1])
    dta_input=False

    potential_lpr_cols_to_read_as_date = []
    atc_date_col = ""
    atc_cols_to_read_as_date = []
    diclaimer_text = "Disclaimer:\nThis tool is provided as-is without any guarantees or warranties regarding the accuracy, completeness, or reliability of the results. The information generated is intended to assist and should not be relied upon as the sole source for decision-making. Users are strongly encouraged to double-check all outputs, validate results independently, and apply their own judgment and common sense. The creators of this tool are not responsible for any consequences arising from its use. By using this tool, you acknowledge and agree to these terms."
    
    print(diclaimer_text)

    if (Version(pd.__version__) < Version(pd_min_version)):
        print("ERROR: Current Pandas is at version",pd.__version__,"but should be >=",pd_min_version)
        sys.exit()
    if ( Version(('{0[0]}.{0[1]}.{0[2]}'.format(sys.version_info))) < Version(python_min_version)):
        print("ERROR: Current Python is at version",('{0[0]}.{0[1]}.{0[2]}'.format(sys.version_info)),"but should be >=",python_min_version)
        sys.exit()
    
    #TODO: Use MinMaxAge to set the variables for the exclusion criteria.
    
    #Set Global variable based on input
    DateFormat = DateFormat_in
    if (Build_Test_Set):
        print("Generating a test dataset and store it in the directory selected with -o.",outfile)
        generate_test_dataset(outfile)
        print("Finished generating the test data. It is stored in the directory selected with -o.")
        sys.exit()

    cluster_run = ""
    # Register (True) or CHB/DBDS run (False) determine this based on the assumption, that all DBDS.CHB runs will have as hostname cld065
    dbds_run = False
    # Get the local hostname
    hostname = socket.gethostname()
    if ("cld065" in hostname or "dprhdbds" in hostname ):
        dbds_run = True
        cluster_run = "CHB_DBDS"
    else:
        dbds_run = False

    if ("srvfsencrr" in hostname):
        ncrr_run = True
        cluster_run = "NCRR_DST"
    else:
        ncrr_run = False

    # Register (True) or iPSYCH run (False) determine this based on the assumption, that all iPSYCH runs will have as hostname fe-ipsych-01
    ipsych_run = False
    # Get the local hostname
    if ("fe-ipsych-" in hostname or "cn-1" in hostname or "s21" in hostname or "dpibp" in hostname):
        ipsych_run = True
        cluster_run = "iPSYCH"
    else:
        ipsych_run = False

    if ("dpibp" in hostname):
        cluster_run = "IBP_computerome"
    dst=False
    if (cluster_run == "NCRR_DST"):
        dta_input=True
        lpr_cols_to_read_as_date = ['fdato','d_inddto','d_uddto']
        dst=True
        opholdsep="\t"
        if (lpr_file == ''):
            stam_file = "E://Data/rawdata/703935/Population/stam2016h.dta"
            stam_cols_to_read_as_date = ['fdato']#,'statd','fdato_m','fdato_f','statd_m','statd_f']
            lpr_file = "E://Data/rawdata/703935/Ipsych2016/Dst/psyk_adm2018.dta,E://Data/rawdata/703935/Ipsych2016/Dst/lpradm1977_2018.dta" 
            lpr2nd_file = "E://Data/rawdata/703935/Ipsych2016/Dst/psyk_diag2018.dta,E://Data/rawdata/703935/Ipsych2016/Dst/lprdiag1977_2018.dta" 
            #lpr_file = "E://Data/rawdata/703935/Population/psyk_adm2016.dta,E://Data/rawdata/703935/Population/lpr_adm2016b.dta" #"E://Data/rawdata/703935/HEALTH/psyk_adm2016.dta,E://Data/rawdata/703935/HEALTH/lpr_adm2016b.dta"
            addition_information_file = "E://Data/rawdata/703935/Population/ipsych2015design_v2.dta"  #"E://Data/rawdata/703935/Population/stamdata2016.dta"
            diagnostic_col="c_adiag"
            pheno_requestcol = "diagnosis"
            birthdatecol = "fdato"
            iidcol="pnr"
            fsep=","
            gsep=","
            input_date_in_name='d_inddto'
            input_date_out_name='d_uddto'
            general_exclusions = ""
            lpr_recnummer = 'recnum'
            lpr2nd_recnummer = lpr_recnummer
            diagnostic2nd_col = "c_diag"

    if (ipsych_run or cluster_run == "iPSYCH"):
        lpr_cols_to_read_as_date = ['fdato','d_inddto','d_uddto']
        opholdsep="\t"
        if (lpr_file == ''):
            DateFormat = "%d/%m/%Y"
            #stam_file = "/home/imlundberg/scripts/first1000.stamdata2016.csv"
            stam_file = "/faststorage/jail/project/ibp_data_secure/danish_population/2016-01-01_register_raw/stamdata2016.csv"
            stam_cols_to_read_as_date = ['fdato','statd','fdato_m','fdato_f','statd_m','statd_f']
            lpr_file = "/faststorage/jail/project/ibp_data_secure/danish_population/2016-01-01_register_raw/psyk_adm2016.csv,/faststorage/jail/project/ibp_data_secure/danish_population/2016-01-01_register_raw/lpr_adm2016.csv"
            addition_information_file = "" #"/faststorage/jail/project/ibp_data_secure/danish_population/2016-01-01_register_raw/civil2016.csv"
            diagnostic_col="c_adiag"
            pheno_requestcol = "diagnosis"
            birthdatecol = "fdato"
            iidcol="pnr"
            fsep=","
            gsep=","
            input_date_in_name='d_inddto'
            input_date_out_name='d_uddto'
            general_exclusions = ""
    
    if (cluster_run == "IBP_computerome" and stam_file == "/faststorage/jail/project/ibp_data_secure/danish_population/2016-01-01_register_raw/stamdata2016.csv"):
        stam_file = "/dpibp/data/raw/2021-03-18_register_clean/stamdata2016.csv"
        lpr_file = "/dpibp/data/raw/2021-03-18_register_clean/psyk_adm2016.csv,/dpibp/data/raw/2021-03-18_register_clean/lpr_adm2016.csv"
        lpr2nd_file = "/dpibp/data/raw/2021-03-18_register_clean/psyk_diag2016.csv,/dpibp/data/raw/2021-03-18_register_clean/lpr_diag2016.csv" 
        lpr_recnummer = 'k_recnum'
        lpr2nd_recnummer = 'v_recnum'
        diagnostic2nd_col = 'c_diag'
        stam_cols_to_read_as_date = ['fdato','statd','fdato_m','fdato_f','statd_m','statd_f']
        lpr_cols_to_read_as_date = ['fdato','d_inddto','d_uddto']
        sexcol="kqn"
        addition_information_file = ""
        diagnostic_col="c_adiag"
        pheno_requestcol = "diagnosis"
        birthdatecol = "fdato"
        iidcol="pnr"
        fsep=","
        gsep=","
        opholdsep="\t"
        input_date_in_name='d_inddto'
        input_date_out_name='d_uddto'
        general_exclusions = ""
        DateFormat = "%d/%m/%Y"
        remove_point_in_diag_request = True
        remove_ICD_naming = True

    if (dbds_run or cluster_run == "CHB_DBDS"):
        DateFormat = "%Y-%m-%d"
        iidcol = "cpr_enc"
        fsep = "\t"
        gsep = "\t"
        #qced_iids = "/data/preprocessed/genetics/chb_degen_freeze_20210503/DEGEN_GSA_FINAL.fam"
        #qced_iids = "/data/preprocessed/genetics/chb_degen_freeze_20230707/01.Genotypes/GSA_degenx_qc.fam"
        qced_iids = "/data/preprocessed/genetics/chb_degen_freeze_20240614/01.Genotypes/GSA_degen_qc.fam"
        #general_exclusions = "/data/preprocessed/genetics/chb_degen_freeze_20210503/degen_exclusion_latest"
        #general_exclusions = "/data/preprocessed/genetics/chb_degen_freeze_20230707/degen_exclusion_latest"
        general_exclusions = "/data/preprocessed/genetics/chb_degen_freeze_20240614/degen_exclusion_latest"
        lpr_cols_to_read_as_date = ['date_in','date_out']
        if (lpr_file == ''):
            diagnostic_col = "diagnosis"
            pheno_requestcol = "diagnosis"
            input_date_in_name = "date_in"
            input_date_out_name = "date_out"
            #lpr_file = "/data/projects_chb/werge_degen/20221220/degen_registry_lpr_lpr3_diagnoses.tsv" #cpr_enc; source;date_in;date_out;type;diagnosis
            #lpr_file = "/data/projects_chb/werge_degen/20240327/degen_registry_lpr_lpr3_diagnoses.tsv" #cpr_enc; source;date_in;date_out;type;diagnosis
            lpr_file = "/data/projects_chb/werge_degen/20250102/degen_registry_lpr_lpr3_diagnoses.tsv" #cpr_enc; source;date_in;date_out;type;diagnosis
            atc_file = "/data/projects_chb/werge_degen/20250102/degen_prescriptions.tsv"
            atc_diag_col = "atc"
            atc_date_col = "eksd"
            atc_cols_to_read_as_date = ['eksd']
            if (test_run):
                lpr_file = "/data/projects_chb/werge_degen/aSchorkLab/data/ExDEP/get_phenotypes_exdep_test_data.tsv"
                pheno_request = "./sample_pheno_request.txt"
        if ("degen_prescriptions.tsv" in lpr_file or "lmdb.tsv" in lpr_file ):
            lpr_cols_to_read_as_date = ['eksd']
            diagnostic_col = "atc"
            pheno_requestcol = "atc"
            input_date_in_name = "eksd"
            input_date_out_name = "eksd"
            atc_cols_to_read_as_date = ['eksd']
            skip_icd_update = True
        stam_cols_to_read_as_date = ['birthdate']
        if (stam_file == ''):
            #stam_file = "/data/projects_chb/werge_degen/20221220/cohort.tsv"
            #stam_file = "/data/projects_chb/werge_degen/20240327/cohort.tsv"
            stam_file = "/data/projects_chb/werge_degen/20250102/cohort.tsv"
            stam_cols_to_read_as_date = ['birthdate']
            addition_information_file = "/data/projects_chb/werge_degen/20250102/t_person.tsv"#,/data/projects_chb/werge_degen/20240327/cohort_sample_dates_20240327"
            
            

    if (test_run and not dbds_run):
        use_predefined_exdep_exclusions = True
        dbds_run = True
        ipsych_run = False
        lpr_file = "./lpr_file.csv"
        stam_file = "./stam_file.csv"
        addition_information_file = "./addition_information_file.csv"
        pheno_request = "./sample_pheno_request.txt"
        iidcol = "cpr_enc"
        diagnostic_col = "diagnosis"
        pheno_requestcol = "diagnosis"
        fsep = ","
        gsep = "\t"
        input_date_in_name = "date_in"
        input_date_out_name = "date_out"
        general_exclusions = ""
        qced_iids = ""

    
    print (hostname+"; CHB/DBDS: "+str(dbds_run)+"; iPSYCH: "+str(ipsych_run)+"; TestRun: "+str(test_run)+"; Cluster: "+cluster_run)
    if (not lpr_cols_to_read_as_date):
        print("INFO: no -f dates supplied, checking predefined colnames.")
        potential_lpr_cols_to_read_as_date = [input_date_in_name,input_date_out_name,birthdatecol,"date_in","date_out","birthdate","date_of_birth"]
    else:
        if not type(lpr_cols_to_read_as_date) == list:
            lpr_cols_to_read_as_date = lpr_cols_to_read_as_date.split(",")
        print("INFO: -f dates supplied, setting them now for later use: ",lpr_cols_to_read_as_date)
    
    if (not stam_cols_to_read_as_date):
        print("INFO: no -i dates supplied, checking predefined colnames.")
        potential_stam_cols_to_read_as_date = [input_date_in_name,input_date_out_name,birthdatecol,"date_in","date_out","birthdate","date_of_birth"]
    else:
        print("INFO: -i dates supplied, setting them now for later use.")
        if not type(stam_cols_to_read_as_date) == list:
            stam_cols_to_read_as_date = stam_cols_to_read_as_date.split(",")

    if (lpr_file == "" or pheno_request == "" or stam_file == ""):
        print("ERROR: Either lpr_file , stam_file or pheno_request file is not given. Exiting")
        sys.exit()


    if (verbose):
        print("Mem before loading all input:")
        usage()


    # Load the pheno request file to do the check thereafter.
    # Check if the phenotype request is based on Codes or based on a already extracted phenotype that fits the specified set of columns.
    if (load_precreated_phenotypes):
        # initialize the not needed dataframes
        df1 = pd.DataFrame() #lpr file
        df3 = pd.DataFrame() #stam file
        df4 = pd.DataFrame() #additional information file
        # Load input files
        casecontrol_df = pd.read_csv(pheno_request, sep=fsep, dtype=object, parse_dates=lpr_cols_to_read_as_date, date_format=DateFormat)
        print("#"*50)
        print("#"*50)
        print("INFO: You are loading pre-created phenotype files to then let us do some manipulation and combining them.")
        print("     This relies on certain data format assumptions:")
        print("     We will use the following columns (you can change this through the adequate flag) iidcol=",iidcol,", diagnostic_col=",diagnostic_col,", pheno_requestcol=",pheno_requestcol,", birthdatecol=",birthdatecol,", sexcol=",sexcol,", input_date_in_name=",input_date_in_name,", input_date_out_name=",input_date_out_name," ")
        print("     Furthermore, when there are the following columns available, we will use them too: ...........")
        print("     The format of your input to --LifetimeExclusion, --PostExclusion, and --OneyPriorExclusion is assumed to be in form of (tab between the two columns; no header)")
        print("DISORDERNAME /path/to/file/for/DISORDERNAME")
        print("#"*50)
        print("#"*50)
        if lifetime_exclusions_file != "":
            if ',' in lifetime_exclusions_file:
                # Split the string by comma
                file_paths = lifetime_exclusions_file.split(',')
                multi_diag_cols = False
                if ',' in diagnostic_col:
                    diag_cols = diagnostic_col.split(',')
                    multi_diag_cols = True
                    diagnostic_col = diag_cols[0]
                    curr=0
                # Now file_paths is a list containing individual file paths
                # Initialize an empty DataFrame to store the concatenated data
                temp_df = pd.DataFrame()
                # Iterate over each file path
                for lifetimefile in file_paths:
                    temp_df = pd.read_csv(lifetimefile, sep=fsep, dtype=object, parse_dates=stam_cols_to_read_as_date, date_format=DateFormat)
                    # update the colnames so that the current disorder/disease will be reflected in the naming as it would be when running it from the start
                    # append the casecontrol_df with the current exclusion

        #use_predefined_exdep_exclusions
        #pheno_request, lifetime_exclusions_file, post_exclusions_file, oneYearPrior_exclusions_file, main_pheno_name, MinMaxAge, DateFormat_in
        minimal_viable_set_of_pheno_cols = [iidcol, diagnostic_col, pheno_requestcol, birthdatecol, sexcol, input_date_in_name, input_date_out_name ]
        # This requires to be a tab separated file with the phenotype file name to be in the first column
        ##  MDD /path/to/data/mdd.pheno.tsv

        # Read the files for pheno and exclusions 
        ## Process them like usually
    else:
        # Load the second file as a DataFrame(should be file with all e.g. ICD codes to extract) 
        with open(pheno_request, 'r') as file:
            in_pheno_codes_lines = file.readlines()
            columns = in_pheno_codes_lines[0].strip().replace(" ", "").split('\t')
            
            if len(columns) == 2:
                multi_inclusions = True
                print("Information: Detected 2 columns in Inclusion file. Inclusions will be ", in_pheno_codes_lines)
                # Remove spaces from each line
                in_pheno_codes = [line.replace(' ', '') for line in in_pheno_codes_lines]

                # Create a dictionary from the cleaned lines
                process_line = lambda line: (line.strip().split('\t')[0], line.strip().split('\t')[1].split(','))
                disorder_dict = dict(map(process_line, in_pheno_codes))

                # Create a DataFrame from the dictionary
                in_pheno_codes = pd.DataFrame(list(disorder_dict.items()), columns=['Disorder', 'Disorder Codes'])

                # Assuming `pheno_requestcol` is the name of the column you want to check
                column_values = in_pheno_codes['Disorder Codes']  # Extract the specific column

                # Check if all or any values in the column start with 'ATC'
                if all(str(value).startswith('ATC') for value in column_values):
                    ATC_Requested = "All"
                elif any(str(value).startswith('ATC') for value in column_values):
                    ATC_Requested = "Some"
                else:
                    ATC_Requested = "None"  # Optional case to handle if none start with 'ATC'
                    
                if verbose:
                    print(in_pheno_codes['Disorder Codes'])

                orig_pheno_request = in_pheno_codes.copy()
                
                for InclusionReason in in_pheno_codes['Disorder']:
                    print("Build IID list regarding ", InclusionReason, " Case inclusion")
                    updated_codes = update_icd_coding(
                        data=disorder_dict.get(InclusionReason), 
                        dbdschb=dbds_run, 
                        ipsych=ipsych_run, 
                        dst=dst,
                        eM=exact_match, 
                        skip=skip_icd_update, 
                        remove_point_in_diag_request=remove_point_in_diag_request,
                        ICDCM=ICDCM,
                        RegisterRun=RegisterRun
                    )
                    
                    print(updated_codes)
                    print(in_pheno_codes.loc[in_pheno_codes['Disorder'] == InclusionReason, 'Disorder Codes'])
                    
                    #in_pheno_codes.loc[in_pheno_codes['Disorder'] == InclusionReason, 'Disorder Codes'] = updated_codes
                    in_pheno_codes.loc[in_pheno_codes['Disorder'] == InclusionReason, 'Disorder Codes'] = [updated_codes]


                    #print("Codes used: ",updated_codes)
                # Clean up
                del(in_pheno_codes_lines, columns, process_line, disorder_dict, updated_codes)
            
            elif len(columns) == 1:
                multi_inclusions = False
                print("Information: You supplied a file with only one column for Inclusions.")
                in_pheno_codes = pd.read_csv(pheno_request, sep=gsep, dtype=str)#, header=0)s
                if verbose:
                    print(in_pheno_codes,",",pheno_requestcol)

                # Assuming `pheno_requestcol` is the name of the column you want to check
                if (type(in_pheno_codes) == list):
                    column_values = in_pheno_codes
                else:
                    if verbose:
                        print(in_pheno_codes.columns[0],pheno_requestcol)
                    if(in_pheno_codes.columns[0] != pheno_requestcol):
                        column_values = in_pheno_codes[in_pheno_codes.columns[0]]
                    else:
                        column_values = in_pheno_codes[pheno_requestcol]  # Extract the specific column

                # Check if all or any values in the column start with 'ATC'
                if all(str(value).startswith('ATC') for value in column_values):
                    ATC_Requested = "All"
                elif any(str(value).startswith('ATC') for value in column_values):
                    ATC_Requested = "Some"
                else:
                    ATC_Requested = "None"  # Optional case to handle if none start with 'ATC'

                orig_pheno_request = in_pheno_codes.copy()
                in_pheno_codes = update_icd_coding(
                        data=in_pheno_codes, 
                        dbdschb=dbds_run, 
                        ipsych=ipsych_run, 
                        dst=dst,
                        eM=exact_match, 
                        skip=skip_icd_update, 
                        remove_point_in_diag_request=remove_point_in_diag_request,
                        ICDCM=ICDCM,
                        RegisterRun=RegisterRun
                    )
                if verbose:
                    print(in_pheno_codes,",",pheno_requestcol)
                #for index, value in in_pheno_codes[pheno_requestcol].items():
                #    try:
                #        # Try converting the value to a numeric type
                #        numeric_value = pd.to_numeric(value)
                #        in_pheno_codes.at[index, pheno_requestcol] = numeric_value
                #    except ValueError:
                #        # If conversion fails, it means the value remains as a string
                #        pass
        

        print("Info: Finished loading -g file")

        if (verbose):
            print("Mem after loading pheno_request input:")
            usage()
        

        # Check if the string has a comma
        if ',' in stam_file:
            # Split the string by comma
            file_paths = stam_file.split(',')
            # Now file_paths is a list containing individual file paths
            # Initialize an empty DataFrame to store the concatenated data
            df3 = pd.DataFrame()
            # Iterate over each file path
            for stamfile in file_paths:
                # Read the CSV file and append it to df3
                if dta_input:
                    df = pd.read_stata(stamfile)
                else:
                    if stam_cols_to_read_as_date:
                        try:
                            df = pd.read_csv(stamfile, sep=fsep, dtype=object, parse_dates=stam_cols_to_read_as_date, date_format=DateFormat)
                        except TypeError:
                            df = pd.read_csv(stamfile, sep=fsep, dtype=object, parse_dates=stam_cols_to_read_as_date)
                            for col in stam_cols_to_read_as_date:
                                df[col] = pd.to_datetime(df[col], format=DateFormat, errors='coerce')
                    else:
                        # Identify the set of columns that are dates
                        try:
                            df = pd.read_csv(stamfile, sep=fsep, dtype=object, nrows=0)
                        except TypeError:
                            df = pd.read_csv(stamfile, sep=fsep, dtype=object, nrows=0, engine='python')
                        if(df.columns.empty):
                            print("ERROR: Could not load -i file: ",stamfile)
                            sys.exit()
                        available_date_cols = list(set([col for col in potential_stam_cols_to_read_as_date if col in df.columns]))
                        print("INFO: No cols supplied (stam) that should be read as dates. Trying to infer them: ",available_date_cols)
                        if available_date_cols:
                            try:
                                df = pd.read_csv(stamfile, sep=fsep, dtype=object, parse_dates=available_date_cols, date_format=DateFormat)
                            except TypeError:
                                df = pd.read_csv(stamfile, sep=fsep, dtype=object, parse_dates=available_date_cols)
                        else:
                            try:
                                df = pd.read_csv(stamfile, sep=fsep, dtype=str)
                            except TypeError:
                                df = pd.read_csv(stamfile, sep=fsep, engine='python', dtype=str)
                df3 = pd.concat([df3, df], ignore_index=True, sort=False)
                del(df)
            del(stamfile)
            del(file_paths)
        else:
            # Load the thrid file as a DataFrame(should be cohort information e.g. with Birthdates and Gender) 
            if dta_input:
                df3 = pd.read_stata(stam_file)
            else:
                if stam_cols_to_read_as_date:
                    try:
                        df3 = pd.read_csv(stam_file, sep=fsep, dtype=object, parse_dates=stam_cols_to_read_as_date, date_format=DateFormat)
                    except TypeError:
                        df3 = pd.read_csv(stam_file, sep=fsep, dtype=object, parse_dates=stam_cols_to_read_as_date)
                else:
                    # Identify the set of columns that are dates
                    try:
                        df3 = pd.read_csv(stam_file, sep=fsep, dtype=object, nrows=0)
                    except TypeError:
                        df3 = pd.read_csv(stam_file, sep=fsep, engine='python', dtype=object, nrows=0)
                    if(df3.columns.empty):
                        print("ERROR: Could not load -i file: ",stam_file)
                        sys.exit()
                    available_date_cols = list(set([col for col in potential_stam_cols_to_read_as_date if col in df3.columns]))
                    print("INFO: No cols supplied (stam) that should be read as dates. Trying to infer them: ",available_date_cols)
                    if available_date_cols:
                        try:
                            df3 = pd.read_csv(stam_file, sep=fsep, dtype=object, parse_dates=available_date_cols, date_format=DateFormat)
                        except TypeError:
                            df3 = pd.read_csv(stam_file, sep=fsep, dtype=object, parse_dates=available_date_cols)
                    else:
                        try:
                            df3 = pd.read_csv(stam_file, sep=fsep, dtype=str)
                        except TypeError:
                            df3 = pd.read_csv(stam_file, sep=fsep, engine='python', dtype=str)
        print("Info: Finished loading -i file(s)")

        # Check if input is plausible (e.g. contains every IID only once)
        if (len(df3) > len(df3[iidcol].unique())):
            print("WARNING: Your input file -i contains duplicated IIDs. If this should not be the case, please check your input!")


        if (birthdatecol in df3.columns and birthdatecol != "birthdate"):
            df3.rename(columns={birthdatecol: "birthdate"},inplace=True)
            if (verbose):
                print("Updating birthdatecol name in df3.")


        if ("birthdate" in df3.columns):
            if(verbose):
                print("Update df3 birthdatecol to Date format.")
            df3['birthdate'] = df3['birthdate']


        if (diagnostic_col in df3.columns and diagnostic_col != "diagnosis"):
            df3.rename(columns={diagnostic_col: "diagnosis"},inplace=True)
            if (verbose):
                print("Updating diagnostic_col name in df3 to \"diagnosis\".")


        if (sexcol != "sex"):
            # Rename sex column 
            df3.rename(columns={sexcol: 'sex'}, inplace=True)


        if (verbose):
            print(df3.columns)
            print("Mem after loading stam_file input:")
            usage()

        if (cluster_run == "iPSYCH" or cluster_run == "IBP_computerome"):
            if (build_ophold):
                try:
                    ophold = pd.read_csv("/faststorage/jail/project/ibp_data_secure/danish_population/2016-01-01_register_raw/ophold2016.csv", sep=opholdsep, dtype=object)
                except TypeError:
                    ophold = pd.read_csv("/faststorage/jail/project/ibp_data_secure/danish_population/2016-01-01_register_raw/ophold2016.csv", engine='python', sep=opholdsep, dtype=object)
                df3 = process_ophold(ophold, df3, "", "/dpibp/shared/aSchorkLab/data/processed_ophold.csv", birthdatecol, iidcol, verbose)
            else:
                if (os.path.isfile("/dpibp/shared/aSchorkLab/data/processed_ophold.csv")):
                    try:
                        ophold = pd.read_csv("/dpibp/shared/aSchorkLab/data/processed_ophold.csv", sep=opholdsep, dtype=object)
                    except TypeError:
                        ophold = pd.read_csv("/dpibp/shared/aSchorkLab/data/processed_ophold.csv", engine='python', sep=opholdsep, dtype=object)
                    df3 = pd.merge(df3,ophold,how="left", on=iidcol)
                
        if (cluster_run == "NCRR_DST"):
            if (build_ophold):
                ophold = pd.read_stata("E://Data/rawdata/703935/Population/ophold2016b.dta", sep=opholdsep, dtype=object)
                df3 = process_ophold(ophold, df3, "", "E://Data/workdata/703935/MisLun/processed_ophold.csv", birthdatecol, iidcol, verbose)
            else:
                if (os.path.isfile("E://Data/workdata/703935/MisLun/processed_ophold.csv")):
                    try:
                        ophold = pd.read_csv("E://Data/workdata/703935/MisLun/processed_ophold.csv", sep=opholdsep, dtype=object)
                        df3 = pd.merge(df3,ophold,how="left", on=iidcol)
                    except: 
                        print("ERROR: No OPHOLD data preprocessed. Please use --BuildOphold flag.")
        n_stam_iids = df3[iidcol].nunique()

        df4 = pd.DataFrame()
        # Load the fourth file as a DataFrame(should be a file with additional rows of information. This will not be merged but appended to df1) 
        if (addition_information_file != ''):
            # Check if the string has a comma
            if ',' in addition_information_file:
                # Split the string by comma
                file_paths = addition_information_file.split(',')
                # Now file_paths is a list containing individual file paths
                # Initialize an empty DataFrame to store the concatenated data
                df4 = pd.DataFrame()
                # Iterate over each file path
                for additionfile in file_paths:
                    if dta_input:
                        df = pd.read_stata(additionfile)# Read the CSV file and append it to df4
                    else:
                        df = pd.read_csv(additionfile, sep=fsep, dtype=str)
                    df4 = pd.concat([df4, df], ignore_index=True, sort=False)
                    del(df)
                del(file_paths)
            else:
                if dta_input:
                        df4 = pd.read_stata(addition_information_file)
                else:
                    df4 = pd.read_csv(addition_information_file, sep=fsep, dtype=str)
            if (birthdatecol in df4.columns and birthdatecol != "birthdate"):
                df4.rename(columns={birthdatecol: "birthdate"},inplace=True)
                if (verbose):
                    print("Updating birthdatecol name in df4.")
            if (diagnostic_col in df4.columns and diagnostic_col != "diagnosis"):
                df4.rename(columns={diagnostic_col: "diagnosis"},inplace=True)
                if (verbose):
                    print("Updating diagnostic_col name in df4.")
            if(("birthdate" in df3.columns) and "birthdate" in df4.columns):# or "birthdate" in df1.columns) and "birthdate" in df4.columns):
                df4.drop("birthdate",inplace=True, axis=1)
            if(("diagnosis" in df3.columns) and "diagnosis" in df4.columns):# or "diagnosis" in df1.columns) and "diagnosis" in df4.columns):
                df4.drop("diagnosis",inplace=True, axis=1)
            print("Info: Finished loading -j file(s)")

        if (use_predefined_exdep_exclusions):
            min_Age = 18
            max_Age = 50
            # Intellectual Disability / Mental retardation
            id_diagnostics = update_icd_coding(dst=dst, RegisterRun=RegisterRun, data=ID_Codes, dbdschb=dbds_run, ipsych=ipsych_run, eM=exact_match, skip=skip_icd_update, remove_point_in_diag_request=remove_point_in_diag_request, ICDCM=ICDCM)
            # Schizophrenia
            scz_diagnostics = update_icd_coding(dst=dst, RegisterRun=RegisterRun, data=SCZ_Codes, dbdschb=dbds_run, ipsych=ipsych_run, eM=exact_match, skip=skip_icd_update, remove_point_in_diag_request=remove_point_in_diag_request, ICDCM=ICDCM)
            # Bipolar Disorder
            bpd_diagnostics = update_icd_coding(dst=dst, RegisterRun=RegisterRun, data=BPD_Codes, dbdschb=dbds_run, ipsych=ipsych_run, eM=exact_match, skip=skip_icd_update, remove_point_in_diag_request=remove_point_in_diag_request, ICDCM=ICDCM)
            # Dementia
            dem_diagnostics = update_icd_coding(dst=dst, RegisterRun=RegisterRun, data=DEM_Codes, dbdschb=dbds_run, ipsych=ipsych_run, eM=exact_match, skip=skip_icd_update, remove_point_in_diag_request=remove_point_in_diag_request, ICDCM=ICDCM)
            # Alcohol use Disorder
            aud_diagnostics = update_icd_coding(dst=dst, RegisterRun=RegisterRun, data=AUD_Codes, dbdschb=dbds_run, ipsych=ipsych_run, eM=exact_match, skip=skip_icd_update, remove_point_in_diag_request=remove_point_in_diag_request, ICDCM=ICDCM)
            # Drug use Disorder
            dud_diagnostics = update_icd_coding(dst=dst, RegisterRun=RegisterRun, data=DUD_Codes, dbdschb=dbds_run, ipsych=ipsych_run, eM=exact_match, skip=skip_icd_update, remove_point_in_diag_request=remove_point_in_diag_request, ICDCM=ICDCM)
            # Mild cognitive Impairment (MCI)
            mci_diagnostics = update_icd_coding(dst=dst, RegisterRun=RegisterRun, data=MCI_Codes, dbdschb=dbds_run, ipsych=ipsych_run, eM=exact_match, skip=skip_icd_update, remove_point_in_diag_request=remove_point_in_diag_request, ICDCM=ICDCM)
            # Concurrent terminal illness
            cti_diagnostics = update_icd_coding(dst=dst, RegisterRun=RegisterRun, data=CTI_Codes, dbdschb=dbds_run, ipsych=ipsych_run, eM=exact_match, skip=skip_icd_update, remove_point_in_diag_request=remove_point_in_diag_request, ICDCM=ICDCM)
            #Chronic Pain
            pain_diagnostics = update_icd_coding(dst=dst, RegisterRun=RegisterRun, data=CP_Codes, dbdschb=dbds_run, ipsych=ipsych_run, eM=exact_match, skip=skip_icd_update, remove_point_in_diag_request=remove_point_in_diag_request, ICDCM=ICDCM) 
            gad_diagnostics = update_icd_coding(dst=dst, RegisterRun=RegisterRun, data=GAD_Codes, dbdschb=dbds_run, ipsych=ipsych_run, eM=exact_match, skip=skip_icd_update, remove_point_in_diag_request=remove_point_in_diag_request, ICDCM=ICDCM) 
            pd_diagnostics = update_icd_coding(dst=dst, RegisterRun=RegisterRun, data=PD_Codes, dbdschb=dbds_run, ipsych=ipsych_run, eM=exact_match, skip=skip_icd_update, remove_point_in_diag_request=remove_point_in_diag_request, ICDCM=ICDCM) 
            phobias_diagnostics = update_icd_coding(dst=dst, RegisterRun=RegisterRun, data=Phobias_Codes, dbdschb=dbds_run, ipsych=ipsych_run, eM=exact_match, skip=skip_icd_update, remove_point_in_diag_request=remove_point_in_diag_request, ICDCM=ICDCM) 
            anx_diagnostics = update_icd_coding(dst=dst, RegisterRun=RegisterRun, data=ANX_Codes, dbdschb=dbds_run, ipsych=ipsych_run, eM=exact_match, skip=skip_icd_update, remove_point_in_diag_request=remove_point_in_diag_request, ICDCM=ICDCM) 
            ptsd_diagnostics = update_icd_coding(dst=dst, RegisterRun=RegisterRun, data=PTSD_Codes, dbdschb=dbds_run, ipsych=ipsych_run, eM=exact_match, skip=skip_icd_update, remove_point_in_diag_request=remove_point_in_diag_request, ICDCM=ICDCM) 
            ocd_diagnostics = update_icd_coding(dst=dst, RegisterRun=RegisterRun, data=OCD_Codes, dbdschb=dbds_run, ipsych=ipsych_run, eM=exact_match, skip=skip_icd_update, remove_point_in_diag_request=remove_point_in_diag_request, ICDCM=ICDCM) 
            adhd_diagnostics = update_icd_coding(dst=dst, RegisterRun=RegisterRun, data=ADHD_Codes, dbdschb=dbds_run, ipsych=ipsych_run, eM=exact_match, skip=skip_icd_update, remove_point_in_diag_request=remove_point_in_diag_request, ICDCM=ICDCM) 
            asd_diagnostics = update_icd_coding(dst=dst, RegisterRun=RegisterRun, data=ASD_Codes, dbdschb=dbds_run, ipsych=ipsych_run, eM=exact_match, skip=skip_icd_update, remove_point_in_diag_request=remove_point_in_diag_request, ICDCM=ICDCM) 
            bul_diagnostics = update_icd_coding(dst=dst, RegisterRun=RegisterRun, data=BUL_Codes, dbdschb=dbds_run, ipsych=ipsych_run, eM=exact_match, skip=skip_icd_update, remove_point_in_diag_request=remove_point_in_diag_request, ICDCM=ICDCM) 
            ano_diagnostics = update_icd_coding(dst=dst, RegisterRun=RegisterRun, data=ANO_Codes, dbdschb=dbds_run, ipsych=ipsych_run, eM=exact_match, skip=skip_icd_update, remove_point_in_diag_request=remove_point_in_diag_request, ICDCM=ICDCM) 
            sleepdisorder_diagnostics = update_icd_coding(dst=dst, RegisterRun=RegisterRun, data=Sleep_Disorder_Codes, dbdschb=dbds_run, ipsych=ipsych_run, eM=exact_match, skip=skip_icd_update, remove_point_in_diag_request=remove_point_in_diag_request, ICDCM=ICDCM) 
            AD_diagnostics = update_icd_coding(dst=dst, RegisterRun=RegisterRun, data=AD_Codes, dbdschb=dbds_run, ipsych=ipsych_run, eM=exact_match, skip=skip_icd_update, remove_point_in_diag_request=remove_point_in_diag_request, ICDCM=ICDCM) 
            Pain_diagnostics = update_icd_coding(dst=dst, RegisterRun=RegisterRun, data=Pain_Codes, dbdschb=dbds_run, ipsych=ipsych_run, eM=exact_match, skip=skip_icd_update, remove_point_in_diag_request=remove_point_in_diag_request, ICDCM=ICDCM) 
            Chronic_diagnostics = update_icd_coding(dst=dst, RegisterRun=RegisterRun, data=Chronic_Codes, dbdschb=dbds_run, ipsych=ipsych_run, eM=False, skip=skip_icd_update, remove_point_in_diag_request=remove_point_in_diag_request, ICDCM=ICDCM) 
            Other_Mental_diagnostics = update_icd_coding(dst=dst, RegisterRun=RegisterRun, data=Other_Mental_Codes, dbdschb=dbds_run, ipsych=ipsych_run, eM=False, skip=skip_icd_update, remove_point_in_diag_request=remove_point_in_diag_request, ICDCM=ICDCM) 
        
        if not lowMem:
            if ',' in lpr_file:
                # Split the string by comma
                file_paths = lpr_file.split(',')
                multi_diag_cols = False
                if ',' in diagnostic_col:
                    diag_cols = diagnostic_col.split(',')
                    multi_diag_cols = True
                    diagnostic_col = diag_cols[0]
                    curr=0
                # Now file_paths is a list containing individual file paths
                # Initialize an empty DataFrame to store the concatenated data
                df1 = pd.DataFrame()
                # Iterate over each file path
                for lprfile in file_paths:
                    # Read the CSV file and append it to df1
                    if dta_input:
                        df = pd.read_stata(lprfile)
                    else:
                        if lpr_cols_to_read_as_date:
                            try:
                                df = pd.read_csv(lprfile, sep=fsep, dtype=object, engine='python', parse_dates=lpr_cols_to_read_as_date, date_format=DateFormat)
                                for col in lpr_cols_to_read_as_date:
                                    df[col] = pd.to_datetime(df[col], format=DateFormat, errors='coerce')
                            except TypeError as error:
                                print("ERROR: While reading lpr file with added date_format (",str(DateFormat),") information: ",error)
                                df = pd.read_csv(lprfile, sep=fsep, dtype=object, parse_dates=lpr_cols_to_read_as_date) 
                                for col in lpr_cols_to_read_as_date:
                                    df[col] = pd.to_datetime(df[col], format=DateFormat, errors='coerce')
                        else:    
                            # Identify the set of columns that are dates
                            df = pd.read_csv(lprfile, sep=fsep, dtype=object, nrows=0)
                            if(df.columns.empty):
                                print("ERROR: Could not load -f file: ",lprfile)
                                sys.exit()
                            available_date_cols = list(set([col for col in potential_lpr_cols_to_read_as_date if col in df.columns]))
                            print("INFO: No cols supplied (lpr) that should be read as dates. Trying to infer them: ",available_date_cols)
                            if available_date_cols:
                                try:
                                    df = pd.read_csv(lprfile, sep=fsep, dtype=object, parse_dates=available_date_cols, date_format=DateFormat)
                                except TypeError:
                                    df = pd.read_csv(lprfile, sep=fsep, dtype=object, parse_dates=available_date_cols)
                                    for col in lpr_cols_to_read_as_date:
                                        df[col] = pd.to_datetime(df[col], format=DateFormat, errors='coerce')
                            else:
                                df = pd.read_csv(lprfile, sep=fsep, dtype=str)
                    if multi_diag_cols:
                        df.rename(columns={diag_cols[curr]: diagnostic_col},inplace=True)
                        curr += 1
                    df1 = pd.concat([df1, df], ignore_index=True, sort=False)
                    del(df)
                if lpr2nd_file != "":
                    print("INFO: As you are also loading the files containing the secondary diagnoses, we will remove all rows in the standard LPR that refer to DZ03* or DZ763 as these may lead to issues. In addition, we will also remove all \'H\' diag_types.")
                    # Remove rows where diagnostic_col refers to accompanying persons. Recommended by Dorte Helenius on 10.01.2025
                    df1 = df1[~df1[diagnostic_col].str.startswith(('DZ03', 'DZ763'), na=False)]
                    file_paths = lpr2nd_file.split(',')
                    for lprfile in file_paths:
                        # Read the CSV file and append it to df1
                        if dta_input:
                            df = pd.read_stata(lprfile)
                        else:
                            df = pd.read_csv(lprfile, sep=fsep, dtype=str)
                        # Remove rows where 'diag_type' column has 'H'
                        df = df[df['c_diagtype'] != 'H']
                        if lpr_recnummer != lpr2nd_recnummer:
                            df.rename(columns={lpr2nd_recnummer: lpr_recnummer},inplace=True)
                        if diagnostic_col != diagnostic2nd_col:
                            df.rename(columns={diagnostic2nd_col: diagnostic_col},inplace=True)
                        df1_temp = pd.merge(
                            df[[diagnostic_col, lpr_recnummer]], 
                            df1.drop(columns=[diagnostic_col], errors='ignore'), 
                            on=[lpr_recnummer], 
                            how='inner'  # Use 'inner', 'outer', 'left', or 'right' as needed
                        )
                        before_rows = len(df1_temp)
                        df1_temp.drop_duplicates(inplace=True)
                        print(f"After merging df with filtered_chunk, we have {before_rows} entries. {before_rows - len(df1_temp)} entries were deleted due to duplication. {df1_temp.head(5)}.\n{df1_temp.describe(include='all')}")
                        del(df)
                        before_rows = len(df1)
                        df1 = pd.concat([df1, df1_temp], ignore_index=True, sort=False)
                        print(f"Finally, the adding of secondary diagnoses resulted in {len(df1)} ({before_rows}+{len(df1_temp)}) entries to be used to determine the CaseControl status.")
                        del(df1_temp)
                del(lprfile)
                del(file_paths)
            else:
                # Load the first file as a DataFrame(should be phenotype file) 
                if dta_input:
                    print("WARNING: .DTA files are not yet supported to be read in with --lowmem flag. Reading in the whole file at once.")
                    df1 = pd.read_stata(lpr_file)
                else:
                    if lpr_cols_to_read_as_date:
                        try:
                            df1 = pd.read_csv(lpr_file, sep=fsep, engine='python', dtype=object, parse_dates=lpr_cols_to_read_as_date, date_format=DateFormat)
                        except TypeError:
                            df1 = pd.read_csv(lpr_file, sep=fsep, engine='python', dtype=object, parse_dates=lpr_cols_to_read_as_date)
                            for col in lpr_cols_to_read_as_date:
                                df1[col] = pd.to_datetime(df1[col], format=DateFormat, errors='coerce')
                    else: 
                        # Identify the set of columns that are dates
                        df1 = pd.read_csv(lpr_file, sep=fsep, engine='python', dtype=object, nrows=0)
                        if (df1.columns.empty):
                            print("ERROR: Could not load -f file: ",lpr_file)
                            sys.exit()
                        available_date_cols = list(set([col for col in potential_lpr_cols_to_read_as_date if col in df1.columns]))
                        print("INFO: No cols supplied (lpr) that should be read as dates. Trying to infer them: ",available_date_cols)
                        if available_date_cols:
                            try:
                                df1 = pd.read_csv(lpr_file, sep=fsep, engine='python', dtype=object, parse_dates=lpr_cols_to_read_as_date, date_format=DateFormat)
                            except TypeError:
                                df1 = pd.read_csv(lpr_file, sep=fsep, engine='python', dtype=object, parse_dates=available_date_cols)
                                for col in lpr_cols_to_read_as_date:
                                    df1[col] = pd.to_datetime(df1[col], format=DateFormat, errors='coerce')
                        else:
                            df1 = pd.read_csv(lpr_file, sep=fsep, engine='python', dtype=str)
                        if lpr2nd_file != "":
                            print("INFO: As you are also loading the files containing the secondary diagnoses, we will remove all rows in the standard LPR that refer to DZ03* or DZ763 as these may lead to issues. In addition, we will also remove all \'H\' diag_types.")
                            # Remove rows where diagnostic_col refers to accompanying persons. Recommended by Dorte Helenius on 10.01.2025
                            df1 = df1[~df1[diagnostic_col].str.startswith(('DZ03', 'DZ763'), na=False)]
                            # Read the CSV file and append it to df1
                            if dta_input:
                                df = pd.read_stata(lpr2nd_file)
                            else:
                                df = pd.read_csv(lpr2nd_file, sep=fsep, dtype=str)
                            # Remove rows where 'diag_type' column has 'H'
                            df = df[df['c_diagtype'] != 'H']
                            if lpr_recnummer != lpr2nd_recnummer:
                                df.rename(columns={lpr2nd_recnummer: lpr_recnummer},inplace=True)
                            if diagnostic_col != diagnostic2nd_col:
                                df.rename(columns={diagnostic2nd_col: diagnostic_col},inplace=True)
                            df1_temp = pd.merge(
                                df[[diagnostic_col, lpr_recnummer]], 
                                df1.drop(columns=[diagnostic_col], errors='ignore'),
                                on=[lpr_recnummer], 
                                how='inner'  # Use 'inner', 'outer', 'left', or 'right' as needed
                            )
                            print(f"After merging df with filtered_chunk, we have {before_rows} entries. {before_rows - len(df1_temp)} entries were deleted due to duplication. {df1_temp.head(5)}.\n{df1_temp.describe(include='all')}")
                            df1_temp.drop_duplicates(inplace=True)
                            del(df)
                            df1 = pd.concat([df1, df1_temp], ignore_index=True, sort=False)
                            print(f"Finally, batch {batch_num + 1} resulted in {len(df)} ({before_rows}+{len(df1_temp)}) entries to be used to determine the CaseControl status.")
                            del(df1_temp)
            if(exact_match):
                print("Info: Updating the diagnostic codes to be all Uppercase to be able to run --eM")
                for index, value in df1[diagnostic_col].items():
                    if isinstance(value, str):
                        df1.at[index, diagnostic_col] = value.upper()
            print("Info: Finished loading -f file(s)")

            if (birthdatecol in df1.columns and birthdatecol != "birthdate"):
                df1.rename(columns={birthdatecol: "birthdate"},inplace=True)
                if (verbose):
                    print("Updating birthdatecol name in df1.")

            if (diagnostic_col in df1.columns and diagnostic_col != "diagnosis"):
                df1.rename(columns={diagnostic_col: "diagnosis"},inplace=True)
                if (verbose):
                    print("Updating diagnostic_col name in df1 to \"diagnosis\".")

            if (verbose):
                print(df1.columns)
                print("Mem after loading lpr_file input:")
                usage()

            # Check if input is plausible (e.g. contains every IID only once)
            if (len(df1) > len(df1[iidcol].unique())):
                print("WARNING: Your input file -f contains duplicated IIDs. This basically means, that each IID is given multiple times, potentially due to multiple diagnostic codes (then it is normal). If this should not be the case, please check your input!")
            
            print("Pheno request: ",in_pheno_codes)
            print("Original Pheno request: ",orig_pheno_request)
            process_pheno_and_exclusions(MatchFI=MatchFI, df1=df1, df3=df3, df4=df4, iidcol=iidcol, verbose=verbose, ctype_excl=ctype_excl, ctype_incl=ctype_incl, Filter_YoB=Filter_YoB, Filter_Gender=Filter_Gender, use_predefined_exdep_exclusions=use_predefined_exdep_exclusions, RegisterRun=RegisterRun, dst=dst, ipsych_run=ipsych_run, dbds_run=dbds_run, cluster_run=cluster_run, exact_match=exact_match, skip_icd_update=skip_icd_update, remove_point_in_diag_request=remove_point_in_diag_request, ICDCM=ICDCM, qced_iids=qced_iids, general_exclusions=general_exclusions, multi_inclusions=multi_inclusions, in_pheno_codes=in_pheno_codes, pheno_requestcol=pheno_requestcol, diagnostic_col=diagnostic_col, atc_diag_col=atc_diag_col, birthdatecol=birthdatecol, atc_date_col=atc_date_col, atc_cols_to_read_as_date=atc_cols_to_read_as_date, atc_file=atc_file, fsep=fsep, BuildEntryExitDates=BuildEntryExitDates, lifetime_exclusions_file=lifetime_exclusions_file, post_exclusions_file=post_exclusions_file, oneYearPrior_exclusions_file=oneYearPrior_exclusions_file, outfile=outfile, write_Plink2_format=write_Plink2_format, write_fastGWA_format=write_fastGWA_format, write_pickle=write_pickle, n_stam_iids=n_stam_iids, exclCHBcontrols=exclCHBcontrols, iidstatus_col=iidstatus_col, addition_information_file=addition_information_file, sexcol=sexcol, input_date_in_name=input_date_in_name, input_date_out_name=input_date_out_name, append=False)

        else:
            #TODO if lowMem.... 
            ## Here, it should loop over all individuals given in IIDs (below) by bins of  100.000 individuals at once. 
            ## This basically leads to the problem, that df1 needs to be loaded in parts, so only the current 100.000 iids. 
            ## These 100.000 iids will then need to be processed as if there were all iids loaded at once. This leads to the issue, that the method is right now assuming to have all individuals loaded at once, meaning it also filters for thos that are in df3 (stam file). 
            ## But we dont want to have to reload the stam file with every itteration as this file is not big and won't take up much memory.
            print("Running in low memory mode. Processing df1 in batches.")
            
            
            batch_load_df1_process_pheno_and_exclusions(lpr_file=lpr_file, df4=df4, batch_size=batchsize, diagnostic_col=diagnostic_col, lpr_cols_to_read_as_date=lpr_cols_to_read_as_date, potential_lpr_cols_to_read_as_date=potential_lpr_cols_to_read_as_date, dta_input=dta_input, MatchFI=MatchFI, df3=df3, iidcol=iidcol, verbose=verbose, ctype_excl=ctype_excl, ctype_incl=ctype_incl, Filter_YoB=Filter_YoB, Filter_Gender=Filter_Gender, use_predefined_exdep_exclusions=use_predefined_exdep_exclusions, RegisterRun=RegisterRun, dst=dst, ipsych_run=ipsych_run, dbds_run=dbds_run, cluster_run=cluster_run, exact_match=exact_match, skip_icd_update=skip_icd_update, remove_point_in_diag_request=remove_point_in_diag_request, ICDCM=ICDCM, qced_iids=qced_iids, general_exclusions=general_exclusions, multi_inclusions=multi_inclusions, in_pheno_codes=in_pheno_codes, pheno_requestcol=pheno_requestcol, atc_diag_col=atc_diag_col, birthdatecol=birthdatecol, atc_date_col=atc_date_col, atc_cols_to_read_as_date=atc_cols_to_read_as_date, atc_file=atc_file, fsep=fsep, BuildEntryExitDates=BuildEntryExitDates, lifetime_exclusions_file=lifetime_exclusions_file, post_exclusions_file=post_exclusions_file, oneYearPrior_exclusions_file=oneYearPrior_exclusions_file, outfile=outfile, write_Plink2_format=write_Plink2_format, write_fastGWA_format=write_fastGWA_format, write_pickle=write_pickle, n_stam_iids=n_stam_iids, exclCHBcontrols=exclCHBcontrols, iidstatus_col=iidstatus_col, addition_information_file=addition_information_file, sexcol=sexcol, input_date_in_name=input_date_in_name, input_date_out_name=input_date_out_name)

        print(diclaimer_text)
        print(f"Your Arguments used to start this program:\n{argstring}")
        




if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='Extracts a Phenotype from input files based on IIDs and diagnostic codes.\nThe best way to start, is to generate a test dataset: \'python get_phenotype.py -g \"\" -o ./ --BuildTestSet\' and then run \'python get_phenotype.py -g \"\" -o testrun.tsv --eM --ExDepExc --testRun\' ')
    parser.add_argument('-g', required=True, help='File with all Diagnostic codes to export')
    parser.add_argument('-o', required=True, help='Outfile name; dont forget to add the location, otherwise it will be saved in the local dir.') 
    parser.add_argument('-f', required=False, default='', help='Diagnosis file. Should have at least \"IID\", \"date_in\", \"date_out\", and \"diagnosis\" as columns. Names can be defined using --iidcol, --din, --don, and --fcol. No default. Will be automatically determined if not entered for GenomeDK and CHB/DBDS (DEGEN protocol)') 
    parser.add_argument('--f2', required=False, default='', help='Secondary diagnosis file. This is meant to be used if you have files with secondary diagnosis information as i.e. in Denmark. Here we would usually use the \'recnum\' to merge these entries with \'-f\'. These files also contain less information as the \'-f\' files. Should have at least \"recnum\", and \"diagnosis\" as columns. Names can be defined using --recnum2 and --f2col. The recnum column in the \'-f\' files can be specified using --recnum. Default: "%(default)s"') 
    parser.add_argument('--atc', required=False, default='', help='PRescription information file. Will be handeld simillar to -f but needs additional information to be specified: --atccol. Default: "%(default)s"') 
    parser.add_argument('-i', required=False, default='', help='This file should adds based on a mapping using the supplied \"IID\" column information about Gender/Sex (needed), Brithdate (needed) ... e.g., information from stamdata file. No default. Will be automatically determined if not entered for GenomeDK and CHB/DBDS (DEGEN protocol)')
    parser.add_argument('-j', required=False, default='', help='This file should have additional \"IID\" information e.g. date of entry to cohort. This is not needed and can also be skipped. No default. Will be automatically determined if not entered for GenomeDK and CHB/DBDS (DEGEN protocol)') 
    parser.add_argument('--ge', required=False, default='', help='General exclusion list. This referrs to a list of IIDs that should be excluded from the study. Within CHB/DBDS it will default to /data/preprocessed/genetics/chb_degen_freeze_20210503/degen_exclusion_latest') 
    parser.add_argument('--qced',required=False, default='', help='List with all IIDs that pass initial QC. Default: "%(default)s" (or \'/data/preprocessed/genetics/chb_degen_freeze_20210503/DEGEN_GSA_FINAL.fam\' in CHB/DBDS)')
    parser.add_argument('--name', required=False, default='MainPheno', help='Define your main Phenotype name. Defaults to "%(default)s"')
    parser.add_argument('--fcol', required=False, default="c_adiag", help='Columname of -f and -i file to be mapped. Defaulting to "%(default)s" (or diagnosis in CHB/DBDS)')
    parser.add_argument('--gcol', required=False, default="c_adiag", help='Columname of -g file to be mapped against. Defaulting to "%(default)s" (or diagnosis in CHB/DBDS)') 
    parser.add_argument('--iidcol', required=False, default="pnr", help='Columname of IDs in -f and -i file. Defaulting to "%(default)s" (or cpr_enc in CHB/DBDS)')
    parser.add_argument('--bdcol', required=False, default="birthdate", help='Columname of Birthdate in files. Defaults to "%(default)s"')
    parser.add_argument('--sexcol', required=False, default="sex", help='Columname of Sex/Gender in files. Defaults to "%(default)s"')
    parser.add_argument('--atccol', required=False, default="", help='Columname of ATC codes in files. Defaults to "%(default)s"')
    parser.add_argument('--fsep', required=False, default=",", help='Separator of -f i.e. tab; default "%(default)s" (or "\\t\" in CHB/DBDS)')
    parser.add_argument('--gsep', required=False, default=",", help='Separator of -g i.e. tab; default "%(default)s" (or "\\t\" in CHB/DBDS)')
    parser.add_argument('--din',required=False, default='d_inddto', help='Columname of first diagnosis date. e.g. \'d_inddto\' or \'date_in\'. Default: "%(default)s" (or \'date_in\' in CHB/DBDS)')
    parser.add_argument('--don',required=False, default='d_uddto', help='Columname of first diagnosis date. e.g. \'d_uddto\' or \'date_out\'. Default: "%(default)s" (or \'date_out\' in CHB/DBDS)')
    parser.add_argument('--recnum',required=False, default='', help='Columname of the recnum field in -f files. Default: "%(default)s" (or \'recnum\' on NCRR)')
    parser.add_argument('--recnum2',required=False, default='', help='Columname of the recnum field in -f files. Default: "%(default)s" (or \'recnum\' on NCRR)')
    parser.add_argument('--f2col',required=False, default='c_adiag', help='Columname of first diagnosis date. e.g. \'d_uddto\' or \'date_out\'. Default: "%(default)s" (or \'date_out\' in CHB/DBDS)')
    parser.add_argument('--ExDepExc', action='store_true' , help='List of diagnostic codes to exclude (e.g. Cases holding also these codes will be excluded). This is currently precoded and will be changed to allowing to specify a file with codes.') 
    parser.add_argument('--eM', action='store_true', help='Exact Match. If the ICD or diagnosis code should max exactly and not e.g. searching for ICD:F33 and receiving ICD:F331, ICD:F33, ICD:F339 and so on.')
    parser.add_argument('--noLeadingICD', action='store_true', help='Set this, if your diagnostic codes in \"-g\" do have a leading ICD*:, but your \"-f\" does not. This will only take affect together with --ExDepExc.')
    parser.add_argument('--ICDCM', action='store_true', help='Set this, if your Cohort is based on ICD10/9-CM codes and not base WHO. This will only take affect together with --ExDepExc.')
    parser.add_argument('--iidstatus', default='', help='Information about the status of the IID, i.e. if they withdrew their consent (), moved outside the country (), or died (). Default: "%(default)s"'),
    parser.add_argument('--DiagTypeExclusions', default='', help='List of diagnostic types to exclude. e.g. \"H,M\". Potential c_types may be +,A,B,C,G,M,H. Default: "%(default)s"'),
    parser.add_argument('--DiagTypeInclusions', default='', help='List of diagnostic types to include. e.g. \"A,B\". Potential c_types may be +,A,B,C,G,M,H. Default: "%(default)s"'),
    parser.add_argument('--LifetimeExclusion', default='', help='Define Lifetime exclusions (if a case has any of the listed codes, it will be excluded). This should be a file containing either one row with all codes listed (comma separated) or per row a exclusion name, followed by a list of diagnostic codes (similar to the input). e.g. "BPD\tICD10:F30,ICD10:F30.0,ICD10:F30.1,ICD10:F30.2,ICD10:F30.8,ICD10:F30.9,ICD10:F31,ICD10:F31.0,ICD10:F31.1,ICD10:F31.2,ICD10:F31.3,ICD10:F31.4,ICD10:F31.5,ICD10:F31.6,ICD10:F31.7,ICD10:F31.8,ICD10:F31.9"\nIf you are using CHB/DBDS data, please remember, that the ICD10 codes have to start with ICD10:D***, e.g. ICD10:DF33.0. Default: "%(default)s"'),
    parser.add_argument('--PostExclusion', default='', help='Define Post exclusions (if a case has any of the listed codes, all main diagnoses after the first occuring date of the listed codes will be excluded). This should be a file containing either one row with all codes listed (comma separated) or  per row a exclusion name, followed by a list of diagnostic codes (similar to the input). Default: "%(default)s"'),
    parser.add_argument('--OneyPriorExclusion', default='', help='Define One Year Prio exclusions (all entries for a case that happen within one year prior to any date of the listed codes, these entries will be excluded). This should be a file containing either one row with all codes listed (comma separated) or  per row a exclusion name, followed by a list of diagnostic codes (similar to the input). Default: "%(default)s"'),
    parser.add_argument('--fDates', default=[], help='Which columns inf your -f file are Dates? This automatically uses the columns you supplied under --bdcol, --don and --din. You only need to specify this, if you have additonal Dates that should be reformatted. Specify as comma separated list without spaces. Default: "%(default)s"'),
    parser.add_argument('--iDates', default=[], help='Which columns inf your -i file are Dates? This automatically uses the columns you supplied under --bdcol, --don and --din. You only need to specify this, if you have additonal Dates that should be reformatted. Specify as comma separated list without spaces. Default: "%(default)s"'),
    parser.add_argument('--DateFormat',required=False, default="%d/%m/%Y", help='Format of Dates in your data. Default "%(default)s" --> 31/01/1980')
    parser.add_argument('--MinMaxAge', default='0,0', help='Filter by Min and Max Age at first diagnosis. This should be given as comma separated numerics in the form of x,y; i.e. 18,50 or 17.99,50.01, 0,0 (no exclusion) and is interpreted as inclusion as case if first diagnosis age >x and <y. Default: "%(default)s"'),
    parser.add_argument('--Fyob', default='', help='Filter by Year of Birth. Everyone before this date will be excluded. Date needs to be given in the following format: \"YYYY-MM-DD\". Default: "%(default)s"'),
    parser.add_argument('--Fgender', default='', help='Filter by gender. Only Individuals with the selected Gender (\"F\" or \"M\") will be included in the output. Default: "%(default)s"'),
    parser.add_argument('--eCc', action='store_true', help='Exclude Individuals that are identified as Controls and are only part of CHB and not DBDS. This flag only works when ran on CHB/DBDS cluster.'),
    parser.add_argument('--removePointInDiagCode', action='store_true', help='If your supplied diagnostic codes contain a point but your -f file does not')
    parser.add_argument('--skipICDUpdate', action='store_true', help='If your supplied diagnostic codes are already in line with your datastructure, you can use this toggle to skip the updating (highly recommended if your input is in a correct format!)')
    parser.add_argument('--MatchFI', action='store_true', help='If you want to keep only the IIDs overlapping between -g and -f use this flag.')
    parser.add_argument('--BuildEntryExitDates',action='store_true', help='If you want to extract additional information about the controls. This basically adds first and last date of observation (any diagnostic code) and writes it to first in date, last out date, in_dates, and out_dates. NB: This can be used if you don\'t have an Entry and Exit date known. Be aware, this will increase time needed for calculation.')
    parser.add_argument('--BuildOphold', action='store_true', help='If you want to update the ophold file used (on IBP Cluster only).')
    parser.add_argument('--RegisterRun', action='store_true', help='We will try to determine this automatically based on known Servers. If you are using this method on an unknown Server and your diagnostic codes are in the follwing format DYXXxx; where Y stands for the letter of the group and XX for the main and xx for the subcode.')
    parser.add_argument('--lpp', action='store_true', help='Set this if you want to load phenotypes (one phenotype per file) and run our exclusions on them. Keep in mind, that -g will only allow one file to be supplied. All others will need to be supplied through the appropriate exclusion flags.: "%(default)s" (or \'date_out\' in CHB/DBDS)')
    parser.add_argument('--write_pickle', action='store_true', help='If you want to write the results to a pickle format in addition to the standard output.')
    parser.add_argument('--write_fastGWA_format', action='store_true', help='If you want to write the phenotype into fastGWA format.')
    parser.add_argument('--write_Plink2_format', action='store_true', help='If you want to write the phenotype into PLINK2 format.')
    parser.add_argument('--BuildTestSet', action='store_true', help='Build the test set that can be used to see an example of the input data or to test if your setup runs smoothly'),
    parser.add_argument('--testRun', action='store_true', help='Run only on smaller test data input (only available for the test dataset)'),
    parser.add_argument('--nthreads', default=8, help='DEPRECATED! - How many threads should be used. Default: "%(default)s"'),
    parser.add_argument('--lowmem', action='store_true', help='Experimental! - This will devide the LPR file into groups of each 100.000 individuals, run the phenotype on them, save it to file and then run the nex 100k until the end. This will increase the runtime.'),
    parser.add_argument('--batchsize', required=False, default=100000, help='Experimental! - This will set the batches (when --lowmem is set) to the desired value. Default: "%(default)s"'),
    parser.add_argument('--verbose', action='store_true', help='Verbose output')

    args = parser.parse_args()

    argstring = ""
    #for arg, value in vars(args).items():
    #    default_value = parser.get_default(arg)  # Get the default value
    for action in parser._actions:
        if action.dest == 'help':  # Skip the help argument
            continue
        value = getattr(args, action.dest)  # Get the current value
        default_value = action.default  # Get the default value    
        if value != default_value:
            arg_name = action.option_strings[0]  # Use the first option string (e.g., --arg1)
            argstring = argstring+(f"{arg_name}: {value} (default: {default_value})\n")

    main(args.f,args.g,args.i,args.j,args.ExDepExc,args.ge,args.fcol,args.gcol,args.iidcol,args.bdcol,args.sexcol,args.fsep,args.gsep,args.o,args.eM,args.din,args.don,args.qced,args.DiagTypeExclusions,args.DiagTypeInclusions,args.LifetimeExclusion,args.PostExclusion,args.OneyPriorExclusion,args.eCc,args.Fyob,args.Fgender,args.verbose,args.BuildTestSet,args.testRun,args.MatchFI,args.skipICDUpdate,args.DateFormat,args.iidstatus,args.removePointInDiagCode,args.nthreads,args.name,args.BuildEntryExitDates,args.BuildOphold,args.write_pickle, args.write_fastGWA_format, args.write_Plink2_format,args.fDates,args.iDates,args.MinMaxAge,args.ICDCM,args.lpp, args.RegisterRun, args.lowmem, args.batchsize, args.noLeadingICD, args.f2, args.recnum, args.recnum2, args.f2col, args.atc, args.atccol, argstring)

# If wantig to start it locally in python and run through it step by step
'''
lpr_file = ""  # Diagnosis file
pheno_request = "/dpibp/home/mislun/scripts/MDD.request"  # File with all Diagnostic codes to export
stam_file = ""  # File adding information about Gender/Sex, Birthdate, etc.
addition_information_file = ""  # File with additional IID information
use_predefined_exdep_exclusions = ""  # List of diagnostic codes to exclude
general_exclusions = ""  # General exclusion list
diagnostic_col = "c_adiag"  # Column name of -f and -i file to be mapped
pheno_requestcol = "c_adiag"  # Column name of -g file to be mapped against
iidcol = "pnr"  # Column name of IDs in -f and -i file
birthdatecol = "birthdate"  # Column name of Birthdate in files
sexcol = "kqn"  # Column name of Sex/Gender in files
main_pheno_name = "MDD"
fsep = ","  # Separator of -f file
gsep = ","  # Separator of -g file
outfile = "/dpibp/home/mislun/scripts/MDD.result.tsv"  # Outfile name
exact_match = False  # Exact Match flag
input_date_in_name = "d_inddto"  # Column name of first diagnosis date
input_date_out_name = "d_uddto"  # Column name of last diagnosis date
qced_iids = ""  # List with all IIDs that pass initial QC
ctype_excl = ""  # List of diagnostic types to exclude
ctype_incl = ""  # List of diagnostic types to include
lifetime_exclusions_file = ""  # Lifetime exclusions file
post_exclusions_file = ""  # Post exclusions file
oneYearPrior_exclusions_file = ""  # One Year Prior exclusions file
exclCHBcontrols = False  # Exclude CHB controls flag
Filter_YoB = ""  # Filter by Year of Birth
Filter_Gender = ""  # Filter by Gender
verbose = False  # Verbose output flag
Build_Test_Set = False  # Build Test Set flag
test_run = False  # Test Run flag
MatchFI = False  # Match FI flag
skip_icd_update = False  # Skip ICD Update flag
DateFormat_in = "%d/%m/%Y"  # Format of Dates in your data
iidstatus_col = ""  # Column name of IID status
remove_point_in_diag_request = False  # Remove point in diag request flag
num_threads = 8  # Number of threads to be used
'''
